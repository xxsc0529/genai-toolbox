[{"body":"About AlloyDB for PostgreSQL is a fully-managed, PostgreSQL-compatible database for demanding transactional workloads. It provides enterprise-grade performance and availability while maintaining 100% compatibility with open-source PostgreSQL.\nIf you are new to AlloyDB for PostgreSQL, you can create a free trial cluster.\nAvailable Tools alloydb-ai-nl\nUse natural language queries on AlloyDB, powered by AlloyDB AI.\npostgres-sql\nExecute SQL queries as prepared statements in AlloyDB Postgres.\npostgres-execute-sql\nRun parameterized SQL statements in AlloyDB Postgres.\nPre-built Configurations AlloyDB using MCP\nConnect your IDE to AlloyDB using Toolbox.\nAlloyDB Admin API using MCP\nCreate your AlloyDB database with MCP Toolbox.\nRequirements IAM Permissions By default, AlloyDB for PostgreSQL source uses the AlloyDB Go Connector to authorize and establish mTLS connections to your AlloyDB instance. The Go connector uses your Application Default Credentials (ADC) to authorize your connection to AlloyDB.\nIn addition to setting the ADC for your server, you need to ensure the IAM identity has been given the following IAM roles (or corresponding permissions):\nroles/alloydb.client roles/serviceusage.serviceUsageConsumer Networking AlloyDB supports connecting over both from external networks via the internet (public IP), and internal networks (private IP). For more information on choosing between the two options, see the AlloyDB page Connection overview.\nYou can configure the ipType parameter in your source configuration to public or private to match your cluster’s configuration. Regardless of which you choose, all connections use IAM-based authorization and are encrypted with mTLS.\nAuthentication This source supports both password-based authentication and IAM authentication (using your Application Default Credentials).\nStandard Authentication To connect using user/password, create a PostgreSQL user and input your credentials in the user and password fields.\nuser: ${USER_NAME} password: ${PASSWORD} IAM Authentication To connect using IAM authentication:\nPrepare your database instance and user following this guide. You could choose one of the two ways to log in: Specify your IAM email as the user. Leave your user field blank. Toolbox will fetch the ADC automatically and log in using the email associated with it. Leave the password field blank. Example sources: my-alloydb-pg-source: kind: alloydb-postgres project: my-project-id region: us-central1 cluster: my-cluster instance: my-instance database: my_db user: ${USER_NAME} password: ${PASSWORD} # ipType: \"public\" Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “alloydb-postgres”. project string true Id of the GCP project that the cluster was created in (e.g. “my-project-id”). region string true Name of the GCP region that the cluster was created in (e.g. “us-central1”). cluster string true Name of the AlloyDB cluster (e.g. “my-cluster”). instance string true Name of the AlloyDB instance within the cluster (e.g. “my-instance”). database string true Name of the Postgres database to connect to (e.g. “my_db”). user string false Name of the Postgres user to connect as (e.g. “my-pg-user”). Defaults to IAM auth using ADC email if unspecified. password string false Password of the Postgres user (e.g. “my-password”). Defaults to attempting IAM authentication if unspecified. ipType string false IP Type of the AlloyDB instance; must be one of public or private. Default: public. ","categories":"","description":"AlloyDB for PostgreSQL is a fully-managed, PostgreSQL-compatible database for  demanding transactional workloads.\n","excerpt":"AlloyDB for PostgreSQL is a fully-managed, PostgreSQL-compatible …","ref":"/genai-toolbox/resources/sources/alloydb-pg/","tags":"","title":"AlloyDB for PostgreSQL"},{"body":"","categories":"","description":"AlloyDB AI NL Tool.\n","excerpt":"AlloyDB AI NL Tool.\n","ref":"/genai-toolbox/resources/tools/alloydbainl/","tags":"","title":"AlloyDB AI NL"},{"body":"About The alloydb-ai-nl tool leverages AlloyDB AI next-generation natural Language support to allow an Agent the ability to query the database directly using natural language. Natural language streamlines the development of generative AI applications by transferring the complexity of converting natural language to SQL from the application layer to the database layer.\nThis tool is compatible with the following sources:\nalloydb-postgres AlloyDB AI Natural Language delivers secure and accurate responses for application end user natural language questions. Natural language streamlines the development of generative AI applications by transferring the complexity of converting natural language to SQL from the application layer to the database layer.\nRequirements Tip\nAlloyDB AI natural language is currently in gated public preview. For more information on availability and limitations, please see AlloyDB AI natural language overview\nTo enable AlloyDB AI natural language for your AlloyDB cluster, please follow the steps listed in the Generate SQL queries that answer natural language questions, including enabling the extension and configuring context for your application.\nConfiguration Specifying an nl_config A nl_config is a configuration that associates an application to schema objects, examples and other contexts that can be used. A large application can also use different configurations for different parts of the app, as long as the correct configuration can be specified when a question is sent from that part of the application.\nOnce you’ve followed the steps for configuring context, you can use the context field when configuring a alloydb-ai-nl tool. When this tool is invoked, the SQL will be generated and executed using this context.\nSpecifying Parameters to PSV’s Parameterized Secure Views (PSVs) are a feature unique to AlloyDB that allows you to require one or more named parameter values passed to the view when querying it, somewhat like bind variables with ordinary database queries.\nYou can use the nlConfigParameters to list the parameters required for your nl_config. You must supply all parameters required for all PSVs in the context. It’s strongly recommended to use features like Authenticated Parameters or Bound Parameters to provide secure access to queries generated using natural language, as these parameters are not visible to the LLM.\nTip\nMake sure to enable the parameterized_views extension before running this tool. You can do so by running this command in the AlloyDB studio:\nCREATE EXTENSION IF NOT EXISTS parameterized_views; Example tools: ask_questions: kind: alloydb-ai-nl source: my-alloydb-source description: \"Ask questions to check information about flights\" nlConfig: \"cymbal_air_nl_config\" nlConfigParameters: - name: user_email type: string description: User ID of the logged in user. # note: we strongly recommend using features like Authenticated or # Bound parameters to prevent the LLM from seeing these params and # specifying values it shouldn't in the tool input authServices: - name: my_google_service field: email Reference field type required description kind string true Must be “alloydb-ai-nl”. source string true Name of the AlloyDB source the natural language query should execute on. description string true Description of the tool that is passed to the LLM. nlConfig string true The name of the nl_config in AlloyDB nlConfigParameters parameters true List of PSV parameters defined in the nl_config ","categories":"","description":"The \"alloydb-ai-nl\" tool leverages [AlloyDB AI](https://cloud.google.com/alloydb/ai) next-generation Natural Language support to provide the ability to query the database directly using natural language.\n","excerpt":"The \"alloydb-ai-nl\" tool leverages [AlloyDB …","ref":"/genai-toolbox/resources/tools/alloydbainl/alloydb-ai-nl/","tags":"","title":"alloydb-ai-nl"},{"body":"AuthServices represent services that handle authentication and authorization. It can primarily be used by Tools in two different ways:\nAuthorized Invocation is when a tool is validated by the auth service before the call can be invoked. Toolbox will reject any calls that fail to validate or have an invalid token. Authenticated Parameters replace the value of a parameter with a field from an OIDC claim. Toolbox will automatically resolve the ID token provided by the client and replace the parameter in the tool call. Example The following configurations are placed at the top level of a tools.yaml file.\nTip\nIf you are accessing Toolbox with multiple applications, each application should register their own Client ID even if they use the same “kind” of auth provider.\nauthServices: my_auth_app_1: kind: google clientId: ${YOUR_CLIENT_ID_1} my_auth_app_2: kind: google clientId: ${YOUR_CLIENT_ID_2} Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nAfter you’ve configured an authService you’ll, need to reference it in the configuration for each tool that should use it:\nAuthorized Invocations for authorizing a tool call, use the authRequired field in a tool config Authenticated Parameters for using the value from a OIDC claim, use the authServices field in a parameter config Specifying ID Tokens from Clients After configuring your authServices section, use a Toolbox SDK to add your ID tokens to the header of a Tool invocation request. When specifying a token you will provide a function (that returns an id). This function is called when the tool is invoked. This allows you to cache and refresh the ID token as needed.\nThe primary method for providing these getters is via the auth_token_getters parameter when loading tools, or the add_auth_token_getter() / add_auth_token_getters() methods on a loaded tool object.\nSpecifying tokens during load Core LangChain Llamaindex import asyncio from toolbox_core import ToolboxClient async def get_auth_token(): # ... Logic to retrieve ID token (e.g., from local storage, OAuth flow) # This example just returns a placeholder. Replace with your actual token retrieval. return \"YOUR_ID_TOKEN\" # Placeholder async def main(): async with ToolboxClient(\"\u003chttp://127.0.0.1:5000\u003e\") as toolbox: auth_tool = await toolbox.load_tool( \"get_sensitive_data\", auth_token_getters={\"my_auth_app_1\": get_auth_token} ) result = await auth_tool(param=\"value\") print(result) if **name** == \"**main**\": asyncio.run(main()) import asyncio from toolbox_langchain import ToolboxClient async def get_auth_token(): # ... Logic to retrieve ID token (e.g., from local storage, OAuth flow) # This example just returns a placeholder. Replace with your actual token retrieval. return \"YOUR_ID_TOKEN\" # Placeholder async def main(): toolbox = ToolboxClient(\"\u003chttp://127.0.0.1:5000\u003e\") auth_tool = await toolbox.aload_tool( \"get_sensitive_data\", auth_token_getters={\"my_auth_app_1\": get_auth_token} ) result = await auth_tool.ainvoke({\"param\": \"value\"}) print(result) if **name** == \"**main**\": asyncio.run(main()) import asyncio from toolbox_llamaindex import ToolboxClient async def get_auth_token(): # ... Logic to retrieve ID token (e.g., from local storage, OAuth flow) # This example just returns a placeholder. Replace with your actual token retrieval. return \"YOUR_ID_TOKEN\" # Placeholder async def main(): toolbox = ToolboxClient(\"\u003chttp://127.0.0.1:5000\u003e\") auth_tool = await toolbox.aload_tool( \"get_sensitive_data\", auth_token_getters={\"my_auth_app_1\": get_auth_token} ) # result = await auth_tool.acall(param=\"value\") # print(result.content) if **name** == \"**main**\": asyncio.run(main()) Specifying tokens for existing tools Core LangChain Llamaindex tools = await toolbox.load_toolset() # for a single token authorized_tool = tools[0].add_auth_token_getter(\"my_auth\", get_auth_token) # OR, if multiple tokens are needed authorized_tool = tools[0].add_auth_token_getters({ \"my_auth1\": get_auth1_token, \"my_auth2\": get_auth2_token, }) tools = toolbox.load_toolset() # for a single token authorized_tool = tools[0].add_auth_token_getter(\"my_auth\", get_auth_token) # OR, if multiple tokens are needed authorized_tool = tools[0].add_auth_token_getters({ \"my_auth1\": get_auth1_token, \"my_auth2\": get_auth2_token, }) tools = toolbox.load_toolset() # for a single token authorized_tool = tools[0].add_auth_token_getter(\"my_auth\", get_auth_token) # OR, if multiple tokens are needed authorized_tool = tools[0].add_auth_token_getters({ \"my_auth1\": get_auth1_token, \"my_auth2\": get_auth2_token, }) Kinds of Auth Services ","categories":"","description":"AuthServices represent services that handle authentication and authorization.\n","excerpt":"AuthServices represent services that handle authentication and …","ref":"/genai-toolbox/resources/authservices/","tags":"","title":"AuthServices"},{"body":"BigQuery Source BigQuery is Google Cloud’s fully managed, petabyte-scale, and cost-effective analytics data warehouse that lets you run analytics over vast amounts of data in near real time. With BigQuery, there’s no infrastructure to set up or manage, letting you focus on finding meaningful insights using GoogleSQL and taking advantage of flexible pricing models across on-demand and flat-rate options.\nIf you are new to BigQuery, you can try to load and query data with the bq tool.\nBigQuery uses GoogleSQL for querying data. GoogleSQL is an ANSI-compliant structured query language (SQL) that is also implemented for other Google Cloud services. SQL queries are handled by cluster nodes in the same way as NoSQL data requests. Therefore, the same best practices apply when creating SQL queries to run against your BigQuery data, such as avoiding full table scans or complex filters.\nAvailable Tools bigquery-sql\nRun SQL queries directly against BigQuery datasets.\nbigquery-execute-sql\nExecute structured queries using parameters.\nbigquery-get-dataset-info\nRetrieve metadata for a specific dataset.\nbigquery-get-table-info\nRetrieve metadata for a specific table.\nbigquery-list-dataset-ids\nList available dataset IDs.\nbigquery-list-table-ids\nList tables in a given dataset.\nPre-built Configurations BigQuery using MCP\nConnect your IDE to BigQuery using Toolbox. Requirements IAM Permissions BigQuery uses Identity and Access Management (IAM) to control user and group access to BigQuery resources like projects, datasets, and tables. Toolbox will use your Application Default Credentials (ADC) to authorize and authenticate when interacting with BigQuery.\nIn addition to setting the ADC for your server, you need to ensure the IAM identity has been given the correct IAM permissions for the queries you intend to run. Common roles include roles/bigquery.user (which includes permissions to run jobs and read data) or roles/bigquery.dataViewer. See Introduction to BigQuery IAM for more information on applying IAM permissions and roles to an identity.\nExample sources: my-bigquery-source: kind: \"bigquery\" project: \"my-project-id\" Reference field type required description kind string true Must be “bigquery”. project string true Id of the GCP project that the cluster was created in (e.g. “my-project-id”). location string false Specifies the location (e.g., ‘us’, ‘asia-northeast1’) in which to run the query job. This location must match the location of any tables referenced in the query. The default behavior is for it to be executed in the US multi-region ","categories":"","description":"BigQuery is Google Cloud's fully managed, petabyte-scale, and cost-effective analytics data warehouse that lets you run analytics over vast amounts of  data in near real time. With BigQuery, there's no infrastructure to set  up or manage, letting you focus on finding meaningful insights using  GoogleSQL and taking advantage of flexible pricing models across on-demand  and flat-rate options.\n","excerpt":"BigQuery is Google Cloud's fully managed, petabyte-scale, and …","ref":"/genai-toolbox/resources/sources/bigquery/","tags":"","title":"BigQuery"},{"body":"","categories":"","description":"Tools that work with BigQuery Sources.\n","excerpt":"Tools that work with BigQuery Sources.\n","ref":"/genai-toolbox/resources/tools/bigquery/","tags":"","title":"BigQuery"},{"body":"","categories":"","description":"How to get started with Toolbox using BigQuery.\n","excerpt":"How to get started with Toolbox using BigQuery.\n","ref":"/genai-toolbox/samples/bigquery/","tags":"","title":"BigQuery"},{"body":"About A bigquery-execute-sql tool executes a SQL statement against BigQuery. It’s compatible with the following sources:\nbigquery bigquery-execute-sql takes a required sql input parameter and runs the SQL statement against the configured source. It also supports an optional dry_run parameter to validate a query without executing it.\nExample tools: execute_sql_tool: kind: bigquery-execute-sql source: my-bigquery-source description: Use this tool to execute sql statement. Reference field type required description kind string true Must be “bigquery-execute-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"bigquery-execute-sql\" tool executes a SQL statement against BigQuery.\n","excerpt":"A \"bigquery-execute-sql\" tool executes a SQL statement against …","ref":"/genai-toolbox/resources/tools/bigquery/bigquery-execute-sql/","tags":"","title":"bigquery-execute-sql"},{"body":"About A bigquery-get-dataset-info tool retrieves metadata for a BigQuery dataset. It’s compatible with the following sources:\nbigquery bigquery-get-dataset-info takes a dataset parameter to specify the dataset on the given source. It also optionally accepts a project parameter to define the Google Cloud project ID. If the project parameter is not provided, the tool defaults to using the project defined in the source configuration.\nExample tools: bigquery_get_dataset_info: kind: bigquery-get-dataset-info source: my-bigquery-source description: Use this tool to get dataset metadata. Reference field type required description kind string true Must be “bigquery-get-dataset-info”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"bigquery-get-dataset-info\" tool retrieves metadata for a BigQuery dataset.\n","excerpt":"A \"bigquery-get-dataset-info\" tool retrieves metadata for a BigQuery …","ref":"/genai-toolbox/resources/tools/bigquery/bigquery-get-dataset-info/","tags":"","title":"bigquery-get-dataset-info"},{"body":"About A bigquery-get-table-info tool retrieves metadata for a BigQuery table. It’s compatible with the following sources:\nbigquery bigquery-get-table-info takes dataset and table parameters to specify the target table. It also optionally accepts a project parameter to define the Google Cloud project ID. If the project parameter is not provided, the tool defaults to using the project defined in the source configuration.\nExample tools: bigquery_get_table_info: kind: bigquery-get-table-info source: my-bigquery-source description: Use this tool to get table metadata. Reference field type required description kind string true Must be “bigquery-get-table-info”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"bigquery-get-table-info\" tool retrieves metadata for a BigQuery table.\n","excerpt":"A \"bigquery-get-table-info\" tool retrieves metadata for a BigQuery …","ref":"/genai-toolbox/resources/tools/bigquery/bigquery-get-table-info/","tags":"","title":"bigquery-get-table-info"},{"body":"About A bigquery-list-dataset-ids tool returns all dataset IDs from the source. It’s compatible with the following sources:\nbigquery bigquery-list-dataset-ids optionally accepts a project parameter to define the Google Cloud project ID. If the project parameter is not provided, the tool defaults to using the project defined in the source configuration.\nExample tools: bigquery_list_dataset_ids: kind: bigquery-list-dataset-ids source: my-bigquery-source description: Use this tool to get dataset metadata. Reference field type required description kind string true Must be “bigquery-list-dataset-ids”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"bigquery-list-dataset-ids\" tool returns all dataset IDs from the source.\n","excerpt":"A \"bigquery-list-dataset-ids\" tool returns all dataset IDs from the …","ref":"/genai-toolbox/resources/tools/bigquery/bigquery-list-dataset-ids/","tags":"","title":"bigquery-list-dataset-ids"},{"body":"About A bigquery-list-table-ids tool returns table IDs in a given BigQuery dataset. It’s compatible with the following sources:\nbigquery bigquery-get-dataset-info takes a required dataset parameter to specify the dataset from which to list table IDs. It also optionally accepts a project parameter to define the Google Cloud project ID. If the project parameter is not provided, the tool defaults to using the project defined in the source configuration.\nExample tools: bigquery_list_table_ids: kind: bigquery-list-table-ids source: my-bigquery-source description: Use this tool to get table metadata. Reference field type required description kind string true Must be “bigquery-list-table-ids”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"bigquery-list-table-ids\" tool returns table IDs in a given BigQuery dataset.\n","excerpt":"A \"bigquery-list-table-ids\" tool returns table IDs in a given BigQuery …","ref":"/genai-toolbox/resources/tools/bigquery/bigquery-list-table-ids/","tags":"","title":"bigquery-list-table-ids"},{"body":"About A bigquery-sql tool executes a pre-defined SQL statement. It’s compatible with the following sources:\nbigquery GoogleSQL BigQuery uses GoogleSQL for querying data. The integration with Toolbox supports this dialect. The specified SQL statement is executed, and parameters can be inserted into the query. BigQuery supports both named parameters (e.g., @name) and positional parameters (?), but they cannot be mixed in the same query.\nExample Note: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\ntools: # Example: Querying a user table in BigQuery search_users_bq: kind: bigquery-sql source: my-bigquery-source statement: | SELECT id, name, email FROM `my-project.my-dataset.users` WHERE id = @id OR email = @email; description: | Use this tool to get information for a specific user. Takes an id number or a name and returns info on the user. Example: {{ \"id\": 123, \"name\": \"Alice\", }} parameters: - name: id type: integer description: User ID - name: email type: string description: Email address of the user Example with Template Parameters Note: This tool allows direct modifications to the SQL statement, including identifiers, column names, and table names. This makes it more vulnerable to SQL injections. Using basic parameters only (see above) is recommended for performance and safety reasons. For more details, please check templateParameters.\ntools: list_table: kind: bigquery-sql source: my-bigquery-source statement: | SELECT * FROM {{.tableName}}; description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", }} templateParameters: - name: tableName type: string description: Table to select from Reference field type required description kind string true Must be “bigquery-sql”. source string true Name of the source the GoogleSQL should execute on. description string true Description of the tool that is passed to the LLM. statement string true The GoogleSQL statement to execute. parameters parameters false List of parameters that will be inserted into the SQL statement. templateParameters templateParameters false List of templateParameters that will be inserted into the SQL statement before executing prepared statement. ","categories":"","description":"A \"bigquery-sql\" tool executes a pre-defined SQL statement.\n","excerpt":"A \"bigquery-sql\" tool executes a pre-defined SQL statement.\n","ref":"/genai-toolbox/resources/tools/bigquery/bigquery-sql/","tags":"","title":"bigquery-sql"},{"body":"Bigtable Source Bigtable is a low-latency NoSQL database service for machine learning, operational analytics, and user-facing operations. It’s a wide-column, key-value store that can scale to billions of rows and thousands of columns. With Bigtable, you can replicate your data to regions across the world for high availability and data resiliency.\nIf you are new to Bigtable, you can try to create an instance and write data with the cbt CLI.\nYou can use GoogleSQL statements to query your Bigtable data. GoogleSQL is an ANSI-compliant structured query language (SQL) that is also implemented for other Google Cloud services. SQL queries are handled by cluster nodes in the same way as NoSQL data requests. Therefore, the same best practices apply when creating SQL queries to run against your Bigtable data, such as avoiding full table scans or complex filters.\nAvailable Tools bigtable-sql Run SQL-like queries over Bigtable rows. Requirements IAM Permissions Bigtable uses Identity and Access Management (IAM) to control user and group access to Bigtable resources at the project, instance, table, and backup level. Toolbox will use your Application Default Credentials (ADC) to authorize and authenticate when interacting with Bigtable.\nIn addition to setting the ADC for your server, you need to ensure the IAM identity has been given the correct IAM permissions for the query provided. See Apply IAM roles for more information on applying IAM permissions and roles to an identity.\nExample sources: my-bigtable-source: kind: \"bigtable\" project: \"my-project-id\" instance: \"test-instance\" Reference field type required description kind string true Must be “bigtable”. project string true Id of the GCP project that the cluster was created in (e.g. “my-project-id”). instance string true Name of the Bigtable instance. ","categories":"","description":"Bigtable is a low-latency NoSQL database service for machine learning, operational analytics, and user-facing operations. It's a wide-column, key-value store that can scale to billions of rows and thousands of columns. With Bigtable, you can replicate your data to regions across the world for high availability and data resiliency.\n","excerpt":"Bigtable is a low-latency NoSQL database service for machine learning, …","ref":"/genai-toolbox/resources/sources/bigtable/","tags":"","title":"Bigtable"},{"body":"","categories":"","description":"Tools that work with Bigtable Sources.\n","excerpt":"Tools that work with Bigtable Sources.\n","ref":"/genai-toolbox/resources/tools/bigtable/","tags":"","title":"Bigtable"},{"body":"About A bigtable-sql tool executes a pre-defined SQL statement against a Bigtable instance. It’s compatible with any of the following sources:\nbigtable GoogleSQL Bigtable supports SQL queries. The integration with Toolbox supports googlesql dialect, the specified SQL statement is executed as a data manipulation language (DML) statements, and specified parameters will inserted according to their name: e.g. @name.\nNote\nBigtable’s GoogleSQL support for DML statements might be limited to certain query types. For detailed information on supported DML statements and use cases, refer to the Bigtable GoogleSQL use cases.\nExample Note: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\ntools: search_user_by_id_or_name: kind: bigtable-sql source: my-bigtable-instance statement: | SELECT TO_INT64(cf[ 'id' ]) as id, CAST(cf[ 'name' ] AS string) as name, FROM mytable WHERE TO_INT64(cf[ 'id' ]) = @id OR CAST(cf[ 'name' ] AS string) = @name; description: | Use this tool to get information for a specific user. Takes an id number or a name and returns info on the user. Example: {{ \"id\": 123, \"name\": \"Alice\", }} parameters: - name: id type: integer description: User ID - name: name type: string description: Name of the user Example with Template Parameters Note: This tool allows direct modifications to the SQL statement, including identifiers, column names, and table names. This makes it more vulnerable to SQL injections. Using basic parameters only (see above) is recommended for performance and safety reasons. For more details, please check templateParameters.\ntools: list_table: kind: bigtable-sql source: my-bigtable-instance statement: | SELECT * FROM {{.tableName}}; description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", }} templateParameters: - name: tableName type: string description: Table to select from Reference field type required description kind string true Must be “bigtable-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. statement string true SQL statement to execute on. parameters parameters false List of parameters that will be inserted into the SQL statement. templateParameters templateParameters false List of templateParameters that will be inserted into the SQL statement before executing prepared statement. Tips Bigtable Studio is a useful to explore and manage your Bigtable data. If you’re unfamiliar with the query syntax, Query Builder lets you build a query, run it against a table, and then view the results in the console. Some Python libraries limit the use of underscore columns such as _key. A workaround would be to leverage Bigtable Logical Views to rename the columns. ","categories":"","description":"A \"bigtable-sql\" tool executes a pre-defined SQL statement against a Google Cloud Bigtable instance.\n","excerpt":"A \"bigtable-sql\" tool executes a pre-defined SQL statement against a …","ref":"/genai-toolbox/resources/tools/bigtable/bigtable-sql/","tags":"","title":"bigtable-sql"},{"body":"About Cloud SQL for MySQL is a fully-managed database service that helps you set up, maintain, manage, and administer your MySQL relational databases on Google Cloud Platform.\nIf you are new to Cloud SQL for MySQL, you can try creating and connecting to a database by following these instructions.\nAvailable Tools mysql-sql\nExecute pre-defined prepared SQL queries in MySQL.\nmysql-execute-sql\nRun parameterized SQL queries in Cloud SQL for MySQL.\nPre-built Configurations Cloud SQL for MySQL using MCP\nConnect your IDE to Cloud SQL for MySQL using Toolbox. Requirements IAM Permissions By default, this source uses the Cloud SQL Go Connector to authorize and establish mTLS connections to your Cloud SQL instance. The Go connector uses your Application Default Credentials (ADC) to authorize your connection to Cloud SQL.\nIn addition to setting the ADC for your server, you need to ensure the IAM identity has been given the following IAM roles (or corresponding permissions):\nroles/cloudsql.client Tip\nIf you are connecting from Compute Engine, make sure your VM also has the proper scope to connect using the Cloud SQL Admin API.\nNetworking Cloud SQL supports connecting over both from external networks via the internet (public IP), and internal networks (private IP). For more information on choosing between the two options, see the Cloud SQL page Connection overview.\nYou can configure the ipType parameter in your source configuration to public or private to match your cluster’s configuration. Regardless of which you choose, all connections use IAM-based authorization and are encrypted with mTLS.\nDatabase User Currently, this source only uses standard authentication. You will need to create a MySQL user to login to the database with.\nExample sources: my-cloud-sql-mysql-source: kind: cloud-sql-mysql project: my-project-id region: us-central1 instance: my-instance database: my_db user: ${USER_NAME} password: ${PASSWORD} # ipType: \"private\" Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “cloud-sql-mysql”. project string true Id of the GCP project that the cluster was created in (e.g. “my-project-id”). region string true Name of the GCP region that the cluster was created in (e.g. “us-central1”). instance string true Name of the Cloud SQL instance within the cluster (e.g. “my-instance”). database string true Name of the MySQL database to connect to (e.g. “my_db”). user string true Name of the MySQL user to connect as (e.g. “my-pg-user”). password string true Password of the MySQL user (e.g. “my-password”). ipType string false IP Type of the Cloud SQL instance; must be one of public or private. Default: public. ","categories":"","description":"Cloud SQL for MySQL is a fully-managed database service for MySQL.\n","excerpt":"Cloud SQL for MySQL is a fully-managed database service for MySQL.\n","ref":"/genai-toolbox/resources/sources/cloud-sql-mysql/","tags":"","title":"Cloud SQL for MySQL"},{"body":"About Cloud SQL for PostgreSQL is a fully-managed database service that helps you set up, maintain, manage, and administer your PostgreSQL relational databases on Google Cloud Platform.\nIf you are new to Cloud SQL for PostgreSQL, you can try creating and connecting to a database by following these instructions.\nAvailable Tools postgres-sql\nExecute SQL queries as prepared statements in PostgreSQL.\npostgres-execute-sql\nRun parameterized SQL statements in PostgreSQL.\nPre-built Configurations Cloud SQL for Postgres using MCP\nConnect your IDE to Cloud SQL for Postgres using Toolbox. Requirements IAM Permissions By default, this source uses the Cloud SQL Go Connector to authorize and establish mTLS connections to your Cloud SQL instance. The Go connector uses your Application Default Credentials (ADC) to authorize your connection to Cloud SQL.\nIn addition to setting the ADC for your server, you need to ensure the IAM identity has been given the following IAM roles (or corresponding permissions):\nroles/cloudsql.client Tip\nIf you are connecting from Compute Engine, make sure your VM also has the proper scope to connect using the Cloud SQL Admin API.\nNetworking Cloud SQL supports connecting over both from external networks via the internet (public IP), and internal networks (private IP). For more information on choosing between the two options, see the Cloud SQL page Connection overview.\nYou can configure the ipType parameter in your source configuration to public or private to match your cluster’s configuration. Regardless of which you choose, all connections use IAM-based authorization and are encrypted with mTLS.\nAuthentication This source supports both password-based authentication and IAM authentication (using your Application Default Credentials).\nStandard Authentication To connect using user/password, create a PostgreSQL user and input your credentials in the user and password fields.\nuser: ${USER_NAME} password: ${PASSWORD} IAM Authentication To connect using IAM authentication:\nPrepare your database instance and user following this guide.\nYou could choose one of the two ways to log in:\nSpecify your IAM email as the user. Leave your user field blank. Toolbox will fetch the ADC automatically and log in using the email associated with it. Leave the password field blank.\nExample sources: my-cloud-sql-pg-source: kind: cloud-sql-postgres project: my-project-id region: us-central1 instance: my-instance database: my_db user: ${USER_NAME} password: ${PASSWORD} # ipType: \"private\" Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “cloud-sql-postgres”. project string true Id of the GCP project that the cluster was created in (e.g. “my-project-id”). region string true Name of the GCP region that the cluster was created in (e.g. “us-central1”). instance string true Name of the Cloud SQL instance within the cluster (e.g. “my-instance”). database string true Name of the Postgres database to connect to (e.g. “my_db”). user string false Name of the Postgres user to connect as (e.g. “my-pg-user”). Defaults to IAM auth using ADC email if unspecified. password string false Password of the Postgres user (e.g. “my-password”). Defaults to attempting IAM authentication if unspecified. ipType string false IP Type of the Cloud SQL instance; must be one of public or private. Default: public. ","categories":"","description":"Cloud SQL for PostgreSQL is a fully-managed database service for Postgres.\n","excerpt":"Cloud SQL for PostgreSQL is a fully-managed database service for …","ref":"/genai-toolbox/resources/sources/cloud-sql-pg/","tags":"","title":"Cloud SQL for PostgreSQL"},{"body":"About Cloud SQL for SQL Server is a managed database service that helps you set up, maintain, manage, and administer your SQL Server databases on Google Cloud.\nIf you are new to Cloud SQL for SQL Server, you can try creating and connecting to a database by following these instructions.\nAvailable Tools mssql-sql\nExecute pre-defined SQL Server queries with placeholder parameters.\nmssql-execute-sql\nRun parameterized SQL Server queries in Cloud SQL for SQL Server.\nPre-built Configurations Cloud SQL for SQL Server using MCP\nConnect your IDE to Cloud SQL for SQL Server using Toolbox. Requirements IAM Permissions By default, this source uses the Cloud SQL Go Connector to authorize and establish mTLS connections to your Cloud SQL instance. The Go connector uses your Application Default Credentials (ADC) to authorize your connection to Cloud SQL.\nIn addition to setting the ADC for your server, you need to ensure the IAM identity has been given the following IAM roles (or corresponding permissions):\nroles/cloudsql.client Tip\nIf you are connecting from Compute Engine, make sure your VM also has the proper scope to connect using the Cloud SQL Admin API.\nNetworking Cloud SQL supports connecting over both from external networks via the internet (public IP), and internal networks (private IP). For more information on choosing between the two options, see the Cloud SQL page Connection overview.\nYou can configure the ipType parameter in your source configuration to public or private to match your cluster’s configuration. Regardless of which you choose, all connections use IAM-based authorization and are encrypted with mTLS.\nDatabase User Currently, this source only uses standard authentication. You will need to create a SQL Server user to login to the database with.\nExample sources: my-cloud-sql-mssql-instance: kind: cloud-sql-mssql project: my-project region: my-region instance: my-instance database: my_db ipAddress: localhost user: ${USER_NAME} password: ${PASSWORD} # ipType: private Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “cloud-sql-mssql”. project string true Id of the GCP project that the cluster was created in (e.g. “my-project-id”). region string true Name of the GCP region that the cluster was created in (e.g. “us-central1”). instance string true Name of the Cloud SQL instance within the cluster (e.g. “my-instance”). database string true Name of the Cloud SQL database to connect to (e.g. “my_db”). ipAddress string true IP address of the Cloud SQL instance to connect to. user string true Name of the SQL Server user to connect as (e.g. “my-pg-user”). password string true Password of the SQL Server user (e.g. “my-password”). ipType string false IP Type of the Cloud SQL instance, must be either public or private. Default: public. ","categories":"","description":"Cloud SQL for SQL Server is a fully-managed database service for SQL Server.\n","excerpt":"Cloud SQL for SQL Server is a fully-managed database service for SQL …","ref":"/genai-toolbox/resources/sources/cloud-sql-mssql/","tags":"","title":"Cloud SQL for SQL Server"},{"body":"","categories":"","description":"List of guides detailing how to connect your AI tools (IDEs) to Toolbox using MCP.\n","excerpt":"List of guides detailing how to connect your AI tools (IDEs) to …","ref":"/genai-toolbox/how-to/connect-ide/","tags":"","title":"Connect from your IDE"},{"body":"Toolbox SDKs vs Model Context Protocol (MCP) Toolbox now supports connections via both the native Toolbox SDKs and via Model Context Protocol (MCP). However, Toolbox has several features which are not supported in the MCP specification (such as Authenticated Parameters and Authorized invocation).\nWe recommend using the native SDKs over MCP clients to leverage these features. The native SDKs can be combined with MCP clients in many cases.\nProtocol Versions Toolbox currently supports the following versions of MCP specification:\n2025-06-18 2025-03-26 2024-11-05 Toolbox AuthZ/AuthN Not Supported by MCP The auth implementation in Toolbox is not supported in MCP’s auth specification. This includes:\nAuthenticated Parameters Authorized Invocations Connecting to Toolbox with an MCP client Before you begin Note\nMCP is only compatible with Toolbox version 0.3.0 and above.\nInstall Toolbox version 0.3.0+.\nMake sure you’ve set up and initialized your database.\nSet up your tools.yaml file.\nConnecting via Standard Input/Output (stdio) Toolbox supports the stdio transport protocol. Users that wish to use stdio will have to include the --stdio flag when running Toolbox.\n./toolbox --stdio When running with stdio, Toolbox will listen via stdio instead of acting as a remote HTTP server. Logs will be set to the warn level by default. debug and info logs are not supported with stdio.\nNote\nToolbox enables dynamic reloading by default. To disable, use the --disable-reload flag.\nConnecting via HTTP Toolbox supports the HTTP transport protocol with and without SSE.\nHTTP with SSE (deprecated) Streamable HTTP Add the following configuration to your MCP client configuration:\n{ \"mcpServers\": { \"toolbox\": { \"type\": \"sse\", \"url\": \"http://127.0.0.1:5000/mcp/sse\", } } } If you would like to connect to a specific toolset, replace url with \"http://127.0.0.1:5000/mcp/{toolset_name}/sse\".\nHTTP with SSE is only supported in version 2024-11-05 and is currently deprecated.\nAdd the following configuration to your MCP client configuration:\n{ \"mcpServers\": { \"toolbox\": { \"type\": \"http\", \"url\": \"http://127.0.0.1:5000/mcp\", } } } If you would like to connect to a specific toolset, replace url with \"http://127.0.0.1:5000/mcp/{toolset_name}\".\nUsing the MCP Inspector with Toolbox Use MCP Inspector for testing and debugging Toolbox server.\nSTDIO HTTP with SSE (deprecated) Streamable HTTP Run Inspector with Toolbox as a subprocess:\nnpx @modelcontextprotocol/inspector ./toolbox --stdio For Transport Type dropdown menu, select STDIO.\nIn Command, make sure that it is set to :./toolbox (or the correct path to where the Toolbox binary is installed).\nIn Arguments, make sure that it’s filled with --stdio.\nClick the Connect button. It might take awhile to spin up Toolbox. Voila! You should be able to inspect your toolbox tools!\nRun Toolbox.\nIn a separate terminal, run Inspector directly through npx:\nnpx @modelcontextprotocol/inspector For Transport Type dropdown menu, select SSE.\nFor URL, type in http://127.0.0.1:5000/mcp/sse to use all tool or http//127.0.0.1:5000/mcp/{toolset_name}/sse to use a specific toolset.\nClick the Connect button. Voila! You should be able to inspect your toolbox tools!\nRun Toolbox.\nIn a separate terminal, run Inspector directly through npx:\nnpx @modelcontextprotocol/inspector For Transport Type dropdown menu, select Streamable HTTP.\nFor URL, type in http://127.0.0.1:5000/mcp to use all tool or http//127.0.0.1:5000/mcp/{toolset_name} to use a specific toolset.\nClick the Connect button. Voila! You should be able to inspect your toolbox tools!\nTested Clients Client SSE Works MCP Config Docs Claude Desktop ✅ https://modelcontextprotocol.io/quickstart/user#1-download-claude-for-desktop MCP Inspector ✅ https://github.com/modelcontextprotocol/inspector Cursor ✅ https://docs.cursor.com/context/model-context-protocol Windsurf ✅ https://docs.windsurf.com/windsurf/mcp VS Code (Insiders) ✅ https://code.visualstudio.com/docs/copilot/chat/mcp-servers ","categories":"","description":"How to connect to Toolbox from a MCP Client.\n","excerpt":"How to connect to Toolbox from a MCP Client.\n","ref":"/genai-toolbox/how-to/connect_via_mcp/","tags":"","title":"Connect via MCP Client"},{"body":"About A couchbase source establishes a connection to a Couchbase database cluster, allowing tools to execute SQL queries against it.\nAvailable Tools couchbase-sql\nRun SQL++ statements on Couchbase with parameterized input. Example sources: my-couchbase-instance: kind: couchbase connectionString: couchbase://localhost:8091 bucket: travel-sample scope: inventory username: Administrator password: password Reference field type required description kind string true Must be “couchbase”. connectionString string true Connection string for the Couchbase cluster. bucket string true Name of the bucket to connect to. scope string true Name of the scope within the bucket. username string false Username for authentication. password string false Password for authentication. clientCert string false Path to client certificate file for TLS authentication. clientCertPassword string false Password for the client certificate. clientKey string false Path to client key file for TLS authentication. clientKeyPassword string false Password for the client key. caCert string false Path to CA certificate file. noSslVerify boolean false If true, skip server certificate verification. Warning: This option should only be used in development or testing environments. Disabling SSL verification poses significant security risks in production as it makes your connection vulnerable to man-in-the-middle attacks. profile string false Name of the connection profile to apply. queryScanConsistency integer false Query scan consistency. Controls the consistency guarantee for index scanning. Values: 1 for “not_bounded” (fastest option, but results may not include the most recent operations), 2 for “request_plus” (highest consistency level, includes all operations up until the query started, but incurs a performance penalty). If not specified, defaults to the Couchbase Go SDK default. ","categories":"","description":"A \"couchbase\" source connects to a Couchbase database.\n","excerpt":"A \"couchbase\" source connects to a Couchbase database.\n","ref":"/genai-toolbox/resources/sources/couchbase/","tags":"","title":"couchbase"},{"body":"","categories":"","description":"Tools that work with Couchbase Sources.\n","excerpt":"Tools that work with Couchbase Sources.\n","ref":"/genai-toolbox/resources/tools/couchbase/","tags":"","title":"Couchbase"},{"body":"About A couchbase-sql tool executes a pre-defined SQL statement against a Couchbase database. It’s compatible with any of the following sources:\ncouchbase The specified SQL statement is executed as a parameterized statement, and specified parameters will be used according to their name: e.g. $id.\nExample Note: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\ntools: search_products_by_category: kind: couchbase-sql source: my-couchbase-instance statement: | SELECT p.name, p.price, p.description FROM products p WHERE p.category = $category AND p.price \u003c $max_price ORDER BY p.price DESC LIMIT 10 description: | Use this tool to get a list of products for a specific category under a maximum price. Takes a category name, e.g. \"Electronics\" and a maximum price e.g 500 and returns a list of product names, prices, and descriptions. Do NOT use this tool with invalid category names. Do NOT guess a category name, Do NOT guess a price. Example: {{ \"category\": \"Electronics\", \"max_price\": 500 }} Example: {{ \"category\": \"Furniture\", \"max_price\": 1000 }} parameters: - name: category type: string description: Product category name - name: max_price type: integer description: Maximum price (positive integer) Example with Template Parameters Note: This tool allows direct modifications to the SQL statement, including identifiers, column names, and table names. This makes it more vulnerable to SQL injections. Using basic parameters only (see above) is recommended for performance and safety reasons. For more details, please check templateParameters.\ntools: list_table: kind: couchbase-sql source: my-couchbase-instance statement: | SELECT * FROM {{.tableName}}; description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", }} templateParameters: - name: tableName type: string description: Table to select from Reference field type required description kind string true Must be “couchbase-sql”. source string true Name of the source the SQL query should execute on. description string true Description of the tool that is passed to the LLM. statement string true SQL statement to execute parameters parameters false List of parameters that will be used with the SQL statement. templateParameters templateParameters false List of templateParameters that will be inserted into the SQL statement before executing prepared statement. authRequired array[string] false List of auth services that are required to use this tool. ","categories":"","description":"A \"couchbase-sql\" tool executes a pre-defined SQL statement against a Couchbase database.\n","excerpt":"A \"couchbase-sql\" tool executes a pre-defined SQL statement against a …","ref":"/genai-toolbox/resources/tools/couchbase/couchbase-sql/","tags":"","title":"couchbase-sql"},{"body":"Dataplex Source Dataplex Universal Catalog is a unified, intelligent governance solution for data and AI assets in Google Cloud. Dataplex Universal Catalog powers AI, analytics, and business intelligence at scale.\nAt the heart of these governance capabilities is a catalog that contains a centralized inventory of the data assets in your organization. Dataplex Universal Catalog holds business, technical, and runtime metadata for all of your data. It helps you discover relationships and semantics in the metadata by applying artificial intelligence and machine learning.\nExample sources: my-dataplex-source: kind: \"dataplex\" project: \"my-project-id\" Sample System Prompt You can use the following system prompt as “Custom Instructions” in your client application.\n# Objective Your primary objective is to help discover, organize and manage metadata related to data assets. # Tone and Style 1. Adopt the persona of a senior subject matter expert 2. Your communication style must be: 1. Concise: Always favor brevity. 2. Direct: Avoid greetings (e.g., \"Hi there!\", \"Certainly!\"). Get straight to the point. Example (Incorrect): Hi there! I see that you are looking for... Example (Correct): This problem likely stems from... 3. Do not reiterate or summarize the question in the answer. 4. Crucially, always convey a tone of uncertainty and caution. Since you are interpreting metadata and have no way to externally verify your answers, never express complete confidence. Frame your responses as interpretations based solely on the provided metadata. Use a suggestive tone, not a prescriptive one: Example (Correct): \"The entry describes...\" Example (Correct): \"According to catalog,...\" Example (Correct): \"Based on the metadata,...\" Example (Correct): \"Based on the search results,...\" 5. Do not make assumptions # Data Model ## Entries Entry represents a specific data asset. Entry acts as a metadata record for something that is managed by Catalog, such as: - A BigQuery table or dataset - A Cloud Storage bucket or folder - An on-premises SQL table ## Aspects While the Entry itself is a container, the rich descriptive information about the asset (e.g., schema, data types, business descriptions, classifications) is stored in associated components called Aspects. Aspects are created based on pre-defined blueprints known as Aspect Types. ## Aspect Types Aspect Type is a reusable template that defines the schema for a set of metadata fields. Think of an Aspect Type as a structure for the kind of metadata that is organized in the catalog within the Entry. Examples: - projects/dataplex-types/locations/global/aspectTypes/analytics-hub-exchange - projects/dataplex-types/locations/global/aspectTypes/analytics-hub - projects/dataplex-types/locations/global/aspectTypes/analytics-hub-listing - projects/dataplex-types/locations/global/aspectTypes/bigquery-connection - projects/dataplex-types/locations/global/aspectTypes/bigquery-data-policy - projects/dataplex-types/locations/global/aspectTypes/bigquery-dataset - projects/dataplex-types/locations/global/aspectTypes/bigquery-model - projects/dataplex-types/locations/global/aspectTypes/bigquery-policy - projects/dataplex-types/locations/global/aspectTypes/bigquery-routine - projects/dataplex-types/locations/global/aspectTypes/bigquery-row-access-policy - projects/dataplex-types/locations/global/aspectTypes/bigquery-table - projects/dataplex-types/locations/global/aspectTypes/bigquery-view - projects/dataplex-types/locations/global/aspectTypes/cloud-bigtable-instance - projects/dataplex-types/locations/global/aspectTypes/cloud-bigtable-table - projects/dataplex-types/locations/global/aspectTypes/cloud-spanner-database - projects/dataplex-types/locations/global/aspectTypes/cloud-spanner-instance - projects/dataplex-types/locations/global/aspectTypes/cloud-spanner-table - projects/dataplex-types/locations/global/aspectTypes/cloud-spanner-view - projects/dataplex-types/locations/global/aspectTypes/cloudsql-database - projects/dataplex-types/locations/global/aspectTypes/cloudsql-instance - projects/dataplex-types/locations/global/aspectTypes/cloudsql-schema - projects/dataplex-types/locations/global/aspectTypes/cloudsql-table - projects/dataplex-types/locations/global/aspectTypes/cloudsql-view - projects/dataplex-types/locations/global/aspectTypes/contacts - projects/dataplex-types/locations/global/aspectTypes/dataform-code-asset - projects/dataplex-types/locations/global/aspectTypes/dataform-repository - projects/dataplex-types/locations/global/aspectTypes/dataform-workspace - projects/dataplex-types/locations/global/aspectTypes/dataproc-metastore-database - projects/dataplex-types/locations/global/aspectTypes/dataproc-metastore-service - projects/dataplex-types/locations/global/aspectTypes/dataproc-metastore-table - projects/dataplex-types/locations/global/aspectTypes/data-product - projects/dataplex-types/locations/global/aspectTypes/data-quality-scorecard - projects/dataplex-types/locations/global/aspectTypes/external-connection - projects/dataplex-types/locations/global/aspectTypes/overview - projects/dataplex-types/locations/global/aspectTypes/pubsub-topic - projects/dataplex-types/locations/global/aspectTypes/schema - projects/dataplex-types/locations/global/aspectTypes/sensitive-data-protection-job-result - projects/dataplex-types/locations/global/aspectTypes/sensitive-data-protection-profile - projects/dataplex-types/locations/global/aspectTypes/sql-access - projects/dataplex-types/locations/global/aspectTypes/storage-bucket - projects/dataplex-types/locations/global/aspectTypes/storage-folder - projects/dataplex-types/locations/global/aspectTypes/storage - projects/dataplex-types/locations/global/aspectTypes/usage ## Entry Types Every Entry must conform to an Entry Type. The Entry Type acts as a template, defining the structure, required aspects, and constraints for Entries of that type. Examples: - projects/dataplex-types/locations/global/entryTypes/analytics-hub-exchange - projects/dataplex-types/locations/global/entryTypes/analytics-hub-listing - projects/dataplex-types/locations/global/entryTypes/bigquery-connection - projects/dataplex-types/locations/global/entryTypes/bigquery-data-policy - projects/dataplex-types/locations/global/entryTypes/bigquery-dataset - projects/dataplex-types/locations/global/entryTypes/bigquery-model - projects/dataplex-types/locations/global/entryTypes/bigquery-routine - projects/dataplex-types/locations/global/entryTypes/bigquery-row-access-policy - projects/dataplex-types/locations/global/entryTypes/bigquery-table - projects/dataplex-types/locations/global/entryTypes/bigquery-view - projects/dataplex-types/locations/global/entryTypes/cloud-bigtable-instance - projects/dataplex-types/locations/global/entryTypes/cloud-bigtable-table - projects/dataplex-types/locations/global/entryTypes/cloud-spanner-database - projects/dataplex-types/locations/global/entryTypes/cloud-spanner-instance - projects/dataplex-types/locations/global/entryTypes/cloud-spanner-table - projects/dataplex-types/locations/global/entryTypes/cloud-spanner-view - projects/dataplex-types/locations/global/entryTypes/cloudsql-mysql-database - projects/dataplex-types/locations/global/entryTypes/cloudsql-mysql-instance - projects/dataplex-types/locations/global/entryTypes/cloudsql-mysql-table - projects/dataplex-types/locations/global/entryTypes/cloudsql-mysql-view - projects/dataplex-types/locations/global/entryTypes/cloudsql-postgresql-database - projects/dataplex-types/locations/global/entryTypes/cloudsql-postgresql-instance - projects/dataplex-types/locations/global/entryTypes/cloudsql-postgresql-schema - projects/dataplex-types/locations/global/entryTypes/cloudsql-postgresql-table - projects/dataplex-types/locations/global/entryTypes/cloudsql-postgresql-view - projects/dataplex-types/locations/global/entryTypes/cloudsql-sqlserver-database - projects/dataplex-types/locations/global/entryTypes/cloudsql-sqlserver-instance - projects/dataplex-types/locations/global/entryTypes/cloudsql-sqlserver-schema - projects/dataplex-types/locations/global/entryTypes/cloudsql-sqlserver-table - projects/dataplex-types/locations/global/entryTypes/cloudsql-sqlserver-view - projects/dataplex-types/locations/global/entryTypes/dataform-code-asset - projects/dataplex-types/locations/global/entryTypes/dataform-repository - projects/dataplex-types/locations/global/entryTypes/dataform-workspace - projects/dataplex-types/locations/global/entryTypes/dataproc-metastore-database - projects/dataplex-types/locations/global/entryTypes/dataproc-metastore-service - projects/dataplex-types/locations/global/entryTypes/dataproc-metastore-table - projects/dataplex-types/locations/global/entryTypes/pubsub-topic - projects/dataplex-types/locations/global/entryTypes/storage-bucket - projects/dataplex-types/locations/global/entryTypes/storage-folder - projects/dataplex-types/locations/global/entryTypes/vertexai-dataset - projects/dataplex-types/locations/global/entryTypes/vertexai-feature-group - projects/dataplex-types/locations/global/entryTypes/vertexai-feature-online-store ## Entry Groups Entries are organized within Entry Groups, which are logical groupings of Entries. An Entry Group acts as a namespace for its Entries. ## Entry Links Entries can be linked together using EntryLinks to represent relationships between data assets (e.g. foreign keys). # Tool instructions ## Tool: dataplex_search_entries ## General - Do not try to search within search results on your own. - Do not fetch multiple pages of results unless explicitly asked. ## Search syntax ### Simple search In its simplest form, a search query consists of a single predicate. Such a predicate can match several pieces of metadata: - A substring of a name, display name, or description of a resource - A substring of the type of a resource - A substring of a column name (or nested column name) in the schema of a resource - A substring of a project ID - A string from an overview description For example, the predicate foo matches the following resources: - Resource with the name foo.bar - Resource with the display name Foo Bar - Resource with the description This is the foo script - Resource with the exact type foo - Column foo_bar in the schema of a resource - Nested column foo_bar in the schema of a resource - Project prod-foo-bar - Resource with an overview containing the word foo ### Qualified predicates You can qualify a predicate by prefixing it with a key that restricts the matching to a specific piece of metadata: - An equal sign (=) restricts the search to an exact match. - A colon (:) after the key matches the predicate to either a substring or a token within the value in the search results. Tokenization splits the stream of text into a series of tokens, with each token usually corresponding to a single word. For example: - name:foo selects resources with names that contain the foo substring, like foo1 and barfoo. - description:foo selects resources with the foo token in the description, like bar and foo. - location=foo matches resources in a specified location with foo as the location name. The predicate keys type, system, location, and orgid support only the exact match (=) qualifier, not the substring qualifier (:). For example, type=foo or orgid=number. Search syntax supports the following qualifiers: - \"name:x\" - Matches x as a substring of the resource ID. - \"displayname:x\" - Match x as a substring of the resource display name. - \"column:x\" - Matches x as a substring of the column name (or nested column name) in the schema of the resource. - \"description:x\" - Matches x as a token in the resource description. - \"label:bar\" - Matches BigQuery resources that have a label (with some value) and the label key has bar as a substring. - \"label=bar\" - Matches BigQuery resources that have a label (with some value) and the label key equals bar as a string. - \"label:bar:x\" - Matches x as a substring in the value of a label with a key bar attached to a BigQuery resource. - \"label=foo:bar\" - Matches BigQuery resources where the key equals foo and the key value equals bar. - \"label.foo=bar\" - Matches BigQuery resources where the key equals foo and the key value equals bar. - \"label.foo\" - Matches BigQuery resources that have a label whose key equals foo as a string. - \"type=TYPE\" - Matches resources of a specific entry type or its type alias. - \"projectid:bar\" - Matches resources within Google Cloud projects that match bar as a substring in the ID. - \"parent:x\" - Matches x as a substring of the hierarchical path of a resource. The parent path is a fully_qualified_name of the parent resource. - \"orgid=number\" - Matches resources within a Google Cloud organization with the exact ID value of the number. - \"system=SYSTEM\" - Matches resources from a specified system. For example, system=bigquery matches BigQuery resources. - \"location=LOCATION\" - Matches resources in a specified location with an exact name. For example, location=us-central1 matches assets hosted in Iowa. BigQuery Omni assets support this qualifier by using the BigQuery Omni location name. For example, location=aws-us-east-1 matches BigQuery Omni assets in Northern Virginia. - \"createtime\" - Finds resources that were created within, before, or after a given date or time. For example \"createtime:2019-01-01\" matches resources created on 2019-01-01. - \"updatetime\" - Finds resources that were updated within, before, or after a given date or time. For example \"updatetime\u003e2019-01-01\" matches resources updated after 2019-01-01. - \"fully_qualified_name:x\" - Matches x as a substring of fully_qualified_name. - \"fully_qualified_name=x\" - Matches x as fully_qualified_name. ### Logical operators A query can consist of several predicates with logical operators. If you don't specify an operator, logical AND is implied. For example, foo bar returns resources that match both predicate foo and predicate bar. Logical AND and logical OR are supported. For example, foo OR bar. You can negate a predicate with a - (hyphen) or NOT prefix. For example, -name:foo returns resources with names that don't match the predicate foo. Logical operators aren't case-sensitive. For example, both or and OR are acceptable. ### Request 1. Always try to rewrite the prompt using search syntax. ### Response 1. If there are multiple search results found 1. Present the list of search results 2. Format the output in nested ordered list, for example: Given ``` { results: [ { name: \"projects/test-project/locations/us/entryGroups/@bigquery-aws-us-east-1/entries/users\" entrySource: { displayName: \"Users\" description: \"Table contains list of users.\" location: \"aws-us-east-1\" system: \"BigQuery\" } }, { name: \"projects/another_project/locations/us-central1/entryGroups/@bigquery/entries/top_customers\" entrySource: { displayName: \"Top customers\", description: \"Table contains list of best customers.\" location: \"us-central1\" system: \"BigQuery\" } }, ] } ``` Return output formatted as markdown nested list: ``` * Users: - projectId: test_project - location: aws-us-east-1 - description: Table contains list of users. * Top customers: - projectId: another_project - location: us-central1 - description: Table contains list of best customers. ``` 3. Ask to select one of the presented search results 2. If there is only one search result found 1. Present the search result immediately. 3. If there are no search result found 1. Explain that no search result was found 2. Suggest to provide a more specific search query. ## Tool: dataplex_lookup_entry ### Request 1. Always try to limit the size of the response by specifying `aspect_types` parameter. Make sure to include to select view=CUSTOM when using aspect_types parameter. 2. If you do not know the name of the entry, use `dataplex_search_entries` tool ### Response 1. Unless asked for a specific aspect, respond with all aspects attached to the entry. Reference field type required description kind string true Must be “dataplex”. project string true ID of the GCP project used for quota and billing purposes (e.g. “my-project-id”). ","categories":"","description":"Dataplex Universal Catalog is a unified, intelligent governance solution for data and AI assets in Google Cloud. Dataplex Universal Catalog powers AI, analytics, and business intelligence at scale.\n","excerpt":"Dataplex Universal Catalog is a unified, intelligent governance …","ref":"/genai-toolbox/resources/sources/dataplex/","tags":"","title":"Dataplex"},{"body":"","categories":"","description":"Tools that work with Dataplex Sources.\n","excerpt":"Tools that work with Dataplex Sources.\n","ref":"/genai-toolbox/resources/tools/dataplex/","tags":"","title":"Dataplex"},{"body":"About A dataplex-lookup-entry tool returns details of a particular entry in Dataplex Catalog. It’s compatible with the following sources:\ndataplex dataplex-lookup-entry takes a required name parameter which contains the project and location to which the request should be attributed in the following form: projects/{project}/locations/{location} and also a required entry parameter which is the resource name of the entry in the following form: projects/{project}/locations/{location}/entryGroups/{entryGroup}/entries/{entry}. It also optionally accepts following parameters:\nview - View to control which parts of an entry the service should return. It takes integer values from 1-4 corresponding to type of view - BASIC, FULL, CUSTOM, ALL aspectTypes - Limits the aspects returned to the provided aspect types in the format projects/{project}/locations/{location}/aspectTypes/{aspectType}. It only works for CUSTOM view. paths - Limits the aspects returned to those associated with the provided paths within the Entry. It only works for CUSTOM view. Requirements IAM Permissions Dataplex uses Identity and Access Management (IAM) to control user and group access to Dataplex resources. Toolbox will use your Application Default Credentials (ADC) to authorize and authenticate when interacting with [Dataplex][dataplex-docs].\nIn addition to setting the ADC for your server, you need to ensure the IAM identity has been given the correct IAM permissions for the tasks you intend to perform. See Dataplex Universal Catalog IAM permissions and Dataplex Universal Catalog IAM roles for more information on applying IAM permissions and roles to an identity.\nExample tools: lookup_entry: kind: dataplex-lookup-entry source: my-dataplex-source description: Use this tool to retrieve a specific entry in Dataplex Catalog. Reference field type required description kind string true Must be “dataplex-lookup-entry”. source string true Name of the source the tool should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"dataplex-lookup-entry\" tool returns details of a particular entry in Dataplex Catalog.\n","excerpt":"A \"dataplex-lookup-entry\" tool returns details of a particular entry …","ref":"/genai-toolbox/resources/tools/dataplex/dataplex-lookup-entry/","tags":"","title":"dataplex-lookup-entry"},{"body":"About A dataplex-search-entries tool returns all entries in Dataplex Catalog (e.g. tables, views, models) that matches given user query. It’s compatible with the following sources:\ndataplex dataplex-search-entries takes a required query parameter based on which entries are filtered and returned to the user and a required name parameter which is constructed using source’s project if user does not provide it explicitly and has the following format: projects/{project}/locations/global. It also optionally accepts following parameters:\npageSize - Number of results in the search page. Defaults to 5. pageToken - Page token received from a previous locations.searchEntries call. orderBy - Specifies the ordering of results. Supported values are: relevance (default), last_modified_timestamp, last_modified_timestamp asc semanticSearch - Specifies whether the search should understand the meaning and intent behind the query, rather than just matching keywords. Defaults to true. scope - The scope under which the search should be operating. Since this parameter is not exposed to the toolbox user, it defaults to the organization where the project provided in name is located. Requirements IAM Permissions Dataplex uses Identity and Access Management (IAM) to control user and group access to Dataplex resources. Toolbox will use your Application Default Credentials (ADC) to authorize and authenticate when interacting with Dataplex.\nIn addition to setting the ADC for your server, you need to ensure the IAM identity has been given the correct IAM permissions for the tasks you intend to perform. See Dataplex Universal Catalog IAM permissions and Dataplex Universal Catalog IAM roles for more information on applying IAM permissions and roles to an identity.\nExample tools: dataplex-search-entries: kind: dataplex-search-entries source: my-dataplex-source description: Use this tool to get all the entries based on the provided query. Reference field type required description kind string true Must be “dataplex-search-entries”. source string true Name of the source the tool should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"dataplex-search-entries\" tool allows to search for entries based on the provided query.\n","excerpt":"A \"dataplex-search-entries\" tool allows to search for entries based on …","ref":"/genai-toolbox/resources/tools/dataplex/dataplex-search-entries/","tags":"","title":"dataplex-search-entries"},{"body":"About Dgraph is an open-source graph database. It is designed for real-time workloads, horizontal scalability, and data flexibility. Implemented as a distributed system, Dgraph processes queries in parallel to deliver the fastest result.\nThis source can connect to either a self-managed Dgraph cluster or one hosted on Dgraph Cloud. If you’re new to Dgraph, the fastest way to get started is to sign up for Dgraph Cloud.\nAvailable Tools dgraph-dql\nRun DQL (Dgraph Query Language) queries. Requirements Database User When connecting to a hosted Dgraph database, this source uses the API key for access. If you are using a dedicated environment, you will additionally need the namespace and user credentials for that namespace.\nFor connecting to a local or self-hosted Dgraph database, use the namespace and user credentials for that namespace.\nExample sources: my-dgraph-source: kind: dgraph dgraphUrl: https://xxxx.cloud.dgraph.io user: ${USER_NAME} password: ${PASSWORD} apiKey: ${API_KEY} namespace : 0 Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference Field Type Required Description kind string true Must be “dgraph”. dgraphUrl string true Connection URI (e.g. “https://xxx.cloud.dgraph.io”, “https://localhost:8080”). user string false Name of the Dgraph user to connect as (e.g., “groot”). password string false Password of the Dgraph user (e.g., “password”). apiKey string false API key to connect to a Dgraph Cloud instance. namespace uint64 false Dgraph namespace (not required for Dgraph Cloud Shared Clusters). ","categories":"","description":"Dgraph is fully open-source, built-for-scale graph database for Gen AI workloads\n","excerpt":"Dgraph is fully open-source, built-for-scale graph database for Gen AI …","ref":"/genai-toolbox/resources/sources/dgraph/","tags":"","title":"Dgraph"},{"body":"","categories":"","description":"Tools that work with Dgraph Sources.\n","excerpt":"Tools that work with Dgraph Sources.\n","ref":"/genai-toolbox/resources/tools/dgraph/","tags":"","title":"Dgraph"},{"body":"About A dgraph-dql tool executes a pre-defined DQL statement against a Dgraph database. It’s compatible with any of the following sources:\ndgraph To run a statement as a query, you need to set the config isQuery=true. For upserts or mutations, set isQuery=false. You can also configure timeout for a query.\nNote: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\nExample Query Mutation tools: search_user: kind: dgraph-dql source: my-dgraph-source statement: | query all($role: string){ users(func: has(name)) @filter(eq(role, $role) AND ge(age, 30) AND le(age, 50)) { uid name email role age } } isQuery: true timeout: 20s description: | Use this tool to retrieve the details of users who are admins and are between 30 and 50 years old. The query returns the user's name, email, role, and age. This can be helpful when you want to fetch admin users within a specific age range. Example: Fetch admins aged between 30 and 50: [ { \"name\": \"Alice\", \"role\": \"admin\", \"age\": 35 }, { \"name\": \"Bob\", \"role\": \"admin\", \"age\": 45 } ] parameters: - name: $role type: string description: admin tools: dgraph-manage-user-instance: kind: dgraph-dql source: my-dgraph-source isQuery: false statement: | { set { _:user1 \u003cname\u003e $user1 . _:user1 \u003cemail\u003e $email1 . _:user1 \u003crole\u003e \"admin\" . _:user1 \u003cage\u003e \"35\" . _:user2 \u003cname\u003e $user2 . _:user2 \u003cemail\u003e $email2 . _:user2 \u003crole\u003e \"admin\" . _:user2 \u003cage\u003e \"45\" . } } description: | Use this tool to insert or update user data into the Dgraph database. The mutation adds or updates user details like name, email, role, and age. Example: Add users Alice and Bob as admins with specific ages. parameters: - name: user1 type: string description: Alice - name: email1 type: string description: alice@email.com - name: user2 type: string description: Bob - name: email2 type: string description: bob@email.com Reference field type required description kind string true Must be “dgraph-dql”. source string true Name of the source the dql query should execute on. description string true Description of the tool that is passed to the LLM. statement string true dql statement to execute isQuery boolean false To run statement as query set true otherwise false timeout string false To set timeout for query parameters parameters false List of parameters that will be used with the dql statement. ","categories":"","description":"A \"dgraph-dql\" tool executes a pre-defined DQL statement against a Dgraph database.\n","excerpt":"A \"dgraph-dql\" tool executes a pre-defined DQL statement against a …","ref":"/genai-toolbox/resources/tools/dgraph/dgraph-dql/","tags":"","title":"dgraph-dql"},{"body":" ","categories":"","description":"All of Toolbox's documentation.\n","excerpt":"All of Toolbox's documentation.\n","ref":"/genai-toolbox/","tags":"","title":"Documentation"},{"body":"Firestore Source Firestore is a NoSQL document database built for automatic scaling, high performance, and ease of application development. While the Firestore interface has many of the same features as traditional databases, as a NoSQL database it differs from them in the way it describes relationships between data objects.\nIf you are new to Firestore, you can create a database and learn the basics.\nRequirements IAM Permissions Firestore uses Identity and Access Management (IAM) to control user and group access to Firestore resources. Toolbox will use your Application Default Credentials (ADC) to authorize and authenticate when interacting with Firestore.\nIn addition to setting the ADC for your server, you need to ensure the IAM identity has been given the correct IAM permissions for accessing Firestore. Common roles include:\nroles/datastore.user - Read and write access to Firestore roles/datastore.viewer - Read-only access to Firestore roles/firebaserules.admin - Full management of Firebase Security Rules for Firestore. This role is required for operations that involve creating, updating, or managing Firestore security rules (see Firebase Security Rules roles) See Firestore access control for more information on applying IAM permissions and roles to an identity.\nDatabase Selection Firestore allows you to create multiple databases within a single project. Each database is isolated from the others and has its own set of documents and collections. If you don’t specify a database in your configuration, the default database named (default) will be used.\nExample sources: my-firestore-source: kind: \"firestore\" project: \"my-project-id\" # database: \"my-database\" # Optional, defaults to \"(default)\" Reference field type required description kind string true Must be “firestore”. project string true Id of the GCP project that contains the Firestore database (e.g. “my-project-id”). database string false Name of the Firestore database to connect to. Defaults to “(default)” if not specified. ","categories":"","description":"Firestore is a NoSQL document database built for automatic scaling, high performance, and ease of application development. It's a fully managed, serverless database that supports mobile, web, and server development.\n","excerpt":"Firestore is a NoSQL document database built for automatic scaling, …","ref":"/genai-toolbox/resources/sources/firestore/","tags":"","title":"Firestore"},{"body":"","categories":"","description":"Tools that work with Firestore Sources.\n","excerpt":"Tools that work with Firestore Sources.\n","ref":"/genai-toolbox/resources/tools/firestore/","tags":"","title":"Firestore"},{"body":"About A firestore-delete-documents tool deletes multiple documents from Firestore by their paths. It’s compatible with the following sources:\nfirestore firestore-delete-documents takes one input parameter documentPaths which is an array of document paths to delete. The tool uses Firestore’s BulkWriter for efficient batch deletion and returns the success status for each document.\nExample tools: delete_user_documents: kind: firestore-delete-documents source: my-firestore-source description: Use this tool to delete multiple documents from Firestore. Reference field type required description kind string true Must be “firestore-delete-documents”. source string true Name of the Firestore source to delete documents from. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"firestore-delete-documents\" tool deletes multiple documents from Firestore by their paths.\n","excerpt":"A \"firestore-delete-documents\" tool deletes multiple documents from …","ref":"/genai-toolbox/resources/tools/firestore/firestore-delete-documents/","tags":"","title":"firestore-delete-documents"},{"body":"About A firestore-get-documents tool retrieves multiple documents from Firestore by their paths. It’s compatible with the following sources:\nfirestore firestore-get-documents takes one input parameter documentPaths which is an array of document paths, and returns the documents’ data along with metadata such as existence status, creation time, update time, and read time.\nExample tools: get_user_documents: kind: firestore-get-documents source: my-firestore-source description: Use this tool to retrieve multiple documents from Firestore. Reference field type required description kind string true Must be “firestore-get-documents”. source string true Name of the Firestore source to retrieve documents from. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"firestore-get-documents\" tool retrieves multiple documents from Firestore by their paths.\n","excerpt":"A \"firestore-get-documents\" tool retrieves multiple documents from …","ref":"/genai-toolbox/resources/tools/firestore/firestore-get-documents/","tags":"","title":"firestore-get-documents"},{"body":"About A firestore-get-rules tool retrieves the active Firestore security rules for the current project. It’s compatible with the following sources:\nfirestore firestore-get-rules takes no input parameters and returns the security rules content along with metadata such as the ruleset name, and timestamps.\nExample tools: get_firestore_rules: kind: firestore-get-rules source: my-firestore-source description: Use this tool to retrieve the active Firestore security rules. Reference field type required description kind string true Must be “firestore-get-rules”. source string true Name of the Firestore source to retrieve rules from. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"firestore-get-rules\" tool retrieves the active Firestore security rules for the current project.\n","excerpt":"A \"firestore-get-rules\" tool retrieves the active Firestore security …","ref":"/genai-toolbox/resources/tools/firestore/firestore-get-rules/","tags":"","title":"firestore-get-rules"},{"body":"About A firestore-list-collections tool lists collections in Firestore, either at the root level or as subcollections of a specific document. It’s compatible with the following sources:\nfirestore firestore-list-collections takes an optional parentPath parameter to specify a document path. If provided, it lists all subcollections of that document. If not provided, it lists all root-level collections in the database.\nExample tools: list_firestore_collections: kind: firestore-list-collections source: my-firestore-source description: Use this tool to list collections in Firestore. Reference field type required description kind string true Must be “firestore-list-collections”. source string true Name of the Firestore source to list collections from. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"firestore-list-collections\" tool lists collections in Firestore, either at the root level or as subcollections of a document.\n","excerpt":"A \"firestore-list-collections\" tool lists collections in Firestore, …","ref":"/genai-toolbox/resources/tools/firestore/firestore-list-collections/","tags":"","title":"firestore-list-collections"},{"body":"About The firestore-query-collection tool allows you to query Firestore collections with filters, ordering, and limit capabilities.\nConfiguration To use this tool, you need to configure it in your YAML configuration file:\nsources: my-firestore: kind: firestore project: my-gcp-project database: \"(default)\" tools: query_collection: kind: firestore-query-collection source: my-firestore description: Query Firestore collections with advanced filtering Parameters parameters type required default description collectionPath string true - The Firestore Rules source code to validate filters array false - Array of filter objects (as JSON strings) to apply to the query orderBy string false - JSON string specifying field and direction to order results limit integer false 100 Maximum number of documents to return analyzeQuery boolean false false If true, returns query explain metrics including execution statistics Filter Format Each filter in the filters array should be a JSON string with the following structure:\n{ \"field\": \"fieldName\", \"op\": \"operator\", \"value\": \"compareValue\" } Supported operators:\n\u003c - Less than \u003c= - Less than or equal to \u003e - Greater than \u003e= - Greater than or equal to == - Equal to != - Not equal to array-contains - Array contains a specific value array-contains-any - Array contains any of the specified values in - Field value is in the specified array not-in - Field value is not in the specified array Value types supported:\nString: \"value\": \"text\" Number: \"value\": 123 or \"value\": 45.67 Boolean: \"value\": true or \"value\": false Array: \"value\": [\"item1\", \"item2\"] (for in, not-in, array-contains-any operators) OrderBy Format The orderBy parameter should be a JSON string with the following structure:\n{ \"field\": \"fieldName\", \"direction\": \"ASCENDING\" } Direction values:\nASCENDING DESCENDING Example Usage Query with filters { \"collectionPath\": \"users\", \"filters\": [ \"{\\\"field\\\": \\\"age\\\", \\\"op\\\": \\\"\u003e\\\", \\\"value\\\": 18}\", \"{\\\"field\\\": \\\"status\\\", \\\"op\\\": \\\"==\\\", \\\"value\\\": \\\"active\\\"}\" ], \"orderBy\": \"{\\\"field\\\": \\\"createdAt\\\", \\\"direction\\\": \\\"DESCENDING\\\"}\", \"limit\": 50 } Query with array contains filter { \"collectionPath\": \"products\", \"filters\": [ \"{\\\"field\\\": \\\"categories\\\", \\\"op\\\": \\\"array-contains\\\", \\\"value\\\": \\\"electronics\\\"}\", \"{\\\"field\\\": \\\"price\\\", \\\"op\\\": \\\"\u003c\\\", \\\"value\\\": 1000}\" ], \"orderBy\": \"{\\\"field\\\": \\\"price\\\", \\\"direction\\\": \\\"ASCENDING\\\"}\", \"limit\": 20 } Query with IN operator { \"collectionPath\": \"orders\", \"filters\": [ \"{\\\"field\\\": \\\"status\\\", \\\"op\\\": \\\"in\\\", \\\"value\\\": [\\\"pending\\\", \\\"processing\\\"]}\" ], \"limit\": 100 } Query with explain metrics { \"collectionPath\": \"users\", \"filters\": [ \"{\\\"field\\\": \\\"age\\\", \\\"op\\\": \\\"\u003e=\\\", \\\"value\\\": 21}\", \"{\\\"field\\\": \\\"active\\\", \\\"op\\\": \\\"==\\\", \\\"value\\\": true}\" ], \"orderBy\": \"{\\\"field\\\": \\\"lastLogin\\\", \\\"direction\\\": \\\"DESCENDING\\\"}\", \"limit\": 25, \"analyzeQuery\": true } Response Format Standard Response (analyzeQuery = false) The tool returns an array of documents, where each document includes:\n{ \"id\": \"documentId\", \"path\": \"collection/documentId\", \"data\": { // Document fields }, \"createTime\": \"2025-01-07T12:00:00Z\", \"updateTime\": \"2025-01-07T12:00:00Z\", \"readTime\": \"2025-01-07T12:00:00Z\" } Response with Query Analysis (analyzeQuery = true) When analyzeQuery is set to true, the tool returns a single object containing documents and explain metrics:\n{ \"documents\": [ // Array of document objects as shown above ], \"explainMetrics\": { \"planSummary\": { \"indexesUsed\": [ { \"query_scope\": \"Collection\", \"properties\": \"(field ASC, __name__ ASC)\" } ] }, \"executionStats\": { \"resultsReturned\": 50, \"readOperations\": 50, \"executionDuration\": \"120ms\", \"debugStats\": { \"indexes_entries_scanned\": \"1000\", \"documents_scanned\": \"50\", \"billing_details\": { \"documents_billable\": \"50\", \"index_entries_billable\": \"1000\", \"min_query_cost\": \"0\" } } } } } Error Handling The tool will return errors for:\nInvalid collection path Malformed filter JSON Unsupported operators Query execution failures Invalid orderBy format ","categories":"","description":"A \"firestore-query-collection\" tool allow to query collections in Firestore.\n","excerpt":"A \"firestore-query-collection\" tool allow to query collections in …","ref":"/genai-toolbox/resources/tools/firestore/firestore-query-collection/","tags":"","title":"firestore-query-collection"},{"body":"Overview The firestore-validate-rules tool validates Firestore security rules syntax and semantic correctness without deploying them. It provides detailed error reporting with source positions and code snippets.\nConfiguration tools: firestore-validate-rules: kind: firestore-validate-rules source: \u003cfirestore-source-name\u003e description: \"Checks the provided Firestore Rules source for syntax and validation errors\" Authentication This tool requires authentication if the source requires authentication.\nParameters parameters type required description source string true The Firestore Rules source code to validate Response The tool returns a ValidationResult object containing:\n{ \"valid\": \"boolean\", \"issueCount\": \"number\", \"formattedIssues\": \"string\", \"rawIssues\": [ { \"sourcePosition\": { \"fileName\": \"string\", \"line\": \"number\", \"column\": \"number\", \"currentOffset\": \"number\", \"endOffset\": \"number\" }, \"description\": \"string\", \"severity\": \"string\" } ] } Example Usage Validate simple rules { \"source\": \"rules_version = '2';\\nservice cloud.firestore {\\n match /databases/{database}/documents {\\n match /{document=**} {\\n allow read, write: if true;\\n }\\n }\\n}\" } Example response for valid rules { \"valid\": true, \"issueCount\": 0, \"formattedIssues\": \"✓ No errors detected. Rules are valid.\" } Example response with errors { \"valid\": false, \"issueCount\": 1, \"formattedIssues\": \"Found 1 issue(s) in rules source:\\n\\nERROR: Unexpected token ';' [Ln 4, Col 32]\\n```\\n allow read, write: if true;;\\n ^\\n```\", \"rawIssues\": [ { \"sourcePosition\": { \"line\": 4, \"column\": 32, \"currentOffset\": 105, \"endOffset\": 106 }, \"description\": \"Unexpected token ';'\", \"severity\": \"ERROR\" } ] } Error Handling The tool will return errors for:\nMissing or empty source parameter API errors when calling the Firebase Rules service Network connectivity issues Use Cases Pre-deployment validation: Validate rules before deploying to production CI/CD integration: Integrate rules validation into your build pipeline Development workflow: Quickly check rules syntax while developing Error debugging: Get detailed error locations with code snippets Related Tools firestore-get-rules: Retrieve current active rules firestore-query-collection: Test rules by querying collections ","categories":"","description":"A \"firestore-validate-rules\" tool validates Firestore security rules syntax and semantic correctness without deploying them. It provides detailed error reporting with source positions and code snippets.\n","excerpt":"A \"firestore-validate-rules\" tool validates Firestore security rules …","ref":"/genai-toolbox/resources/tools/firestore/firestore-validate-rules/","tags":"","title":"firestore-validate-rules"},{"body":"","categories":"","description":"How to get started with Toolbox.\n","excerpt":"How to get started with Toolbox.\n","ref":"/genai-toolbox/getting-started/","tags":"","title":"Getting Started"},{"body":"Getting Started Google Sign-In manages the OAuth 2.0 flow and token lifecycle. To integrate the Google Sign-In workflow to your web app follow this guide.\nAfter setting up the Google Sign-In workflow, you should have registered your application and retrieved a Client ID. Configure your auth service in with the Client ID.\nBehavior Authorized Invocations When using Authorized Invocations, a tool will be considered authorized if it has a valid Oauth 2.0 token that matches the Client ID.\nAuthenticated Parameters When using Authenticated Parameters, any claim provided by the id-token can be used for the parameter.\nExample authServices: my-google-auth: kind: google clientId: ${YOUR_GOOGLE_CLIENT_ID} Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “google”. clientId string true Client ID of your application from registering your application. ","categories":"","description":"Use Google Sign-In for Oauth 2.0 flow and token lifecycle.\n","excerpt":"Use Google Sign-In for Oauth 2.0 flow and token lifecycle.\n","ref":"/genai-toolbox/resources/authservices/google/","tags":"","title":"Google Sign-In"},{"body":"About The http tool allows you to make HTTP requests to APIs to retrieve data. An HTTP request is the method by which a client communicates with a server to retrieve or manipulate resources. Toolbox allows you to configure the request URL, method, headers, query parameters, and the request body for an HTTP Tool.\nURL An HTTP request URL identifies the target the client wants to access. Toolbox composes the request URL from 3 places:\nThe HTTP Source’s baseUrl. The HTTP Tool’s path field. The HTTP Tool’s pathParams for dynamic path composed during Tool invocation. For example, the following config allows you to reach different paths of the same server using multiple Tools:\nsources: my-http-source: kind: http baseUrl: https://api.example.com tools: my-post-tool: kind: http source: my-http-source method: POST path: /update description: Tool to update information to the example API my-get-tool: kind: http source: my-http-source method: GET path: /search description: Tool to search information from the example API my-dynamic-path-tool: kind: http source: my-http-source method: GET path: /{{.myPathParam}}/search description: Tool to reach endpoint based on the input to `myPathParam` pathParams: - name: myPathParam type: string description: The dynamic path parameter Headers An HTTP request header is a key-value pair sent by a client to a server, providing additional information about the request, such as the client’s preferences, the request body content type, and other metadata. Headers specified by the HTTP Tool are combined with its HTTP Source headers for the resulting HTTP request, and override the Source headers in case of conflict. The HTTP Tool allows you to specify headers in two different ways:\nStatic headers can be specified using the headers field, and will be the same for every invocation: my-http-tool: kind: http source: my-http-source method: GET path: /search description: Tool to search data from API headers: Authorization: API_KEY Content-Type: application/json Dynamic headers can be specified as parameters in the headerParams field. The name of the headerParams will be used as the header key, and the value is determined by the LLM input upon Tool invocation: my-http-tool: kind: http source: my-http-source method: GET path: /search description: some description headerParams: - name: Content-Type # Example LLM input: \"application/json\" description: request content type type: string Query parameters Query parameters are key-value pairs appended to a URL after a question mark (?) to provide additional information to the server for processing the request, like filtering or sorting data.\nStatic request query parameters should be specified in the path as part of the URL itself: my-http-tool: kind: http source: my-http-source method: GET path: /search?language=en\u0026id=1 description: Tool to search for item with ID 1 in English Dynamic request query parameters should be specified as parameters in the queryParams section: my-http-tool: kind: http source: my-http-source method: GET path: /search description: Tool to search for item with ID queryParams: - name: id description: item ID type: integer Request body The request body payload is a string that supports parameter replacement following Go template’s annotations. The parameter names in the requestBody should be preceded by “.” and enclosed by double curly brackets “{{}}”. The values will be populated into the request body payload upon Tool invocation.\nExample:\nmy-http-tool: kind: http source: my-http-source method: GET path: /search description: Tool to search for person with name and age requestBody: | { \"age\": {{.age}}, \"name\": \"{{.name}}\" } bodyParams: - name: age description: age number type: integer - name: name description: name string type: string Formatting Parameters Some complex parameters (such as arrays) may require additional formatting to match the expected output. For convenience, you can specify one of the following pre-defined functions before the parameter name to format it:\nJSON The json keyword converts a parameter into a JSON format.\nNote\nUsing JSON may add quotes to the variable name for certain types (such as strings).\nExample:\nrequestBody: | { \"age\": {{json .age}}, \"name\": {{json .name}}, \"nickname\": \"{{json .nickname}}\", \"nameArray\": {{json .nameArray}} } will send the following output:\n{ \"age\": 18, \"name\": \"Katherine\", \"nickname\": \"\"Kat\"\", # Duplicate quotes \"nameArray\": [\"A\", \"B\", \"C\"] } Example my-http-tool: kind: http source: my-http-source method: GET path: /search description: some description authRequired: - my-google-auth-service - other-auth-service queryParams: - name: country description: some description type: string requestBody: | { \"age\": {{.age}}, \"city\": \"{{.city}}\" } bodyParams: - name: age description: age number type: integer - name: city description: city string type: string headers: Authorization: API_KEY Content-Type: application/json headerParams: - name: Language description: language string type: string Reference field type required description kind string true Must be “http”. source string true Name of the source the HTTP request should be sent to. description string true Description of the tool that is passed to the LLM. path string true The path of the HTTP request. You can include static query parameters in the path string. method string true The HTTP method to use (e.g., GET, POST, PUT, DELETE). headers map[string]string false A map of headers to include in the HTTP request (overrides source headers). requestBody string false The request body payload. Use go template with the parameter name as the placeholder (e.g., {{.id}} will be replaced with the value of the parameter that has name id in the bodyParams section). queryParams parameters false List of parameters that will be inserted into the query string. bodyParams parameters false List of parameters that will be inserted into the request body payload. headerParams parameters false List of parameters that will be inserted as the request headers. ","categories":"","description":"A \"http\" tool sends out an HTTP request to an HTTP endpoint.\n","excerpt":"A \"http\" tool sends out an HTTP request to an HTTP endpoint.\n","ref":"/genai-toolbox/resources/tools/http/http/","tags":"","title":"http"},{"body":"About The HTTP Source allows Toolbox to retrieve data from arbitrary HTTP endpoints. This enables Generative AI applications to access data from web APIs and other HTTP-accessible resources.\nAvailable Tools http\nMake HTTP requests to REST APIs or other web services. Example sources: my-http-source: kind: http baseUrl: https://api.example.com/data timeout: 10s # default to 30s headers: Authorization: Bearer ${API_KEY} Content-Type: application/json queryParams: param1: value1 param2: value2 # disableSslVerification: false Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “http”. baseUrl string true The base URL for the HTTP requests (e.g., https://api.example.com). timeout string false The timeout for HTTP requests (e.g., “5s”, “1m”, refer to ParseDuration for more examples). Defaults to 30s. headers map[string]string false Default headers to include in the HTTP requests. queryParams map[string]string false Default query parameters to include in the HTTP requests. disableSslVerification bool false Disable SSL certificate verification. This should only be used for local development. Defaults to false. ","categories":"","description":"The HTTP source enables the Toolbox to retrieve data from a remote server using HTTP requests.\n","excerpt":"The HTTP source enables the Toolbox to retrieve data from a remote …","ref":"/genai-toolbox/resources/sources/http/","tags":"","title":"HTTP"},{"body":"","categories":"","description":"Tools that work with HTTP Sources.\n","excerpt":"Tools that work with HTTP Sources.\n","ref":"/genai-toolbox/resources/tools/http/","tags":"","title":"HTTP"},{"body":"MCP Toolbox for Databases is an open source MCP server for databases. It enables you to develop tools easier, faster, and more securely by handling the complexities such as connection pooling, authentication, and more.\nNote\nThis solution was originally named “Gen AI Toolbox for Databases” as its initial development predated MCP, but was renamed to align with recently added MCP compatibility.\nWhy Toolbox? Toolbox helps you build Gen AI tools that let your agents access data in your database. Toolbox provides:\nSimplified development: Integrate tools to your agent in less than 10 lines of code, reuse tools between multiple agents or frameworks, and deploy new versions of tools more easily. Better performance: Best practices such as connection pooling, authentication, and more. Enhanced security: Integrated auth for more secure access to your data End-to-end observability: Out of the box metrics and tracing with built-in support for OpenTelemetry. ⚡ Supercharge Your Workflow with an AI Database Assistant ⚡\nStop context-switching and let your AI assistant become a true co-developer. By connecting your IDE to your databases with MCP Toolbox, you can delegate complex and time-consuming database tasks, allowing you to build faster and focus on what matters. This isn’t just about code completion; it’s about giving your AI the context it needs to handle the entire development lifecycle.\nHere’s how it will save you time:\nQuery in Plain English: Interact with your data using natural language right from your IDE. Ask complex questions like, “How many orders were delivered in 2024, and what items were in them?” without writing any SQL. Automate Database Management: Simply describe your data needs, and let the AI assistant manage your database for you. It can handle generating queries, creating tables, adding indexes, and more. Generate Context-Aware Code: Empower your AI assistant to generate application code and tests with a deep understanding of your real-time database schema. This accelerates the development cycle by ensuring the generated code is directly usable. Slash Development Overhead: Radically reduce the time spent on manual setup and boilerplate. MCP Toolbox helps streamline lengthy database configurations, repetitive code, and error-prone schema migrations. Learn how to connect your AI tools (IDEs) to Toolbox using MCP.\nGeneral Architecture Toolbox sits between your application’s orchestration framework and your database, providing a control plane that is used to modify, distribute, or invoke tools. It simplifies the management of your tools by providing you with a centralized location to store and update tools, allowing you to share tools between agents and applications and update those tools without necessarily redeploying your application.\nGetting Started Installing the server For the latest version, check the releases page and use the following instructions for your OS and CPU architecture.\nBinary Container image Homebrew Compile from source To install Toolbox as a binary:\n# see releases page for other versions export VERSION=0.11.0 curl -O https://storage.googleapis.com/genai-toolbox/v$VERSION/linux/amd64/toolbox chmod +x toolbox You can also install Toolbox as a container:\n# see releases page for other versions export VERSION=0.11.0 docker pull us-central1-docker.pkg.dev/database-toolbox/toolbox/toolbox:$VERSION To install Toolbox using Homebrew on macOS or Linux:\nbrew install mcp-toolbox To install from source, ensure you have the latest version of Go installed, and then run the following command:\ngo install github.com/googleapis/genai-toolbox@v0.11.0 Running the server Configure a tools.yaml to define your tools, and then execute toolbox to start the server:\n./toolbox --tools-file \"tools.yaml\" Note\nToolbox enables dynamic reloading by default. To disable, use the --disable-reload flag.\nLaunching Toolbox UI To launch Toolbox’s interactive UI, use the --ui flag. This allows you to test tools and toolsets with features such as authorized parameters. To learn more, visit Toolbox UI.\n./toolbox --ui Homebrew Users If you installed Toolbox using Homebrew, the toolbox binary is available in your system path. You can start the server with the same command:\ntoolbox --tools-file \"tools.yaml\" You can use toolbox help for a full list of flags! To stop the server, send a terminate signal (ctrl+c on most platforms).\nFor more detailed documentation on deploying to different environments, check out the resources in the How-to section\nIntegrating your application Once your server is up and running, you can load the tools into your application. See below the list of Client SDKs for using various frameworks:\nPython Core LangChain Llamaindex Once you’ve installed the Toolbox Core SDK, you can load tools:\nfrom toolbox_core import ToolboxClient # update the url to point to your server async with ToolboxClient(\"http://127.0.0.1:5000\") as client: # these tools can be passed to your application! tools = await client.load_toolset(\"toolset_name\") For more detailed instructions on using the Toolbox Core SDK, see the project’s README.\nOnce you’ve installed the Toolbox LangChain SDK, you can load tools:\nfrom toolbox_langchain import ToolboxClient # update the url to point to your server async with ToolboxClient(\"http://127.0.0.1:5000\") as client: # these tools can be passed to your application! tools = client.load_toolset() For more detailed instructions on using the Toolbox LangChain SDK, see the project’s README.\nOnce you’ve installed the Toolbox Llamaindex SDK, you can load tools:\nfrom toolbox_llamaindex import ToolboxClient # update the url to point to your server async with ToolboxClient(\"http://127.0.0.1:5000\") as client: # these tools can be passed to your application tools = client.load_toolset() For more detailed instructions on using the Toolbox Llamaindex SDK, see the project’s README.\nJavascript/Typescript Once you’ve installed the Toolbox Core SDK, you can load tools:\nCore LangChain/Langraph Genkit LlamaIndex import { ToolboxClient } from '@toolbox-sdk/core'; // update the url to point to your server const URL = 'http://127.0.0.1:5000'; let client = new ToolboxClient(URL); // these tools can be passed to your application! const toolboxTools = await client.loadToolset('toolsetName'); import { ToolboxClient } from '@toolbox-sdk/core'; // update the url to point to your server const URL = 'http://127.0.0.1:5000'; let client = new ToolboxClient(URL); // these tools can be passed to your application! const toolboxTools = await client.loadToolset('toolsetName'); // Define the basics of the tool: name, description, schema and core logic const getTool = (toolboxTool) =\u003e tool(currTool, { name: toolboxTool.getName(), description: toolboxTool.getDescription(), schema: toolboxTool.getParamSchema() }); // Use these tools in your Langchain/Langraph applications const tools = toolboxTools.map(getTool); import { ToolboxClient } from '@toolbox-sdk/core'; import { genkit } from 'genkit'; // Initialise genkit const ai = genkit({ plugins: [ googleAI({ apiKey: process.env.GEMINI_API_KEY || process.env.GOOGLE_API_KEY }) ], model: googleAI.model('gemini-2.0-flash'), }); // update the url to point to your server const URL = 'http://127.0.0.1:5000'; let client = new ToolboxClient(URL); // these tools can be passed to your application! const toolboxTools = await client.loadToolset('toolsetName'); // Define the basics of the tool: name, description, schema and core logic const getTool = (toolboxTool) =\u003e ai.defineTool({ name: toolboxTool.getName(), description: toolboxTool.getDescription(), schema: toolboxTool.getParamSchema() }, toolboxTool) // Use these tools in your Genkit applications const tools = toolboxTools.map(getTool); import { ToolboxClient } from '@toolbox-sdk/core'; import { tool } from \"llamaindex\"; // update the url to point to your server const URL = 'http://127.0.0.1:5000'; let client = new ToolboxClient(URL); // these tools can be passed to your application! const toolboxTools = await client.loadToolset('toolsetName'); // Define the basics of the tool: name, description, schema and core logic const getTool = (toolboxTool) =\u003e tool({ name: toolboxTool.getName(), description: toolboxTool.getDescription(), parameters: toolboxTool.getParamSchema(), execute: toolboxTool });; // Use these tools in your LlamaIndex applications const tools = toolboxTools.map(getTool); For more detailed instructions on using the Toolbox Core SDK, see the project’s README.\nGo Once you’ve installed the Toolbox Go SDK, you can load tools:\nCore LangChain Go Genkit Go Go GenAI OpenAI Go package main import ( \"context\" \"log\" \"github.com/googleapis/mcp-toolbox-sdk-go/core\" ) func main() { // update the url to point to your server URL := \"http://127.0.0.1:5000\" ctx := context.Background() client, err := core.NewToolboxClient(URL) if err != nil { log.Fatalf(\"Failed to create Toolbox client: %v\", err) } // Framework agnostic tools tools, err := client.LoadToolset(\"toolsetName\", ctx) if err != nil { log.Fatalf(\"Failed to load tools: %v\", err) } } package main import ( \"context\" \"encoding/json\" \"log\" \"github.com/googleapis/mcp-toolbox-sdk-go/core\" \"github.com/tmc/langchaingo/llms\" ) func main() { // Make sure to add the error checks // update the url to point to your server URL := \"http://127.0.0.1:5000\" ctx := context.Background() client, err := core.NewToolboxClient(URL) if err != nil { log.Fatalf(\"Failed to create Toolbox client: %v\", err) } // Framework agnostic tool tool, err := client.LoadTool(\"toolName\", ctx) if err != nil { log.Fatalf(\"Failed to load tools: %v\", err) } // Fetch the tool's input schema inputschema, err := tool.InputSchema() if err != nil { log.Fatalf(\"Failed to fetch inputSchema: %v\", err) } var paramsSchema map[string]any _ = json.Unmarshal(inputschema, \u0026paramsSchema) // Use this tool with LangChainGo langChainTool := llms.Tool{ Type: \"function\", Function: \u0026llms.FunctionDefinition{ Name: tool.Name(), Description: tool.Description(), Parameters: paramsSchema, }, } } package main import ( \"context\" \"encoding/json\" \"log\" \"github.com/firebase/genkit/go/ai\" \"github.com/firebase/genkit/go/genkit\" \"github.com/googleapis/mcp-toolbox-sdk-go/core\" \"github.com/googleapis/mcp-toolbox-sdk-go/tbgenkit\" \"github.com/invopop/jsonschema\" ) func main() { // Make sure to add the error checks // Update the url to point to your server URL := \"http://127.0.0.1:5000\" ctx := context.Background() g, err := genkit.Init(ctx) client, err := core.NewToolboxClient(URL) if err != nil { log.Fatalf(\"Failed to create Toolbox client: %v\", err) } // Framework agnostic tool tool, err := client.LoadTool(\"toolName\", ctx) if err != nil { log.Fatalf(\"Failed to load tools: %v\", err) } // Convert the tool using the tbgenkit package // Use this tool with Genkit Go genkitTool, err := tbgenkit.ToGenkitTool(tool, g) if err != nil { log.Fatalf(\"Failed to convert tool: %v\\n\", err) } } package main import ( \"context\" \"encoding/json\" \"log\" \"github.com/googleapis/mcp-toolbox-sdk-go/core\" \"google.golang.org/genai\" ) func main() { // Make sure to add the error checks // Update the url to point to your server URL := \"http://127.0.0.1:5000\" ctx := context.Background() client, err := core.NewToolboxClient(URL) if err != nil { log.Fatalf(\"Failed to create Toolbox client: %v\", err) } // Framework agnostic tool tool, err := client.LoadTool(\"toolName\", ctx) if err != nil { log.Fatalf(\"Failed to load tools: %v\", err) } // Fetch the tool's input schema inputschema, err := tool.InputSchema() if err != nil { log.Fatalf(\"Failed to fetch inputSchema: %v\", err) } var schema *genai.Schema _ = json.Unmarshal(inputschema, \u0026schema) funcDeclaration := \u0026genai.FunctionDeclaration{ Name: tool.Name(), Description: tool.Description(), Parameters: schema, } // Use this tool with Go GenAI genAITool := \u0026genai.Tool{ FunctionDeclarations: []*genai.FunctionDeclaration{funcDeclaration}, } } package main import ( \"context\" \"encoding/json\" \"log\" \"github.com/googleapis/mcp-toolbox-sdk-go/core\" openai \"github.com/openai/openai-go\" ) func main() { // Make sure to add the error checks // Update the url to point to your server URL := \"http://127.0.0.1:5000\" ctx := context.Background() client, err := core.NewToolboxClient(URL) if err != nil { log.Fatalf(\"Failed to create Toolbox client: %v\", err) } // Framework agnostic tool tool, err := client.LoadTool(\"toolName\", ctx) if err != nil { log.Fatalf(\"Failed to load tools: %v\", err) } // Fetch the tool's input schema inputschema, err := tool.InputSchema() if err != nil { log.Fatalf(\"Failed to fetch inputSchema: %v\", err) } var paramsSchema openai.FunctionParameters _ = json.Unmarshal(inputschema, \u0026paramsSchema) // Use this tool with OpenAI Go openAITool := openai.ChatCompletionToolParam{ Function: openai.FunctionDefinitionParam{ Name: tool.Name(), Description: openai.String(tool.Description()), Parameters: paramsSchema, }, } } For more detailed instructions on using the Toolbox Go SDK, see the project’s README.\nFor end-to-end samples on using the Toolbox Go SDK with orchestration frameworks, see the project’s samples\n","categories":"","description":"An introduction to MCP Toolbox for Databases.\n","excerpt":"An introduction to MCP Toolbox for Databases.\n","ref":"/genai-toolbox/getting-started/introduction/","tags":"","title":"Introduction"},{"body":"About Looker is a web based business intelligence and data management tool that provides a semantic layer to facilitate querying. It can be deployed in the cloud, on GCP, or on premises.\nRequirements Database User This source only uses API authentication. You will need to create an API user to login to Looker.\nExample sources: my-looker-source: kind: looker base_url: http://looker.example.com client_id: ${LOOKER_CLIENT_ID} client_secret: ${LOOKER_CLIENT_SECRET} verify_ssl: true timeout: 600s The Looker base url will look like “https://looker.example.com”, don’t include a trailing “/”. In some cases, especially if your Looker is deployed on-premises, you may need to add the API port numner like “https://looker.example.com:19999”.\nVerify ssl should almost always be “true” (all lower case) unless you are using a self-signed ssl certificate for the Looker server. Anything other than “true” will be interpretted as false.\nThe client id and client secret are seemingly random character sequences assigned by the looker server.\nTip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “looker”. base_url string true The URL of your Looker server with no trailing /). client_id string true The client id assigned by Looker. client_secret string true The client secret assigned by Looker. verify_ssl string true Whether to check the ssl certificate of the server. timeout string false Maximum time to wait for query execution (e.g. “30s”, “2m”). By default, 120s is applied. ","categories":"","description":"Looker is a business intelligence tool that also provides a semantic layer.\n","excerpt":"Looker is a business intelligence tool that also provides a semantic …","ref":"/genai-toolbox/resources/sources/looker/","tags":"","title":"Looker"},{"body":"","categories":"","description":"Tools that work with Looker Sources.\n","excerpt":"Tools that work with Looker Sources.\n","ref":"/genai-toolbox/resources/tools/looker/","tags":"","title":"Looker"},{"body":"","categories":"","description":"How to get started with Toolbox using Looker.\n","excerpt":"How to get started with Toolbox using Looker.\n","ref":"/genai-toolbox/samples/looker/","tags":"","title":"Looker"},{"body":"About A looker-get-dimensions tool returns all the dimensions from a given explore in a given mode in the source.\nIt’s compatible with the following sources:\nlooker looker-get-dimensions accepts two parameters, the model and the explore.\nExample tools: get_dimensions: kind: looker-get-dimensions source: looker-source description: | The get_dimensions tool retrieves the list of dimensions defined in an explore. It takes two parameters, the model_name looked up from get_models and the explore_name looked up from get_explores. Reference field type required description kind string true Must be “looker-get-dimensions”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"looker-get-dimensions\" tool returns all the dimensions from a given explore in a given model in the source.\n","excerpt":"A \"looker-get-dimensions\" tool returns all the dimensions from a given …","ref":"/genai-toolbox/resources/tools/looker/looker-get-dimensions/","tags":"","title":"looker-get-dimensions"},{"body":"About A looker-get-explores tool returns all explores for a given model from the source.\nIt’s compatible with the following sources:\nlooker looker-get-explores accepts one parameter, the model id.\nExample tools: get_explores: kind: looker-get-explores source: looker-source description: | The get_explores tool retrieves the list of explores defined in a LookML model in the Looker system. It takes one parameter, the model_name looked up from get_models. Reference field type required description kind string true Must be “looker-get-explores”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"looker-get-explores\" tool returns all explores for the given model from the source.\n","excerpt":"A \"looker-get-explores\" tool returns all explores for the given model …","ref":"/genai-toolbox/resources/tools/looker/looker-get-explores/","tags":"","title":"looker-get-explores"},{"body":"About A looker-get-filters tool returns all the filters from a given explore in a given mode in the source.\nIt’s compatible with the following sources:\nlooker looker-get-filters accepts two parameters, the model and the explore.\nExample tools: get_dimensions: kind: looker-get-filters source: looker-source description: | The get_filters tool retrieves the list of filters defined in an explore. It takes two parameters, the model_name looked up from get_models and the explore_name looked up from get_explores. Reference field type required description kind string true Must be “looker-get-filters”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"looker-get-filters\" tool returns all the filters from a given explore in a given model in the source.\n","excerpt":"A \"looker-get-filters\" tool returns all the filters from a given …","ref":"/genai-toolbox/resources/tools/looker/looker-get-filters/","tags":"","title":"looker-get-filters"},{"body":"About The looker-get-looks tool searches for a saved Look by name or description.\nIt’s compatible with the following sources:\nlooker looker-get-looks takes four parameters, the title, desc, limit and offset.\nTitle and description use SQL style wildcards and are case insensitive.\nLimit and offset are used to page through a larger set of matches and default to 100 and 0.\nExample tools: get_looks: kind: looker-get-looks source: looker-source description: | get_looks Tool This tool is used to search for saved looks in a Looker instance. String search params use case-insensitive matching. String search params can contain % and '_' as SQL LIKE pattern match wildcard expressions. example=\"dan%\" will match \"danger\" and \"Danzig\" but not \"David\" example=\"D_m%\" will match \"Damage\" and \"dump\". Most search params can accept \"IS NULL\" and \"NOT NULL\" as special expressions to match or exclude (respectively) rows where the column is null. The limit and offset are used to paginate the results. The result of the get_looks tool is a list of json objects. Reference field type required description kind string true Must be “looker-get-looks” source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"\"looker-get-looks\" searches for saved Looks in a Looker source.\n","excerpt":"\"looker-get-looks\" searches for saved Looks in a Looker source.\n","ref":"/genai-toolbox/resources/tools/looker/looker-get-looks/","tags":"","title":"looker-get-looks"},{"body":"About A looker-get-measures tool returns all the measures from a given explore in a given mode in the source.\nIt’s compatible with the following sources:\nlooker looker-get-measures accepts two parameters, the model and the explore.\nExample tools: get_measures: kind: looker-get-measures source: looker-source description: | The get_measures tool retrieves the list of measures defined in an explore. It takes two parameters, the model_name looked up from get_models and the explore_name looked up from get_explores. Reference field type required description kind string true Must be “looker-get-measures”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"looker-get-measures\" tool returns all the measures from a given explore in a given model in the source.\n","excerpt":"A \"looker-get-measures\" tool returns all the measures from a given …","ref":"/genai-toolbox/resources/tools/looker/looker-get-measures/","tags":"","title":"looker-get-measures"},{"body":"About A looker-get-models tool returns all the models the source.\nIt’s compatible with the following sources:\nlooker looker-get-models accepts no parameters.\nExample tools: get_models: kind: looker-get-models source: looker-source description: | The get_models tool retrieves the list of LookML models in the Looker system. It takes no parameters. Reference field type required description kind string true Must be “looker-get-models”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"looker-get-models\" tool returns all the models in the source.\n","excerpt":"A \"looker-get-models\" tool returns all the models in the source.\n","ref":"/genai-toolbox/resources/tools/looker/looker-get-models/","tags":"","title":"looker-get-models"},{"body":"About A looker-get-parameters tool returns all the parameters from a given explore in a given mode in the source.\nIt’s compatible with the following sources:\nlooker looker-get-parameters accepts two parameters, the model and the explore.\nExample tools: get_parameters: kind: looker-get-parameters source: looker-source description: | The get_parameters tool retrieves the list of parameters defined in an explore. It takes two parameters, the model_name looked up from get_models and the explore_name looked up from get_explores. Reference field type required description kind string true Must be “looker-get-parameters”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"looker-get-parameters\" tool returns all the parameters from a given explore in a given model in the source.\n","excerpt":"A \"looker-get-parameters\" tool returns all the parameters from a given …","ref":"/genai-toolbox/resources/tools/looker/looker-get-parameters/","tags":"","title":"looker-get-parameters"},{"body":"About The looker-make-look creates a saved Look in the user’s Looker personal folder.\nIt’s compatible with the following sources:\nlooker looker-make-look takes eight parameters:\nthe model the explore the fields list an optional set of filters an optional set of pivots an optional set of sorts an optional limit an optional tz an optional vis_config the title an optional description Example tools: make_look: kind: looker-make-look source: looker-source description: | make_look Tool This tool creates a new look in Looker, using the query parameters and the vis_config specified. Most of the parameters are the same as the query_url tool. In addition, there is a title and a description that must be provided. The newly created look will be created in the user's personal folder in looker. The look name must be unique. The result is a json document with a link to the newly created look. Reference field type required description kind string true Must be “looker-make-look” source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"\"looker-make-look\" generates a Looker look in the users personal folder in Looker\n","excerpt":"\"looker-make-look\" generates a Looker look in the users personal …","ref":"/genai-toolbox/resources/tools/looker/looker-make-look/","tags":"","title":"looker-make-look"},{"body":"About The looker-query tool runs a query using the Looker semantic model.\nIt’s compatible with the following sources:\nlooker looker-query takes eight parameters:\nthe model the explore the fields list an optional set of filters an optional set of pivots an optional set of sorts an optional limit an optional tz Example tools: query: kind: looker-query source: looker-source description: | Query Tool This tool is used to run a query against the LookML model. The model, explore, and fields list must be specified. Pivots, filters and sorts are optional. The model can be found from the get_models tool. The explore can be found from the get_explores tool passing in the model. The fields can be found from the get_dimensions, get_measures, get_filters, and get_parameters tools, passing in the model and the explore. Provide a model_id and explore_name, then a list of fields. Optionally a list of pivots can be provided. The pivots must also be included in the fields list. Filters are provided as a map of {\"field.id\": \"condition\", \"field.id2\": \"condition2\", ...}. Do not put the field.id in quotes. Filter expressions can be found at https://cloud.google.com/looker/docs/filter-expressions. Sorts can be specified like [ \"field.id desc 0\" ]. An optional row limit can be added. If not provided the limit will default to 500. \"-1\" can be specified for unlimited. An optional query timezone can be added. The query_timezone to will default to that of the workstation where this MCP server is running, or Etc/UTC if that can't be determined. Not all models support custom timezones. The result of the query tool is JSON Reference field type required description kind string true Must be “looker-query” source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"\"looker-query\" runs an inline query using the Looker semantic model.\n","excerpt":"\"looker-query\" runs an inline query using the Looker semantic model.\n","ref":"/genai-toolbox/resources/tools/looker/looker-query/","tags":"","title":"looker-query"},{"body":"About The looker-query-sql generates a sql query using the Looker semantic model.\nIt’s compatible with the following sources:\nlooker looker-query-sql takes eight parameters:\nthe model the explore the fields list an optional set of filters an optional set of pivots an optional set of sorts an optional limit an optional tz Example tools: query_sql: kind: looker-query-sql source: looker-source description: | Query SQL Tool This tool is used to generate a sql query against the LookML model. The model, explore, and fields list must be specified. Pivots, filters and sorts are optional. The model can be found from the get_models tool. The explore can be found from the get_explores tool passing in the model. The fields can be found from the get_dimensions, get_measures, get_filters, and get_parameters tools, passing in the model and the explore. Provide a model_id and explore_name, then a list of fields. Optionally a list of pivots can be provided. The pivots must also be included in the fields list. Filters are provided as a map of {\"field.id\": \"condition\", \"field.id2\": \"condition2\", ...}. Do not put the field.id in quotes. Filter expressions can be found at https://cloud.google.com/looker/docs/filter-expressions. Sorts can be specified like [ \"field.id desc 0\" ]. An optional row limit can be added. If not provided the limit will default to 500. \"-1\" can be specified for unlimited. An optional query timezone can be added. The query_timezone to will default to that of the workstation where this MCP server is running, or Etc/UTC if that can't be determined. Not all models support custom timezones. The result of the query tool is the sql string. Reference field type required description kind string true Must be “looker-query-sql” source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"\"looker-query-sql\" generates a sql query using the Looker semantic model.\n","excerpt":"\"looker-query-sql\" generates a sql query using the Looker semantic …","ref":"/genai-toolbox/resources/tools/looker/looker-query-sql/","tags":"","title":"looker-query-sql"},{"body":"About The looker-query-url generates a url link to an explore in Looker so the query can be investigated further.\nIt’s compatible with the following sources:\nlooker looker-query-url takes eight parameters:\nthe model the explore the fields list an optional set of filters an optional set of pivots an optional set of sorts an optional limit an optional tz an optional vis_config Example tools: query_url: kind: looker-query-url source: looker-source description: | Query URL Tool This tool is used to generate the URL of a query in Looker. The user can then explore the query further inside Looker. The tool also returns the query_id and slug. The parameters are the same as the query tool with an additional vis_config parameter. The vis_config is optional. If provided, it will be used to control the default visualization for the query. These are some sample vis_config settings. A bar chart - {{ \"defaults_version\": 1, \"label_density\": 25, \"legend_position\": \"center\", \"limit_displayed_rows\": false, \"ordering\": \"none\", \"plot_size_by_field\": false, \"point_style\": \"none\", \"show_null_labels\": false, \"show_silhouette\": false, \"show_totals_labels\": false, \"show_value_labels\": false, \"show_view_names\": false, \"show_x_axis_label\": true, \"show_x_axis_ticks\": true, \"show_y_axis_labels\": true, \"show_y_axis_ticks\": true, \"stacking\": \"normal\", \"totals_color\": \"#808080\", \"trellis\": \"\", \"type\": \"looker_bar\", \"x_axis_gridlines\": false, \"x_axis_reversed\": false, \"x_axis_scale\": \"auto\", \"x_axis_zoom\": true, \"y_axis_combined\": true, \"y_axis_gridlines\": true, \"y_axis_reversed\": false, \"y_axis_scale_mode\": \"linear\", \"y_axis_tick_density\": \"default\", \"y_axis_tick_density_custom\": 5, \"y_axis_zoom\": true }} A column chart with an option advanced_vis_config - {{ \"advanced_vis_config\": \"{ chart: { type: 'pie', spacingBottom: 50, spacingLeft: 50, spacingRight: 50, spacingTop: 50, }, legend: { enabled: false, }, plotOptions: { pie: { dataLabels: { enabled: true, format: '\\u003cb\\u003e{key}\\u003c/b\\u003e\\u003cspan style=\\\"font-weight: normal\\\"\\u003e - {percentage:.2f}%\\u003c/span\\u003e', }, showInLegend: false, }, }, series: [], }\", \"colors\": [ \"grey\" ], \"defaults_version\": 1, \"hidden_fields\": [], \"label_density\": 25, \"legend_position\": \"center\", \"limit_displayed_rows\": false, \"note_display\": \"below\", \"note_state\": \"collapsed\", \"note_text\": \"Unsold inventory only\", \"ordering\": \"none\", \"plot_size_by_field\": false, \"point_style\": \"none\", \"series_colors\": {}, \"show_null_labels\": false, \"show_silhouette\": false, \"show_totals_labels\": false, \"show_value_labels\": true, \"show_view_names\": false, \"show_x_axis_label\": true, \"show_x_axis_ticks\": true, \"show_y_axis_labels\": true, \"show_y_axis_ticks\": true, \"stacking\": \"normal\", \"totals_color\": \"#808080\", \"trellis\": \"\", \"type\": \"looker_column\", \"x_axis_gridlines\": false, \"x_axis_reversed\": false, \"x_axis_scale\": \"auto\", \"x_axis_zoom\": true, \"y_axes\": [], \"y_axis_combined\": true, \"y_axis_gridlines\": true, \"y_axis_reversed\": false, \"y_axis_scale_mode\": \"linear\", \"y_axis_tick_density\": \"default\", \"y_axis_tick_density_custom\": 5, \"y_axis_zoom\": true }} A line chart - {{ \"defaults_version\": 1, \"hidden_pivots\": {}, \"hidden_series\": [], \"interpolation\": \"linear\", \"label_density\": 25, \"legend_position\": \"center\", \"limit_displayed_rows\": false, \"plot_size_by_field\": false, \"point_style\": \"none\", \"series_types\": {}, \"show_null_points\": true, \"show_value_labels\": false, \"show_view_names\": false, \"show_x_axis_label\": true, \"show_x_axis_ticks\": true, \"show_y_axis_labels\": true, \"show_y_axis_ticks\": true, \"stacking\": \"\", \"trellis\": \"\", \"type\": \"looker_line\", \"x_axis_gridlines\": false, \"x_axis_reversed\": false, \"x_axis_scale\": \"auto\", \"y_axis_combined\": true, \"y_axis_gridlines\": true, \"y_axis_reversed\": false, \"y_axis_scale_mode\": \"linear\", \"y_axis_tick_density\": \"default\", \"y_axis_tick_density_custom\": 5 }} An area chart - {{ \"defaults_version\": 1, \"interpolation\": \"linear\", \"label_density\": 25, \"legend_position\": \"center\", \"limit_displayed_rows\": false, \"plot_size_by_field\": false, \"point_style\": \"none\", \"series_types\": {}, \"show_null_points\": true, \"show_silhouette\": false, \"show_totals_labels\": false, \"show_value_labels\": false, \"show_view_names\": false, \"show_x_axis_label\": true, \"show_x_axis_ticks\": true, \"show_y_axis_labels\": true, \"show_y_axis_ticks\": true, \"stacking\": \"normal\", \"totals_color\": \"#808080\", \"trellis\": \"\", \"type\": \"looker_area\", \"x_axis_gridlines\": false, \"x_axis_reversed\": false, \"x_axis_scale\": \"auto\", \"x_axis_zoom\": true, \"y_axis_combined\": true, \"y_axis_gridlines\": true, \"y_axis_reversed\": false, \"y_axis_scale_mode\": \"linear\", \"y_axis_tick_density\": \"default\", \"y_axis_tick_density_custom\": 5, \"y_axis_zoom\": true }} A scatter plot - {{ \"cluster_points\": false, \"custom_quadrant_point_x\": 5, \"custom_quadrant_point_y\": 5, \"custom_value_label_column\": \"\", \"custom_x_column\": \"\", \"custom_y_column\": \"\", \"defaults_version\": 1, \"hidden_fields\": [], \"hidden_pivots\": {}, \"hidden_points_if_no\": [], \"hidden_series\": [], \"interpolation\": \"linear\", \"label_density\": 25, \"legend_position\": \"center\", \"limit_displayed_rows\": false, \"limit_displayed_rows_values\": { \"first_last\": \"first\", \"num_rows\": 0, \"show_hide\": \"hide\" }, \"plot_size_by_field\": false, \"point_style\": \"circle\", \"quadrant_properties\": { \"0\": { \"color\": \"\", \"label\": \"Quadrant 1\" }, \"1\": { \"color\": \"\", \"label\": \"Quadrant 2\" }, \"2\": { \"color\": \"\", \"label\": \"Quadrant 3\" }, \"3\": { \"color\": \"\", \"label\": \"Quadrant 4\" } }, \"quadrants_enabled\": false, \"series_labels\": {}, \"series_types\": {}, \"show_null_points\": false, \"show_value_labels\": false, \"show_view_names\": true, \"show_x_axis_label\": true, \"show_x_axis_ticks\": true, \"show_y_axis_labels\": true, \"show_y_axis_ticks\": true, \"size_by_field\": \"roi\", \"stacking\": \"normal\", \"swap_axes\": true, \"trellis\": \"\", \"type\": \"looker_scatter\", \"x_axis_gridlines\": false, \"x_axis_reversed\": false, \"x_axis_scale\": \"auto\", \"x_axis_zoom\": true, \"y_axes\": [ { \"label\": \"\", \"orientation\": \"bottom\", \"series\": [ { \"axisId\": \"Channel_0 - average_of_roi_first\", \"id\": \"Channel_0 - average_of_roi_first\", \"name\": \"Channel_0\" }, { \"axisId\": \"Channel_1 - average_of_roi_first\", \"id\": \"Channel_1 - average_of_roi_first\", \"name\": \"Channel_1\" }, { \"axisId\": \"Channel_2 - average_of_roi_first\", \"id\": \"Channel_2 - average_of_roi_first\", \"name\": \"Channel_2\" }, { \"axisId\": \"Channel_3 - average_of_roi_first\", \"id\": \"Channel_3 - average_of_roi_first\", \"name\": \"Channel_3\" }, { \"axisId\": \"Channel_4 - average_of_roi_first\", \"id\": \"Channel_4 - average_of_roi_first\", \"name\": \"Channel_4\" } ], \"showLabels\": true, \"showValues\": true, \"tickDensity\": \"custom\", \"tickDensityCustom\": 100, \"type\": \"linear\", \"unpinAxis\": false } ], \"y_axis_combined\": true, \"y_axis_gridlines\": true, \"y_axis_reversed\": false, \"y_axis_scale_mode\": \"linear\", \"y_axis_tick_density\": \"default\", \"y_axis_tick_density_custom\": 5, \"y_axis_zoom\": true }} A single value visualization - {{ \"defaults_version\": 1, \"show_view_names\": false, \"type\": \"looker_single_record\" }} A Pie chart - {{ \"defaults_version\": 1, \"label_density\": 25, \"label_type\": \"labPer\", \"legend_position\": \"center\", \"limit_displayed_rows\": false, \"ordering\": \"none\", \"plot_size_by_field\": false, \"point_style\": \"none\", \"series_types\": {}, \"show_null_labels\": false, \"show_silhouette\": false, \"show_totals_labels\": false, \"show_value_labels\": false, \"show_view_names\": false, \"show_x_axis_label\": true, \"show_x_axis_ticks\": true, \"show_y_axis_labels\": true, \"show_y_axis_ticks\": true, \"stacking\": \"\", \"totals_color\": \"#808080\", \"trellis\": \"\", \"type\": \"looker_pie\", \"value_labels\": \"legend\", \"x_axis_gridlines\": false, \"x_axis_reversed\": false, \"x_axis_scale\": \"auto\", \"y_axis_combined\": true, \"y_axis_gridlines\": true, \"y_axis_reversed\": false, \"y_axis_scale_mode\": \"linear\", \"y_axis_tick_density\": \"default\", \"y_axis_tick_density_custom\": 5 }} The result is a JSON object with the id, slug, the url, and the long_url. Reference field type required description kind string true Must be “looker-query-url” source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"\"looker-query-url\" generates a url link to a Looker explore.\n","excerpt":"\"looker-query-url\" generates a url link to a Looker explore.\n","ref":"/genai-toolbox/resources/tools/looker/looker-query-url/","tags":"","title":"looker-query-url"},{"body":"About The looker-run-look tool runs the query associated with a saved Look.\nIt’s compatible with the following sources:\nlooker looker-run-look takes one parameter, the look_id.\nExample tools: run_look: kind: looker-run-look source: looker-source description: | run_look Tool This tool runs the query associated with a look and returns the data in a JSON structure. It accepts the look_id as the parameter. Reference field type required description kind string true Must be “looker-run-look” source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"\"looker-run-look\" runs the query associated with a saved Look.\n","excerpt":"\"looker-run-look\" runs the query associated with a saved Look.\n","ref":"/genai-toolbox/resources/tools/looker/looker-run-look/","tags":"","title":"looker-run-look"},{"body":"About MongoDB is a popular NoSQL database that stores data in flexible, JSON-like documents, making it easy to develop and scale applications.\nExample sources: my-mongodb: kind: mongodb uri: \"mongodb+srv://username:password@host.mongodb.net\" database: sample_mflix Reference field type required description kind string true Must be “mongodb”. uri string true connection string to connect to MongoDB database string true Name of the mongodb database to connect to (e.g. “sample_mflix”). ","categories":"","description":"MongoDB is a no-sql data platform that can not only serve general purpose data requirements also perform VectorSearch where both operational data and embeddings used of search can reside in same document.\n","excerpt":"MongoDB is a no-sql data platform that can not only serve general …","ref":"/genai-toolbox/resources/sources/mongodb/","tags":"","title":"MongoDB"},{"body":"","categories":"","description":"Tools that work with the MongoDB Source.\n","excerpt":"Tools that work with the MongoDB Source.\n","ref":"/genai-toolbox/resources/tools/mongodb/","tags":"","title":"MongoDB"},{"body":"About The mongodb-aggregate tool is the most powerful query tool for MongoDB, allowing you to process data through a multi-stage pipeline. Each stage transforms the documents as they pass through, enabling complex operations like grouping, filtering, reshaping documents, and performing calculations.\nThe core of this tool is the pipelinePayload, which must be a string containing a JSON array of pipeline stage documents. The tool returns a JSON array of documents produced by the final stage of the pipeline.\nA readOnly flag can be set to true as a safety measure to ensure the pipeline does not contain any write stages (like $out or $merge).\nThis tool is compatible with the following source kind:\nmongodb Example Here is an example that calculates the average price and total count of products for each category, but only for products with an “active” status.\ntools: get_category_stats: kind: mongodb-aggregate source: my-mongo-source description: Calculates average price and count of products, grouped by category. database: ecommerce collection: products readOnly: true pipelinePayload: | [ { \"$match\": { \"status\": {{json .status_filter}} } }, { \"$group\": { \"_id\": \"$category\", \"average_price\": { \"$avg\": \"$price\" }, \"item_count\": { \"$sum\": 1 } } }, { \"$sort\": { \"average_price\": -1 } } ] pipelineParams: - name: status_filter type: string description: The product status to filter by (e.g., \"active\"). Reference field type required description kind string true Must be mongodb-aggregate. source string true The name of the mongodb source to use. description string true A description of the tool that is passed to the LLM. database string true The name of the MongoDB database containing the collection. collection string true The name of the MongoDB collection to run the aggregation on. pipelinePayload string true A JSON array of aggregation stage documents, provided as a string. Uses {{json .param_name}} for templating. pipelineParams list true A list of parameter objects that define the variables used in the pipelinePayload. canonical bool false Determines if the pipeline string is parsed using MongoDB’s Canonical or Relaxed Extended JSON format. readOnly bool false If true, the tool will fail if the pipeline contains write stages ($out or $merge). Defaults to false. ","categories":"","description":"A \"mongodb-aggregate\" tool executes a multi-stage aggregation pipeline against a MongoDB collection.\n","excerpt":"A \"mongodb-aggregate\" tool executes a multi-stage aggregation pipeline …","ref":"/genai-toolbox/resources/tools/mongodb/mongodb-aggregate/","tags":"","title":"mongodb-aggregate"},{"body":"About The mongodb-delete-many tool performs a bulk destructive operation, deleting ALL documents from a collection that match a specified filter.\nThe tool returns the total count of documents that were deleted. If the filter does not match any documents (i.e., the deleted count is 0), the tool will return an error.\nThis tool is compatible with the following source kind:\nmongodb Example Here is an example that performs a cleanup task by deleting all products from the inventory collection that belong to a discontinued brand.\ntools: retire_brand_products: kind: mongodb-delete-many source: my-mongo-source description: Deletes all products from a specified discontinued brand. database: ecommerce collection: inventory filterPayload: | { \"brand_name\": {{json .brand_to_delete}} } filterParams: - name: brand_to_delete type: string description: The name of the discontinued brand whose products should be deleted. Reference field type required description kind string true Must be mongodb-delete-many. source string true The name of the mongodb source to use. description string true A description of the tool that is passed to the LLM. database string true The name of the MongoDB database containing the collection. collection string true The name of the MongoDB collection from which to delete documents. filterPayload string true The MongoDB query filter document to select the documents for deletion. Uses {{json .param_name}} for templating. filterParams list true A list of parameter objects that define the variables used in the filterPayload. ","categories":"","description":"A \"mongodb-delete-many\" tool deletes all documents from a MongoDB collection that match a filter.\n","excerpt":"A \"mongodb-delete-many\" tool deletes all documents from a MongoDB …","ref":"/genai-toolbox/resources/tools/mongodb/mongodb-delete-many/","tags":"","title":"mongodb-delete-many"},{"body":"About The mongodb-delete-one tool performs a destructive operation, deleting the first single document that matches a specified filter from a MongoDB collection.\nIf the filter matches multiple documents, only the first one found by the database will be deleted. This tool is useful for removing specific entries, such as a user account or a single item from an inventory based on a unique ID.\nThe tool returns the number of documents deleted, which will be either 1 if a document was found and deleted, or 0 if no matching document was found.\nThis tool is compatible with the following source kind:\nmongodb Example Here is an example that deletes a specific user account from the users collection by matching their unique email address. This is a permanent action.\ntools: delete_user_account: kind: mongodb-delete-one source: my-mongo-source description: Permanently deletes a user account by their email address. database: user_data collection: users filterPayload: | { \"email\": {{json .email_address}} } filterParams: - name: email_address type: string description: The email of the user account to delete. Reference field type required description kind string true Must be mongodb-delete-one. source string true The name of the mongodb source to use. description string true A description of the tool that is passed to the LLM. database string true The name of the MongoDB database containing the collection. collection string true The name of the MongoDB collection from which to delete a document. filterPayload string true The MongoDB query filter document to select the document for deletion. Uses {{json .param_name}} for templating. filterParams list true A list of parameter objects that define the variables used in the filterPayload. ","categories":"","description":"A \"mongodb-delete-one\" tool deletes a single document from a MongoDB collection.\n","excerpt":"A \"mongodb-delete-one\" tool deletes a single document from a MongoDB …","ref":"/genai-toolbox/resources/tools/mongodb/mongodb-delete-one/","tags":"","title":"mongodb-delete-one"},{"body":"About A mongodb-find tool is used to query a MongoDB collection and retrieve documents that match a specified filter. It’s a flexible tool that allows you to shape the output by selecting specific fields (projection), ordering the results (sorting), and restricting the number of documents returned (limiting).\nThe tool returns a JSON array of the documents found.\nThis tool is compatible with the following source kind:\nmongodb Example Here’s an example that finds up to 10 users from the customers collection who live in a specific city. The results are sorted by their last name, and only their first name, last name, and email are returned.\ntools: find_local_customers: kind: mongodb-find source: my-mongo-source description: Finds customers by city, sorted by last name. database: crm collection: customers limit: 10 filterPayload: | { \"address.city\": {{json .city}} } filterParams: - name: city type: string description: The city to search for customers in. projectPayload: | { \"first_name\": 1, \"last_name\": 1, \"email\": 1, \"_id\": 0 } sortPayload: | { \"last_name\": {{json .sort_order}} } sortParams: - name: sort_order type: integer description: The sort order (1 for ascending, -1 for descending). Reference field type required description kind string true Must be mongodb-find. source string true The name of the mongodb source to use. description string true A description of the tool that is passed to the LLM. database string true The name of the MongoDB database to query. collection string true The name of the MongoDB collection to query. filterPayload string true The MongoDB query filter document to select which documents to return. Uses {{json .param_name}} for templating. filterParams list true A list of parameter objects that define the variables used in the filterPayload. projectPayload string false An optional MongoDB projection document to specify which fields to include (1) or exclude (0) in the results. projectParams list false A list of parameter objects for the projectPayload. sortPayload string false An optional MongoDB sort document to define the order of the returned documents. Use 1 for ascending and -1 for descending. sortParams list false A list of parameter objects for the sortPayload. limit integer false An optional integer specifying the maximum number of documents to return. ","categories":"","description":"A \"mongodb-find\" tool finds and retrieves documents from a MongoDB collection.\n","excerpt":"A \"mongodb-find\" tool finds and retrieves documents from a MongoDB …","ref":"/genai-toolbox/resources/tools/mongodb/mongodb-find/","tags":"","title":"mongodb-find"},{"body":"About A mongodb-find-one tool is used to retrieve the first single document that matches a specified filter from a MongoDB collection. If multiple documents match the filter, you can use sort options to control which document is returned. Otherwise, the selection is not guaranteed.\nThe tool returns a single JSON object representing the document, wrapped in a JSON array.\nThis tool is compatible with the following source kind:\nmongodb Example Here’s a common use case: finding a specific user by their unique email address and returning their profile information, while excluding sensitive fields like the password hash.\ntools: get_user_profile: kind: mongodb-find-one source: my-mongo-source description: Retrieves a user's profile by their email address. database: user_data collection: profiles filterPayload: | { \"email\": {{json .email}} } filterParams: - name: email type: string description: The email address of the user to find. projectPayload: | { \"password_hash\": 0, \"login_history\": 0 } Reference field type required description kind string true Must be mongodb-find-one. source string true The name of the mongodb source to use. description string true A description of the tool that is passed to the LLM. database string true The name of the MongoDB database to query. collection string true The name of the MongoDB collection to query. filterPayload string true The MongoDB query filter document to select the document. Uses {{json .param_name}} for templating. filterParams list true A list of parameter objects that define the variables used in the filterPayload. projectPayload string false An optional MongoDB projection document to specify which fields to include (1) or exclude (0) in the result. projectParams list false A list of parameter objects for the projectPayload. sortPayload string false An optional MongoDB sort document. Useful for selecting which document to return if the filter matches multiple (e.g., get the most recent). sortParams list false A list of parameter objects for the sortPayload. ","categories":"","description":"A \"mongodb-find-one\" tool finds and retrieves a single document from a MongoDB collection.\n","excerpt":"A \"mongodb-find-one\" tool finds and retrieves a single document from a …","ref":"/genai-toolbox/resources/tools/mongodb/mongodb-find-one/","tags":"","title":"mongodb-find-one"},{"body":"About The mongodb-insert-many tool inserts multiple new documents into a specified MongoDB collection in a single bulk operation. This is highly efficient for adding large amounts of data at once.\nThis tool takes one required parameter named data. This data parameter must be a string containing a JSON array of document objects. Upon successful insertion, the tool returns a JSON array containing the unique _id of each new document that was created.\nThis tool is compatible with the following source kind:\nmongodb Example Here is an example configuration for a tool that logs multiple events at once.\ntools: log_batch_events: kind: mongodb-insert-many source: my-mongo-source description: Inserts a batch of event logs into the database. database: logging collection: events canonical: true An LLM would call this tool by providing an array of documents as a JSON string in the data parameter, like this: tool_code: log_batch_events(data='[{\"event\": \"login\", \"user\": \"user1\"}, {\"event\": \"click\", \"user\": \"user2\"}, {\"event\": \"logout\", \"user\": \"user1\"}]')\nReference field type required description kind string true Must be mongodb-insert-many. source string true The name of the mongodb source to use. description string true A description of the tool that is passed to the LLM. database string true The name of the MongoDB database containing the collection. collection string true The name of the MongoDB collection into which the documents will be inserted. canonical bool true Determines if the data string is parsed using MongoDB’s Canonical or Relaxed Extended JSON format. ","categories":"","description":"A \"mongodb-insert-many\" tool inserts multiple new documents into a MongoDB collection.\n","excerpt":"A \"mongodb-insert-many\" tool inserts multiple new documents into a …","ref":"/genai-toolbox/resources/tools/mongodb/mongodb-insert-many/","tags":"","title":"mongodb-insert-many"},{"body":"About The mongodb-insert-one tool inserts a single new document into a specified MongoDB collection.\nThis tool takes one required parameter named data, which must be a string containing the JSON object you want to insert. Upon successful insertion, the tool returns the unique _id of the newly created document.\nThis tool is compatible with the following source kind:\nmongodb Example Here is an example configuration for a tool that adds a new user to a users collection.\ntools: create_new_user: kind: mongodb-insert-one source: my-mongo-source description: Creates a new user record in the database. database: user_data collection: users canonical: false An LLM would call this tool by providing the document as a JSON string in the data parameter, like this: tool_code: create_new_user(data='{\"email\": \"new.user@example.com\", \"name\": \"Jane Doe\", \"status\": \"active\"}')\nReference field type required description kind string true Must be mongodb-insert-one. source string true The name of the mongodb source to use. description string true A description of the tool that is passed to the LLM. database string true The name of the MongoDB database containing the collection. collection string true The name of the MongoDB collection into which the document will be inserted. canonical bool true Determines if the data string is parsed using MongoDB’s Canonical or Relaxed Extended JSON format. ","categories":"","description":"A \"mongodb-insert-one\" tool inserts a single new document into a MongoDB collection.\n","excerpt":"A \"mongodb-insert-one\" tool inserts a single new document into a …","ref":"/genai-toolbox/resources/tools/mongodb/mongodb-insert-one/","tags":"","title":"mongodb-insert-one"},{"body":"About A mongodb-update-many tool updates all documents within a specified MongoDB collection that match a given filter. It locates the documents using a filterPayload and applies the modifications defined in an updatePayload.\nThe tool returns an array of three integers: [ModifiedCount, UpsertedCount, MatchedCount].\nThis tool is compatible with the following source kind:\nmongodb Example Here’s an example configuration. This tool applies a discount to all items within a specific category and also marks them as being on sale.\ntools: apply_category_discount: kind: mongodb-update-many source: my-mongo-source description: Use this tool to apply a discount to all items in a given category. database: products collection: inventory filterPayload: | { \"category\": {{json .category_name}} } filterParams: - name: category_name type: string description: The category of items to update. updatePayload: | { \"$mul\": { \"price\": {{json .discount_multiplier}} }, \"$set\": { \"on_sale\": true } } updateParams: - name: discount_multiplier type: number description: The multiplier to apply to the price (e.g., 0.8 for a 20% discount). canonical: false upsert: false Reference field type required description kind string true Must be mongodb-update-many. source string true The name of the mongodb source to use. description string true A description of the tool that is passed to the LLM. database string true The name of the MongoDB database containing the collection. collection string true The name of the MongoDB collection in which to update documents. filterPayload string true The MongoDB query filter document to select the documents for updating. It’s written as a Go template, using {{json .param_name}} to insert parameters. filterParams list true A list of parameter objects that define the variables used in the filterPayload. updatePayload string true The MongoDB update document, It’s written as a Go template, using {{json .param_name}} to insert parameters. updateParams list true A list of parameter objects that define the variables used in the updatePayload. canonical bool true Determines if the filterPayload and updatePayload strings are parsed using MongoDB’s Canonical or Relaxed Extended JSON format. Canonical is stricter about type representation, while Relaxed is more lenient. upsert bool false If true, a new document is created if no document matches the filterPayload. Defaults to false. ","categories":"","description":"A \"mongodb-update-many\" tool updates all documents in a MongoDB collection that match a filter.\n","excerpt":"A \"mongodb-update-many\" tool updates all documents in a MongoDB …","ref":"/genai-toolbox/resources/tools/mongodb/mongodb-update-many/","tags":"","title":"mongodb-update-many"},{"body":"About A mongodb-update-one tool updates a single document within a specified MongoDB collection. It locates the document to be updated using a filterPayload and applies modifications defined in an updatePayload. If the filter matches multiple documents, only the first one found will be updated.\nThis tool is compatible with the following source kind:\nmongodb Example Here’s an example of a mongodb-update-one tool configuration. This tool updates the stock and status fields of a document in the inventory collection where the item field matches a provided value. If no matching document is found, the upsert: true option will create a new one.\ntools: update_inventory_item: kind: mongodb-update-one source: my-mongo-source description: Use this tool to update an item's stock and status in the inventory. database: products collection: inventory filterPayload: | { \"item\": {{json .item_name}} } filterParams: - name: item_name type: string description: The name of the item to update. updatePayload: | { \"$set\": { \"stock\": {{json .new_stock}}, \"status\": {{json .new_status}} } } updateParams: - name: new_stock type: integer description: The new stock quantity. - name: new_status type: string description: The new status of the item (e.g., \"In Stock\", \"Backordered\"). canonical: false upsert: true Reference field type required description kind string true Must be mongodb-update-one. source string true The name of the mongodb source to use. description string true A description of the tool that is passed to the LLM. database string true The name of the MongoDB database containing the collection. collection string true The name of the MongoDB collection to update a document in. filterPayload string true The MongoDB query filter document to select the document for updating. It’s written as a Go template, using {{json .param_name}} to insert parameters. filterParams list true A list of parameter objects that define the variables used in the filterPayload. updatePayload string true The MongoDB update document, which specifies the modifications. This often uses update operators like $set. It’s written as a Go template, using {{json .param_name}} to insert parameters. updateParams list true A list of parameter objects that define the variables used in the updatePayload. canonical bool true Determines if the updatePayload string is parsed using MongoDB’s Canonical or Relaxed Extended JSON format. Canonical is stricter about type representation (e.g., {\"$numberInt\": \"42\"}), while Relaxed is more lenient (e.g., 42). upsert bool false If true, a new document is created if no document matches the filterPayload. Defaults to false. ","categories":"","description":"A \"mongodb-update-one\" tool updates a single document in a MongoDB collection.\n","excerpt":"A \"mongodb-update-one\" tool updates a single document in a MongoDB …","ref":"/genai-toolbox/resources/tools/mongodb/mongodb-update-one/","tags":"","title":"mongodb-update-one"},{"body":"About A mssql-execute-sql tool executes a SQL statement against a SQL Server database. It’s compatible with any of the following sources:\ncloud-sql-mssql mssql mssql-execute-sql takes one input parameter sql and run the sql statement against the source.\nNote: This tool is intended for developer assistant workflows with human-in-the-loop and shouldn’t be used for production agents.\nExample tools: execute_sql_tool: kind: mssql-execute-sql source: my-mssql-instance description: Use this tool to execute sql statement. Reference field type required description kind string true Must be “mssql-execute-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"mssql-execute-sql\" tool executes a SQL statement against a SQL Server database.\n","excerpt":"A \"mssql-execute-sql\" tool executes a SQL statement against a SQL …","ref":"/genai-toolbox/resources/tools/mssql/mssql-execute-sql/","tags":"","title":"mssql-execute-sql"},{"body":"About A mssql-sql tool executes a pre-defined SQL statement against a SQL Server database. It’s compatible with any of the following sources:\ncloud-sql-mssql mssql Toolbox supports the prepare statement syntax of MS SQL Server and expects parameters in the SQL query to be in the form of either @Name or @p1 to @pN (ordinal position).\ndb.QueryContext(ctx, `select * from t where ID = @ID and Name = @p2;`, sql.Named(\"ID\", 6), \"Bob\") Example Note: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\ntools: search_flights_by_number: kind: mssql-sql source: my-instance statement: | SELECT * FROM flights WHERE airline = @airline AND flight_number = @flight_number LIMIT 10 description: | Use this tool to get information for a specific flight. Takes an airline code and flight number and returns info on the flight. Do NOT use this tool with a flight id. Do NOT guess an airline code or flight number. A airline code is a code for an airline service consisting of two-character airline designator and followed by flight number, which is 1 to 4 digit number. For example, if given CY 0123, the airline is \"CY\", and flight_number is \"123\". Another example for this is DL 1234, the airline is \"DL\", and flight_number is \"1234\". If the tool returns more than one option choose the date closes to today. Example: {{ \"airline\": \"CY\", \"flight_number\": \"888\", }} Example: {{ \"airline\": \"DL\", \"flight_number\": \"1234\", }} parameters: - name: airline type: string description: Airline unique 2 letter identifier - name: flight_number type: string description: 1 to 4 digit number Example with Template Parameters Note: This tool allows direct modifications to the SQL statement, including identifiers, column names, and table names. This makes it more vulnerable to SQL injections. Using basic parameters only (see above) is recommended for performance and safety reasons. For more details, please check templateParameters.\ntools: list_table: kind: mssql-sql source: my-instance statement: | SELECT * FROM {{.tableName}}; description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", }} templateParameters: - name: tableName type: string description: Table to select from Reference field type required description kind string true Must be “mssql-sql”. source string true Name of the source the T-SQL statement should execute on. description string true Description of the tool that is passed to the LLM. statement string true SQL statement to execute. parameters parameters false List of parameters that will be inserted into the SQL statement. templateParameters templateParameters false List of templateParameters that will be inserted into the SQL statement before executing prepared statement. ","categories":"","description":"A \"mssql-sql\" tool executes a pre-defined SQL statement against a SQL Server database.\n","excerpt":"A \"mssql-sql\" tool executes a pre-defined SQL statement against a SQL …","ref":"/genai-toolbox/resources/tools/mssql/mssql-sql/","tags":"","title":"mssql-sql"},{"body":"About MySQL is a relational database management system (RDBMS) that stores and manages data. It’s a popular choice for developers because of its reliability, performance, and ease of use.\nAvailable Tools mysql-sql\nExecute pre-defined prepared SQL queries in MySQL.\nmysql-execute-sql\nRun parameterized SQL queries in MySQL.\nRequirements Database User This source only uses standard authentication. You will need to create a MySQL user to login to the database with.\nExample sources: my-mysql-source: kind: mysql host: 127.0.0.1 port: 3306 database: my_db user: ${USER_NAME} password: ${PASSWORD} queryTimeout: 30s # Optional: query timeout duration Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “mysql”. host string true IP address to connect to (e.g. “127.0.0.1”). port string true Port to connect to (e.g. “3306”). database string true Name of the MySQL database to connect to (e.g. “my_db”). user string true Name of the MySQL user to connect as (e.g. “my-mysql-user”). password string true Password of the MySQL user (e.g. “my-password”). queryTimeout string false Maximum time to wait for query execution (e.g. “30s”, “2m”). By default, no timeout is applied. ","categories":"","description":"MySQL is a relational database management system that stores and manages data.\n","excerpt":"MySQL is a relational database management system that stores and …","ref":"/genai-toolbox/resources/sources/mysql/","tags":"","title":"MySQL"},{"body":"","categories":"","description":"Tools that work with MySQL Sources, such as Cloud SQL for MySQL.\n","excerpt":"Tools that work with MySQL Sources, such as Cloud SQL for MySQL.\n","ref":"/genai-toolbox/resources/tools/mysql/","tags":"","title":"MySQL"},{"body":"About A mysql-execute-sql tool executes a SQL statement against a MySQL database. It’s compatible with any of the following sources:\ncloud-sql-mysql mysql mysql-execute-sql takes one input parameter sql and run the sql statement against the source.\nNote: This tool is intended for developer assistant workflows with human-in-the-loop and shouldn’t be used for production agents.\nExample tools: execute_sql_tool: kind: mysql-execute-sql source: my-mysql-instance description: Use this tool to execute sql statement. Reference field type required description kind string true Must be “mysql-execute-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"mysql-execute-sql\" tool executes a SQL statement against a MySQL database.\n","excerpt":"A \"mysql-execute-sql\" tool executes a SQL statement against a MySQL …","ref":"/genai-toolbox/resources/tools/mysql/mysql-execute-sql/","tags":"","title":"mysql-execute-sql"},{"body":"About A mysql-sql tool executes a pre-defined SQL statement against a MySQL database. It’s compatible with any of the following sources:\ncloud-sql-mysql mysql The specified SQL statement is executed as a prepared statement, and expects parameters in the SQL query to be in the form of placeholders ?.\nExample Note: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\ntools: search_flights_by_number: kind: mysql-sql source: my-mysql-instance statement: | SELECT * FROM flights WHERE airline = ? AND flight_number = ? LIMIT 10 description: | Use this tool to get information for a specific flight. Takes an airline code and flight number and returns info on the flight. Do NOT use this tool with a flight id. Do NOT guess an airline code or flight number. A airline code is a code for an airline service consisting of two-character airline designator and followed by flight number, which is 1 to 4 digit number. For example, if given CY 0123, the airline is \"CY\", and flight_number is \"123\". Another example for this is DL 1234, the airline is \"DL\", and flight_number is \"1234\". If the tool returns more than one option choose the date closes to today. Example: {{ \"airline\": \"CY\", \"flight_number\": \"888\", }} Example: {{ \"airline\": \"DL\", \"flight_number\": \"1234\", }} parameters: - name: airline type: string description: Airline unique 2 letter identifier - name: flight_number type: string description: 1 to 4 digit number Example with Template Parameters Note: This tool allows direct modifications to the SQL statement, including identifiers, column names, and table names. This makes it more vulnerable to SQL injections. Using basic parameters only (see above) is recommended for performance and safety reasons. For more details, please check templateParameters.\ntools: list_table: kind: mysql-sql source: my-mysql-instance statement: | SELECT * FROM {{.tableName}}; description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", }} templateParameters: - name: tableName type: string description: Table to select from Reference field type required description kind string true Must be “mysql-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. statement string true SQL statement to execute on. parameters parameters false List of parameters that will be inserted into the SQL statement. templateParameters templateParameters false List of templateParameters that will be inserted into the SQL statement before executing prepared statement. ","categories":"","description":"A \"mysql-sql\" tool executes a pre-defined SQL statement against a MySQL database.\n","excerpt":"A \"mysql-sql\" tool executes a pre-defined SQL statement against a …","ref":"/genai-toolbox/resources/tools/mysql/mysql-sql/","tags":"","title":"mysql-sql"},{"body":"About Neo4j is a powerful, open source graph database system with over 15 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.\nAvailable Tools neo4j-cypher\nRun Cypher queries against your Neo4j graph database. Requirements Database User This source only uses standard authentication. You will need to create a Neo4j user to log in to the database with, or use the default neo4j user if available.\nExample sources: my-neo4j-source: kind: neo4j uri: neo4j+s://xxxx.databases.neo4j.io:7687 user: ${USER_NAME} password: ${PASSWORD} database: \"neo4j\" Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “neo4j”. uri string true Connect URI (“bolt://localhost”, “neo4j+s://xxx.databases.neo4j.io”) user string true Name of the Neo4j user to connect as (e.g. “neo4j”). password string true Password of the Neo4j user (e.g. “my-password”). database string true Name of the Neo4j database to connect to (e.g. “neo4j”). ","categories":"","description":"Neo4j is a powerful, open source graph database system\n","excerpt":"Neo4j is a powerful, open source graph database system\n","ref":"/genai-toolbox/resources/sources/neo4j/","tags":"","title":"Neo4j"},{"body":"","categories":"","description":"Tools that work with Neo4j Sources.\n","excerpt":"Tools that work with Neo4j Sources.\n","ref":"/genai-toolbox/resources/tools/neo4j/","tags":"","title":"Neo4j"},{"body":"About A neo4j-cypher tool executes a pre-defined Cypher statement against a Neo4j database. It’s compatible with any of the following sources:\nneo4j The specified Cypher statement is executed as a parameterized statement, and specified parameters will be used according to their name: e.g. $id.\nNote: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\nExample tools: search_movies_by_actor: kind: neo4j-cypher source: my-neo4j-movies-instance statement: | MATCH (m:Movie)\u003c-[:ACTED_IN]-(p:Person) WHERE p.name = $name AND m.year \u003e $year RETURN m.title, m.year LIMIT 10 description: | Use this tool to get a list of movies for a specific actor and a given minimum release year. Takes an full actor name, e.g. \"Tom Hanks\" and a year e.g 1993 and returns a list of movie titles and release years. Do NOT use this tool with a movie title. Do NOT guess an actor name, Do NOT guess a year. A actor name is a fully qualified name with first and last name separated by a space. For example, if given \"Hanks, Tom\" the actor name is \"Tom Hanks\". If the tool returns more than one option choose the most recent movies. Example: {{ \"name\": \"Meg Ryan\", \"year\": 1993 }} Example: {{ \"name\": \"Clint Eastwood\", \"year\": 2000 }} parameters: - name: name type: string description: Full actor name, \"firstname lastname\" - name: year type: integer description: 4 digit number starting in 1900 up to the current year Reference field type required description kind string true Must be “neo4j-cypher”. source string true Name of the source the Cypher query should execute on. description string true Description of the tool that is passed to the LLM. statement string true Cypher statement to execute parameters parameters false List of parameters that will be used with the Cypher statement. ","categories":"","description":"A \"neo4j-cypher\" tool executes a pre-defined cypher statement against a Neo4j database.\n","excerpt":"A \"neo4j-cypher\" tool executes a pre-defined cypher statement against …","ref":"/genai-toolbox/resources/tools/neo4j/neo4j-cypher/","tags":"","title":"neo4j-cypher"},{"body":"About A neo4j-execute-cypher tool executes an arbitrary Cypher query provided as a string parameter against a Neo4j database. It’s designed to be a flexible tool for interacting with the database when a pre-defined query is not sufficient. This tool is compatible with any of the following sources:\nneo4j For security, the tool can be configured to be read-only. If the readOnly flag is set to true, the tool will analyze the incoming Cypher query and reject any write operations (like CREATE, MERGE, DELETE, etc.) before execution.\nThe Cypher query uses standard Neo4j Cypher syntax and supports all Cypher features, including pattern matching, filtering, and aggregation.\nneo4j-execute-cypher takes one input parameter cypher and run the cypher query against the source.\nNote: This tool is intended for developer assistant workflows with human-in-the-loop and shouldn’t be used for production agents.\nExample tools: query_neo4j: kind: neo4j-execute-cypher source: my-neo4j-prod-db readOnly: true description: | Use this tool to execute a Cypher query against the production database. Only read-only queries are allowed. Takes a single 'cypher' parameter containing the full query string. Example: {{ \"cypher\": \"MATCH (m:Movie {title: 'The Matrix'}) RETURN m.released\" }} Reference field type required description kind string true Must be “neo4j-cypher”. source string true Name of the source the Cypher query should execute on. description string true Description of the tool that is passed to the LLM. readOnly boolean false If set to true, the tool will reject any write operations in the Cypher query. Default is false. ","categories":"","description":"A \"neo4j-execute-cypher\" tool executes any arbitrary Cypher statement against a Neo4j database.\n","excerpt":"A \"neo4j-execute-cypher\" tool executes any arbitrary Cypher statement …","ref":"/genai-toolbox/resources/tools/neo4j/neo4j-execute-cypher/","tags":"","title":"neo4j-execute-cypher"},{"body":"About A neo4j-schema tool connects to a Neo4j database and extracts its complete schema information. It runs multiple queries concurrently to efficiently gather details about node labels, relationships, properties, constraints, and indexes.\nThe tool automatically detects if the APOC (Awesome Procedures on Cypher) library is available. If so, it uses APOC procedures like apoc.meta.schema for a highly detailed overview of the database structure; otherwise, it falls back to using native Cypher queries.\nThe extracted schema is cached to improve performance for subsequent requests. The output is a structured JSON object containing all the schema details, which can be invaluable for providing database context to an LLM. This tool is compatible with a neo4j source and takes no parameters.\nExample tools: get_movie_db_schema: kind: neo4j-schema source: my-neo4j-movies-instance description: | Use this tool to get the full schema of the movie database. This provides information on all available node labels (like Movie, Person), relationships (like ACTED_IN), and the properties on each. This tool takes no parameters. # Optional configuration to cache the schema for 2 hours cacheExpireMinutes: 120 Reference field type required description kind string true Must be neo4j-db-schema. source string true Name of the source the schema should be extracted from. description string true Description of the tool that is passed to the LLM. cacheExpireMinutes integer false Cache expiration time in minutes. Defaults to 60. ","categories":"","description":"A \"neo4j-schema\" tool extracts a comprehensive schema from a Neo4j database.\n","excerpt":"A \"neo4j-schema\" tool extracts a comprehensive schema from a Neo4j …","ref":"/genai-toolbox/resources/tools/neo4j/neo4j-schema/","tags":"","title":"neo4j-schema"},{"body":"About OceanBase is a distributed relational database management system (RDBMS) that provides high availability, scalability, and strong consistency. It’s designed to handle large-scale data processing and is compatible with MySQL, making it easy for developers to migrate from MySQL to OceanBase.\nRequirements Database User This source only uses standard authentication. You will need to create an OceanBase user to login to the database with. OceanBase supports MySQL-compatible user management syntax.\nNetwork Connectivity Ensure that your application can connect to the OceanBase cluster. OceanBase typically runs on ports 2881 (for MySQL protocol) or 3881 (for MySQL protocol with SSL).\nExample sources: my-oceanbase-source: kind: oceanbase host: 127.0.0.1 port: 2881 database: my_db user: ${USER_NAME} password: ${PASSWORD} queryTimeout: 30s # Optional: query timeout duration Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “oceanbase”. host string true IP address to connect to (e.g. “127.0.0.1”). port string true Port to connect to (e.g. “2881”). database string true Name of the OceanBase database to connect to (e.g. “my_db”). user string true Name of the OceanBase user to connect as (e.g. “my-oceanbase-user”). password string true Password of the OceanBase user (e.g. “my-password”). queryTimeout string false Maximum time to wait for query execution (e.g. “30s”, “2m”). By default, no timeout is applied. Features MySQL Compatibility OceanBase is highly compatible with MySQL, supporting most MySQL SQL syntax, data types, and functions. This makes it easy to migrate existing MySQL applications to OceanBase.\nHigh Availability OceanBase provides automatic failover and data replication across multiple nodes, ensuring high availability and data durability.\nScalability OceanBase can scale horizontally by adding more nodes to the cluster, making it suitable for large-scale applications.\nStrong Consistency OceanBase provides strong consistency guarantees, ensuring that all transactions are ACID compliant.\n","categories":"","description":"OceanBase is a distributed relational database that provides high availability, scalability, and compatibility with MySQL.\n","excerpt":"OceanBase is a distributed relational database that provides high …","ref":"/genai-toolbox/resources/sources/oceanbase/","tags":"","title":"OceanBase"},{"body":"","categories":"","description":"Tools that work with OceanBase Sources.\n","excerpt":"Tools that work with OceanBase Sources.\n","ref":"/genai-toolbox/resources/tools/oceanbase/","tags":"","title":"OceanBase"},{"body":"About An oceanbase-execute-sql tool executes a SQL statement against an OceanBase database. It’s compatible with the following source:\noceanbase oceanbase-execute-sql takes one input parameter sql and runs the sql statement against the source.\nNote: This tool is intended for developer assistant workflows with human-in-the-loop and shouldn’t be used for production agents.\nExample tools: execute_sql_tool: kind: oceanbase-execute-sql source: my-oceanbase-instance description: Use this tool to execute sql statement. Reference field type required description kind string true Must be “oceanbase-execute-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"An \"oceanbase-execute-sql\" tool executes a SQL statement against an OceanBase database.\n","excerpt":"An \"oceanbase-execute-sql\" tool executes a SQL statement against an …","ref":"/genai-toolbox/resources/tools/oceanbase/oceanbase-execute-sql/","tags":"","title":"oceanbase-execute-sql"},{"body":"About An oceanbase-sql tool executes a pre-defined SQL statement against an OceanBase database. It’s compatible with the following source:\noceanbase The specified SQL statement is executed as a prepared statement, and expects parameters in the SQL query to be in the form of placeholders ?.\nExample Note: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\ntools: search_flights_by_number: kind: oceanbase-sql source: my-oceanbase-instance statement: | SELECT * FROM flights WHERE airline = ? AND flight_number = ? LIMIT 10 description: | Use this tool to get information for a specific flight. Takes an airline code and flight number and returns info on the flight. Do NOT use this tool with a flight id. Do NOT guess an airline code or flight number. Example: {{ \"airline\": \"CY\", \"flight_number\": \"888\", }} parameters: - name: airline type: string description: Airline unique 2 letter identifier - name: flight_number type: string description: 1 to 4 digit number Example with Template Parameters Note: This tool allows direct modifications to the SQL statement, including identifiers, column names, and table names. This makes it more vulnerable to SQL injections. Using basic parameters only (see above) is recommended for performance and safety reasons.\ntools: list_table: kind: oceanbase-sql source: my-oceanbase-instance statement: | SELECT * FROM {{.tableName}}; description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", }} templateParameters: - name: tableName type: string description: Table to select from Example with Array Parameters tools: search_flights_by_ids: kind: oceanbase-sql source: my-oceanbase-instance statement: | SELECT * FROM flights WHERE id IN (?) AND status IN (?) description: | Use this tool to get information for multiple flights by their IDs and statuses. Example: {{ \"flight_ids\": [1, 2, 3], \"statuses\": [\"active\", \"scheduled\"] }} parameters: - name: flight_ids type: array description: List of flight IDs to search for items: name: flight_id type: integer description: Individual flight ID - name: statuses type: array description: List of flight statuses to filter by items: name: status type: string description: Individual flight status Reference field type required description kind string true Must be “oceanbase-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. statement string true SQL statement to execute on. parameters parameters false List of parameters that will be inserted into the SQL statement. templateParameters templateParameters false List of templateParameters that will be inserted into the SQL statement before executing prepared statement. ","categories":"","description":"An \"oceanbase-sql\" tool executes a pre-defined SQL statement against an OceanBase database.\n","excerpt":"An \"oceanbase-sql\" tool executes a pre-defined SQL statement against …","ref":"/genai-toolbox/resources/tools/oceanbase/oceanbase-sql/","tags":"","title":"oceanbase-sql"},{"body":"","categories":"","description":"Tools that work with Postgres Sources, such as Cloud SQL for Postgres and AlloyDB. \n","excerpt":"Tools that work with Postgres Sources, such as Cloud SQL for Postgres …","ref":"/genai-toolbox/resources/tools/postgres/","tags":"","title":"Postgres"},{"body":"About A postgres-execute-sql tool executes a SQL statement against a Postgres database. It’s compatible with any of the following sources:\nalloydb-postgres cloud-sql-postgres postgres postgres-execute-sql takes one input parameter sql and run the sql statement against the source.\nNote: This tool is intended for developer assistant workflows with human-in-the-loop and shouldn’t be used for production agents.\nExample tools: execute_sql_tool: kind: postgres-execute-sql source: my-pg-instance description: Use this tool to execute sql statement. Reference field type required description kind string true Must be “postgres-execute-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"postgres-execute-sql\" tool executes a SQL statement against a Postgres database.\n","excerpt":"A \"postgres-execute-sql\" tool executes a SQL statement against a …","ref":"/genai-toolbox/resources/tools/postgres/postgres-execute-sql/","tags":"","title":"postgres-execute-sql"},{"body":"About A postgres-sql tool executes a pre-defined SQL statement against a Postgres database. It’s compatible with any of the following sources:\nalloydb-postgres cloud-sql-postgres postgres The specified SQL statement is executed as a prepared statement, and specified parameters will inserted according to their position: e.g. 1 will be the first parameter specified, $@ will be the second parameter, and so on. If template parameters are included, they will be resolved before execution of the prepared statement.\nExample Note: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\ntools: search_flights_by_number: kind: postgres-sql source: my-pg-instance statement: | SELECT * FROM flights WHERE airline = $1 AND flight_number = $2 LIMIT 10 description: | Use this tool to get information for a specific flight. Takes an airline code and flight number and returns info on the flight. Do NOT use this tool with a flight id. Do NOT guess an airline code or flight number. A airline code is a code for an airline service consisting of two-character airline designator and followed by flight number, which is 1 to 4 digit number. For example, if given CY 0123, the airline is \"CY\", and flight_number is \"123\". Another example for this is DL 1234, the airline is \"DL\", and flight_number is \"1234\". If the tool returns more than one option choose the date closes to today. Example: {{ \"airline\": \"CY\", \"flight_number\": \"888\", }} Example: {{ \"airline\": \"DL\", \"flight_number\": \"1234\", }} parameters: - name: airline type: string description: Airline unique 2 letter identifier - name: flight_number type: string description: 1 to 4 digit number Example with Template Parameters Note: This tool allows direct modifications to the SQL statement, including identifiers, column names, and table names. This makes it more vulnerable to SQL injections. Using basic parameters only (see above) is recommended for performance and safety reasons. For more details, please check templateParameters.\ntools: list_table: kind: postgres-sql source: my-pg-instance statement: | SELECT * FROM {{.tableName}} description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", }} templateParameters: - name: tableName type: string description: Table to select from Reference field type required description kind string true Must be “postgres-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. statement string true SQL statement to execute on. parameters parameters false List of parameters that will be inserted into the SQL statement. templateParameters templateParameters false List of templateParameters that will be inserted into the SQL statement before executing prepared statement. ","categories":"","description":"A \"postgres-sql\" tool executes a pre-defined SQL statement against a Postgres database.\n","excerpt":"A \"postgres-sql\" tool executes a pre-defined SQL statement against a …","ref":"/genai-toolbox/resources/tools/postgres/postgres-sql/","tags":"","title":"postgres-sql"},{"body":"About PostgreSQL is a powerful, open source object-relational database system with over 35 years of active development that has earned it a strong reputation for reliability, feature robustness, and performance.\nAvailable Tools postgres-sql\nExecute SQL queries as prepared statements in PostgreSQL.\npostgres-execute-sql\nRun parameterized SQL statements in PostgreSQL.\nPre-built Configurations PostgreSQL using MCP\nConnect your IDE to PostgreSQL using Toolbox. Requirements Database User This source only uses standard authentication. You will need to create a PostgreSQL user to login to the database with.\nExample sources: my-pg-source: kind: postgres host: 127.0.0.1 port: 5432 database: my_db user: ${USER_NAME} password: ${PASSWORD} Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “postgres”. host string true IP address to connect to (e.g. “127.0.0.1”) port string true Port to connect to (e.g. “5432”) database string true Name of the Postgres database to connect to (e.g. “my_db”). user string true Name of the Postgres user to connect as (e.g. “my-pg-user”). password string true Password of the Postgres user (e.g. “my-password”). ","categories":"","description":"PostgreSQL is a powerful, open source object-relational database.\n","excerpt":"PostgreSQL is a powerful, open source object-relational database.\n","ref":"/genai-toolbox/resources/sources/postgres/","tags":"","title":"PostgreSQL"},{"body":"\nBefore you begin This guide assumes you have already done the following:\nInstalled Python 3.9+ (including pip and your preferred virtual environment tool for managing dependencies e.g. venv).\nInstalled and configured the Google Cloud SDK (gcloud CLI).\nAuthenticated with Google Cloud for Application Default Credentials (ADC):\ngcloud auth login --update-adc Set your default Google Cloud project (replace YOUR_PROJECT_ID with your actual project ID):\ngcloud config set project YOUR_PROJECT_ID export GOOGLE_CLOUD_PROJECT=YOUR_PROJECT_ID Toolbox and the client libraries will use this project for BigQuery, unless overridden in configurations.\nEnabled the BigQuery API in your Google Cloud project.\nInstalled the BigQuery client library for Python:\npip install google-cloud-bigquery Completed setup for usage with an LLM model such as Core LangChain LlamaIndex ADK langchain-vertexai package.\nlangchain-google-genai package.\nlangchain-anthropic package.\nlangchain-vertexai package.\nlangchain-google-genai package.\nlangchain-anthropic package.\nllama-index-llms-google-genai package.\nllama-index-llms-anthropic package.\ngoogle-adk package. Step 1: Set up your BigQuery Dataset and Table In this section, we will create a BigQuery dataset and a table, then insert some data that needs to be accessed by our agent. BigQuery operations are performed against your configured Google Cloud project.\nCreate a new BigQuery dataset (replace YOUR_DATASET_NAME with your desired dataset name, e.g., toolbox_ds, and optionally specify a location like US or EU):\nexport BQ_DATASET_NAME=\"YOUR_DATASET_NAME\" # e.g., toolbox_ds export BQ_LOCATION=\"US\" # e.g., US, EU, asia-northeast1 bq --location=$BQ_LOCATION mk $BQ_DATASET_NAME You can also do this through the Google Cloud Console.\nTip\nFor a real application, ensure that the service account or user running Toolbox has the necessary IAM permissions (e.g., BigQuery Data Editor, BigQuery User) on the dataset or project. For this local quickstart with user credentials, your own permissions will apply.\nThe hotels table needs to be defined in your new dataset for use with the bq query command. First, create a file named create_hotels_table.sql with the following content:\nCREATE TABLE IF NOT EXISTS `YOUR_PROJECT_ID.YOUR_DATASET_NAME.hotels` ( id INT64 NOT NULL, name STRING NOT NULL, location STRING NOT NULL, price_tier STRING NOT NULL, checkin_date DATE NOT NULL, checkout_date DATE NOT NULL, booked BOOLEAN NOT NULL ); Note: Replace YOUR_PROJECT_ID and YOUR_DATASET_NAME in the SQL with your actual project ID and dataset name.\nThen run the command below to execute the sql query:\nbq query --project_id=$GOOGLE_CLOUD_PROJECT --dataset_id=$BQ_DATASET_NAME --use_legacy_sql=false \u003c create_hotels_table.sql Next, populate the hotels table with some initial data. To do this, create a file named insert_hotels_data.sql and add the following SQL INSERT statement to it.\nINSERT INTO `YOUR_PROJECT_ID.YOUR_DATASET_NAME.hotels` (id, name, location, price_tier, checkin_date, checkout_date, booked) VALUES (1, 'Hilton Basel', 'Basel', 'Luxury', '2024-04-20', '2024-04-22', FALSE), (2, 'Marriott Zurich', 'Zurich', 'Upscale', '2024-04-14', '2024-04-21', FALSE), (3, 'Hyatt Regency Basel', 'Basel', 'Upper Upscale', '2024-04-02', '2024-04-20', FALSE), (4, 'Radisson Blu Lucerne', 'Lucerne', 'Midscale', '2024-04-05', '2024-04-24', FALSE), (5, 'Best Western Bern', 'Bern', 'Upper Midscale', '2024-04-01', '2024-04-23', FALSE), (6, 'InterContinental Geneva', 'Geneva', 'Luxury', '2024-04-23', '2024-04-28', FALSE), (7, 'Sheraton Zurich', 'Zurich', 'Upper Upscale', '2024-04-02', '2024-04-27', FALSE), (8, 'Holiday Inn Basel', 'Basel', 'Upper Midscale', '2024-04-09', '2024-04-24', FALSE), (9, 'Courtyard Zurich', 'Zurich', 'Upscale', '2024-04-03', '2024-04-13', FALSE), (10, 'Comfort Inn Bern', 'Bern', 'Midscale', '2024-04-04', '2024-04-16', FALSE); Note: Replace YOUR_PROJECT_ID and YOUR_DATASET_NAME in the SQL with your actual project ID and dataset name.\nThen run the command below to execute the sql query:\nbq query --project_id=$GOOGLE_CLOUD_PROJECT --dataset_id=$BQ_DATASET_NAME --use_legacy_sql=false \u003c insert_hotels_data.sql Step 2: Install and configure Toolbox In this section, we will download Toolbox, configure our tools in a tools.yaml to use BigQuery, and then run the Toolbox server.\nDownload the latest version of Toolbox as a binary:\nTip\nSelect the correct binary corresponding to your OS and CPU architecture.\nexport OS=\"linux/amd64\" # one of linux/amd64, darwin/arm64, darwin/amd64, or windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/$OS/toolbox Make the binary executable:\nchmod +x toolbox Write the following into a tools.yaml file. You must replace the YOUR_PROJECT_ID and YOUR_DATASET_NAME placeholder in the config with your actual BigQuery project and dataset name. The location field is optional; if not specified, it defaults to ‘us’. The table name hotels is used directly in the statements.\nTip\nAuthentication with BigQuery is handled via Application Default Credentials (ADC). Ensure you have run gcloud auth application-default login.\nsources: my-bigquery-source: kind: bigquery project: YOUR_PROJECT_ID location: us tools: search-hotels-by-name: kind: bigquery-sql source: my-bigquery-source description: Search for hotels based on name. parameters: - name: name type: string description: The name of the hotel. statement: SELECT * FROM `YOUR_DATASET_NAME.hotels` WHERE LOWER(name) LIKE LOWER(CONCAT('%', @name, '%')); search-hotels-by-location: kind: bigquery-sql source: my-bigquery-source description: Search for hotels based on location. parameters: - name: location type: string description: The location of the hotel. statement: SELECT * FROM `YOUR_DATASET_NAME.hotels` WHERE LOWER(location) LIKE LOWER(CONCAT('%', @location, '%')); book-hotel: kind: bigquery-sql source: my-bigquery-source description: \u003e- Book a hotel by its ID. If the hotel is successfully booked, returns a NULL, raises an error if not. parameters: - name: hotel_id type: integer description: The ID of the hotel to book. statement: UPDATE `YOUR_DATASET_NAME.hotels` SET booked = TRUE WHERE id = @hotel_id; update-hotel: kind: bigquery-sql source: my-bigquery-source description: \u003e- Update a hotel's check-in and check-out dates by its ID. Returns a message indicating whether the hotel was successfully updated or not. parameters: - name: checkin_date type: string description: The new check-in date of the hotel. - name: checkout_date type: string description: The new check-out date of the hotel. - name: hotel_id type: integer description: The ID of the hotel to update. statement: \u003e- UPDATE `YOUR_DATASET_NAME.hotels` SET checkin_date = PARSE_DATE('%Y-%m-%d', @checkin_date), checkout_date = PARSE_DATE('%Y-%m-%d', @checkout_date) WHERE id = @hotel_id; cancel-hotel: kind: bigquery-sql source: my-bigquery-source description: Cancel a hotel by its ID. parameters: - name: hotel_id type: integer description: The ID of the hotel to cancel. statement: UPDATE `YOUR_DATASET_NAME.hotels` SET booked = FALSE WHERE id = @hotel_id; Important Note on toolsets: The tools.yaml content above does not include a toolsets section. The Python agent examples in Step 3 (e.g., await toolbox_client.load_toolset(\"my-toolset\")) rely on a toolset named my-toolset. To make those examples work, you will need to add a toolsets section to your tools.yaml file, for example:\n# Add this to your tools.yaml if using load_toolset(\"my-toolset\") # Ensure it's at the same indentation level as 'sources:' and 'tools:' toolsets: my-toolset: - search-hotels-by-name - search-hotels-by-location - book-hotel - update-hotel - cancel-hotel Alternatively, you can modify the agent code to load tools individually (e.g., using await toolbox_client.load_tool(\"search-hotels-by-name\")).\nFor more info on tools, check out the Resources section of the docs.\nRun the Toolbox server, pointing to the tools.yaml file created earlier:\n./toolbox --tools-file \"tools.yaml\" Note\nToolbox enables dynamic reloading by default. To disable, use the `--disable-reload` flag. Step 3: Connect your agent to Toolbox In this section, we will write and run an agent that will load the Tools from Toolbox.\nTip\nIf you prefer to experiment within a Google Colab environment, you can connect to a local runtime.\nIn a new terminal, install the SDK package.\nCore Langchain LlamaIndex ADK pip install toolbox-core pip install toolbox-langchain pip install toolbox-llamaindex pip install google-adk Install other required dependencies:\nCore Langchain LlamaIndex ADK # TODO(developer): replace with correct package if needed pip install langgraph langchain-google-vertexai # pip install langchain-google-genai # pip install langchain-anthropic # TODO(developer): replace with correct package if needed pip install langgraph langchain-google-vertexai # pip install langchain-google-genai # pip install langchain-anthropic # TODO(developer): replace with correct package if needed pip install llama-index-llms-google-genai # pip install llama-index-llms-anthropic pip install toolbox-core Create a new file named hotel_agent.py and copy the following code to create an agent: Core LangChain LlamaIndex ADK import asyncio from google import genai from google.genai.types import ( Content, FunctionDeclaration, GenerateContentConfig, Part, Tool, ) from toolbox_core import ToolboxClient prompt = \"\"\" You're a helpful hotel assistant. You handle hotel searching, booking and cancellations. When the user searches for a hotel, mention it's name, id, location and price tier. Always mention hotel id while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. \"\"\" queries = [ \"Find hotels in Basel with Basel in it's name.\", \"Please book the hotel Hilton Basel for me.\", \"This is too expensive. Please cancel it.\", \"Please book Hyatt Regency for me\", \"My check in dates for my booking would be from April 10, 2024 to April 19, 2024.\", ] async def run_application(): async with ToolboxClient(\"\u003chttp://127.0.0.1:5000\u003e\") as toolbox_client: # The toolbox_tools list contains Python callables (functions/methods) designed for LLM tool-use # integration. While this example uses Google's genai client, these callables can be adapted for # various function-calling or agent frameworks. For easier integration with supported frameworks # (https://github.com/googleapis/mcp-toolbox-python-sdk/tree/main/packages), use the # provided wrapper packages, which handle framework-specific boilerplate. toolbox_tools = await toolbox_client.load_toolset(\"my-toolset\") genai_client = genai.Client( vertexai=True, project=\"project-id\", location=\"us-central1\" ) genai_tools = [ Tool( function_declarations=[ FunctionDeclaration.from_callable_with_api_option(callable=tool) ] ) for tool in toolbox_tools ] history = [] for query in queries: user_prompt_content = Content( role=\"user\", parts=[Part.from_text(text=query)], ) history.append(user_prompt_content) response = genai_client.models.generate_content( model=\"gemini-2.0-flash-001\", contents=history, config=GenerateContentConfig( system_instruction=prompt, tools=genai_tools, ), ) history.append(response.candidates[0].content) function_response_parts = [] for function_call in response.function_calls: fn_name = function_call.name # The tools are sorted alphabetically if fn_name == \"search-hotels-by-name\": function_result = await toolbox_tools[3](**function_call.args) elif fn_name == \"search-hotels-by-location\": function_result = await toolbox_tools[2](**function_call.args) elif fn_name == \"book-hotel\": function_result = await toolbox_tools[0](**function_call.args) elif fn_name == \"update-hotel\": function_result = await toolbox_tools[4](**function_call.args) elif fn_name == \"cancel-hotel\": function_result = await toolbox_tools[1](**function_call.args) else: raise ValueError(\"Function name not present.\") function_response = {\"result\": function_result} function_response_part = Part.from_function_response( name=function_call.name, response=function_response, ) function_response_parts.append(function_response_part) if function_response_parts: tool_response_content = Content(role=\"tool\", parts=function_response_parts) history.append(tool_response_content) response2 = genai_client.models.generate_content( model=\"gemini-2.0-flash-001\", contents=history, config=GenerateContentConfig( tools=genai_tools, ), ) final_model_response_content = response2.candidates[0].content history.append(final_model_response_content) print(response2.text) asyncio.run(run_application()) import asyncio from langgraph.prebuilt import create_react_agent # TODO(developer): replace this with another import if needed from langchain_google_vertexai import ChatVertexAI # from langchain_google_genai import ChatGoogleGenerativeAI # from langchain_anthropic import ChatAnthropic from langgraph.checkpoint.memory import MemorySaver from toolbox_langchain import ToolboxClient prompt = \"\"\" You're a helpful hotel assistant. You handle hotel searching, booking and cancellations. When the user searches for a hotel, mention it's name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. \"\"\" queries = [ \"Find hotels in Basel with Basel in it's name.\", \"Can you book the Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", ] async def main(): # TODO(developer): replace this with another model if needed model = ChatVertexAI(model_name=\"gemini-2.0-flash-001\") # model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\") # model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\") # Load the tools from the Toolbox server client = ToolboxClient(\"http://127.0.0.1:5000\") tools = await client.aload_toolset() agent = create_react_agent(model, tools, checkpointer=MemorySaver()) config = {\"configurable\": {\"thread_id\": \"thread-1\"}} for query in queries: inputs = {\"messages\": [(\"user\", prompt + query)]} response = await agent.ainvoke(inputs, stream_mode=\"values\", config=config) print(response[\"messages\"][-1].content) asyncio.run(main()) import asyncio import os from llama_index.core.agent.workflow import AgentWorkflow from llama_index.core.workflow import Context # TODO(developer): replace this with another import if needed from llama_index.llms.google_genai import GoogleGenAI # from llama_index.llms.anthropic import Anthropic from toolbox_llamaindex import ToolboxClient prompt = \"\"\" You're a helpful hotel assistant. You handle hotel searching, booking and cancellations. When the user searches for a hotel, mention it's name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. \"\"\" queries = [ \"Find hotels in Basel with Basel in it's name.\", \"Can you book the Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", ] async def main(): # TODO(developer): replace this with another model if needed llm = GoogleGenAI( model=\"gemini-2.0-flash-001\", vertexai_config={\"location\": \"us-central1\"}, ) # llm = GoogleGenAI( # api_key=os.getenv(\"GOOGLE_API_KEY\"), # model=\"gemini-2.0-flash-001\", # ) # llm = Anthropic( # model=\"claude-3-7-sonnet-latest\", # api_key=os.getenv(\"ANTHROPIC_API_KEY\") # ) # Load the tools from the Toolbox server client = ToolboxClient(\"http://127.0.0.1:5000\") tools = await client.aload_toolset() agent = AgentWorkflow.from_tools_or_functions( tools, llm=llm, system_prompt=prompt, ) ctx = Context(agent) for query in queries: response = await agent.arun(user_msg=query, ctx=ctx) print(f\"---- {query} ----\") print(str(response)) asyncio.run(main()) from google.adk.agents import Agent from google.adk.runners import Runner from google.adk.sessions import InMemorySessionService from google.adk.artifacts.in_memory_artifact_service import InMemoryArtifactService from google.genai import types # For constructing message content from toolbox_core import ToolboxSyncClient import os os.environ['GOOGLE_GENAI_USE_VERTEXAI'] = 'True' # TODO(developer): Replace 'YOUR_PROJECT_ID' with your Google Cloud Project ID os.environ['GOOGLE_CLOUD_PROJECT'] = 'YOUR_PROJECT_ID' # TODO(developer): Replace 'us-central1' with your Google Cloud Location (region) os.environ['GOOGLE_CLOUD_LOCATION'] = 'us-central1' # --- Load Tools from Toolbox --- # TODO(developer): Ensure the Toolbox server is running at \u003chttp://127.0.0.1:5000\u003e with ToolboxSyncClient(\"\u003chttp://127.0.0.1:5000\u003e\") as toolbox_client: # TODO(developer): Replace \"my-toolset\" with the actual ID of your toolset as configured in your MCP Toolbox server. agent_toolset = toolbox_client.load_toolset(\"my-toolset\") # --- Define the Agent's Prompt --- prompt = \"\"\" You're a helpful hotel assistant. You handle hotel searching, booking and cancellations. When the user searches for a hotel, mention it's name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. \"\"\" # --- Configure the Agent --- root_agent = Agent( model='gemini-2.0-flash-001', name='hotel_agent', description='A helpful AI assistant that can search and book hotels.', instruction=prompt, tools=agent_toolset, # Pass the loaded toolset ) # --- Initialize Services for Running the Agent --- session_service = InMemorySessionService() artifacts_service = InMemoryArtifactService() # Create a new session for the interaction. session = session_service.create_session( state={}, app_name='hotel_agent', user_id='123' ) runner = Runner( app_name='hotel_agent', agent=root_agent, artifact_service=artifacts_service, session_service=session_service, ) # --- Define Queries and Run the Agent --- queries = [ \"Find hotels in Basel with Basel in it's name.\", \"Can you book the Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", ] for query in queries: content = types.Content(role='user', parts=[types.Part(text=query)]) events = runner.run(session_id=session.id, user_id='123', new_message=content) responses = ( part.text for event in events for part in event.content.parts if part.text is not None ) for text in responses: print(text) Core Langchain LlamaIndex ADK To learn more about the Core SDK, check out the Toolbox Core SDK documentation.\nTo learn more about Agents in LangChain, check out the LangGraph Agent documentation.\nTo learn more about Agents in LlamaIndex, check out the LlamaIndex AgentWorkflow documentation.\nTo learn more about Agents in ADK, check out the ADK documentation.\nRun your agent, and observe the results:\npython hotel_agent.py ","categories":"","description":"How to get started running Toolbox locally with Python, BigQuery, and LangGraph, LlamaIndex, or ADK.\n","excerpt":"How to get started running Toolbox locally with Python, BigQuery, and …","ref":"/genai-toolbox/samples/bigquery/local_quickstart/","tags":"","title":"Quickstart (Local with BigQuery)"},{"body":"About A redis tool executes a series of pre-defined Redis commands against a Redis source.\nThe specified Redis commands are executed sequentially. Each command is represented as a string list, where the first element is the command name (e.g., SET, GET, HGETALL) and subsequent elements are its arguments.\nDynamic Command Parameters Command arguments can be templated using the $variableName annotation. The array type parameters will be expanded once into multiple arguments. Take the following config for example:\ncommands: - [SADD, userNames, $userNames] # Array will be flattened into multiple arguments. parameters: - name: userNames type: array description: The user names to be set. If the input is an array of strings [\"Alice\", \"Sid\", \"Bob\"], The final command to be executed after argument expansion will be [SADD, userNames, Alice, Sid, Bob].\nExample tools: user_data_tool: kind: redis source: my-redis-instance description: | Use this tool to interact with user data stored in Redis. It can set, retrieve, and delete user-specific information. commands: - [SADD, userNames, $userNames] # Array will be flattened into multiple arguments. - [GET, $userId] parameters: - name: userId type: string description: The unique identifier for the user. - name: userNames type: array description: The user names to be set. ","categories":"","description":"A \"redis\" tool executes a set of pre-defined Redis commands against a Redis instance.\n","excerpt":"A \"redis\" tool executes a set of pre-defined Redis commands against a …","ref":"/genai-toolbox/resources/tools/redis/redis/","tags":"","title":"redis"},{"body":"About Redis is an open-source, in-memory data structure store, used as a database, cache, and message broker. It supports data structures such as strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, and geospatial indexes with radius queries.\nIf you are new to Redis, you can find installation and getting started guides on the official Redis website.\nAvailable Tools redis\nRun Redis commands and interact with key-value pairs. Requirements Redis AUTH string is a password for connection to Redis. If you have the requirepass directive set in your Redis configuration, incoming client connections must authenticate in order to connect.\nSpecify your AUTH string in the password field:\nsources: my-redis-instance: kind: redis address: - 127.0.0.1:6379 username: ${MY_USER_NAME} password: ${MY_AUTH_STRING} # Omit this field if you don't have a password. # database: 0 # clusterEnabled: false # useGCPIAM: false Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nMemorystore For Redis Memorystore standalone instances support authentication using an AUTH string.\nHere is an example tools.yaml config with AUTH enabled:\nsources: my-redis-cluster-instance: kind: memorystore-redis address: - 127.0.0.1:6379 password: ${MY_AUTH_STRING} # useGCPIAM: false # clusterEnabled: false Memorystore Redis Cluster supports IAM authentication instead. Grant your account the required IAM role and make sure to set useGCPIAM to true.\nHere is an example tools.yaml config for Memorystore Redis Cluster instances using IAM authentication:\nsources: my-redis-cluster-instance: kind: memorystore-redis address: - 127.0.0.1:6379 useGCPIAM: true clusterEnabled: true Reference field type required description kind string true Must be “memorystore-redis”. address string true Primary endpoint for the Memorystore Redis instance to connect to. username string false If you are using a non-default user, specify the user name here. If you are using Memorystore for Redis, leave this field blank password string false If you have Redis AUTH enabled, specify the AUTH string here database int false The Redis database to connect to. Not applicable for cluster enabled instances. The default database is 0. clusterEnabled bool false Set it to true if using a Redis Cluster instance. Defaults to false. useGCPIAM string false Set it to true if you are using GCP’s IAM authentication. Defaults to false. ","categories":"","description":"Redis is an open-source, in-memory data structure store.\n","excerpt":"Redis is an open-source, in-memory data structure store.\n","ref":"/genai-toolbox/resources/sources/redis/","tags":"","title":"Redis"},{"body":"","categories":"","description":"Tools that work with Redis Sources.\n","excerpt":"Tools that work with Redis Sources.\n","ref":"/genai-toolbox/resources/tools/redis/","tags":"","title":"Redis"},{"body":"A Source represents a data sources that a tool can interact with. You can define Sources as a map in the sources section of your tools.yaml file. Typically, a source configuration will contain any information needed to connect with and interact with the database.\nTip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nsources: my-cloud-sql-source: kind: cloud-sql-postgres project: my-project-id region: us-central1 instance: my-instance-name database: my_db user: ${USER_NAME} password: ${PASSWORD} In implementation, each source is a different connection pool or client that used to connect to the database and execute the tool.\nAvailable Sources ","categories":"","description":"Sources represent your different data sources that a tool can interact with.\n","excerpt":"Sources represent your different data sources that a tool can interact …","ref":"/genai-toolbox/resources/sources/","tags":"","title":"Sources"},{"body":"Spanner Source Spanner is a fully managed, mission-critical database service that brings together relational, graph, key-value, and search. It offers transactional consistency at global scale, automatic, synchronous replication for high availability, and support for two SQL dialects: GoogleSQL (ANSI 2011 with extensions) and PostgreSQL.\nIf you are new to Spanner, you can try to create and query a database using the Google Cloud console.\nAvailable Tools spanner-sql\nExecute SQL on Google Cloud Spanner.\nspanner-execute-sql\nRun structured and parameterized queries on Spanner.\nPre-built Configurations Spanner using MCP\nConnect your IDE to Spanner using Toolbox. Requirements IAM Permissions Spanner uses Identity and Access Management (IAM) to control user and group access to Spanner resources at the project, Spanner instance, and Spanner database levels. Toolbox will use your Application Default Credentials (ADC) to authorize and authenticate when interacting with Spanner.\nIn addition to setting the ADC for your server, you need to ensure the IAM identity has been given the correct IAM permissions for the query provided. See Apply IAM roles for more information on applying IAM permissions and roles to an identity.\nExample sources: my-spanner-source: kind: \"spanner\" project: \"my-project-id\" instance: \"my-instance\" database: \"my_db\" # dialect: \"googlesql\" Reference field type required description kind string true Must be “spanner”. project string true Id of the GCP project that the cluster was created in (e.g. “my-project-id”). instance string true Name of the Spanner instance. database string true Name of the database on the Spanner instance dialect string false Name of the dialect type of the Spanner database, must be either googlesql or postgresql. Default: googlesql. ","categories":"","description":"Spanner is a fully managed database service from Google Cloud that combines  relational, key-value, graph, and search capabilities.\n","excerpt":"Spanner is a fully managed database service from Google Cloud that …","ref":"/genai-toolbox/resources/sources/spanner/","tags":"","title":"Spanner"},{"body":"","categories":"","description":"Tools that work with Spanner Sources.\n","excerpt":"Tools that work with Spanner Sources.\n","ref":"/genai-toolbox/resources/tools/spanner/","tags":"","title":"Spanner"},{"body":"About A spanner-execute-sql tool executes a SQL statement against a Spanner database. It’s compatible with any of the following sources:\nspanner spanner-execute-sql takes one input parameter sql and run the sql statement against the source.\nNote: This tool is intended for developer assistant workflows with human-in-the-loop and shouldn’t be used for production agents.\nExample tools: execute_sql_tool: kind: spanner-execute-sql source: my-spanner-instance description: Use this tool to execute sql statement. Reference field type required description kind string true Must be “spanner-execute-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. readOnly bool false When set to true, the statement is run as a read-only transaction. Default: false. ","categories":"","description":"A \"spanner-execute-sql\" tool executes a SQL statement against a Spanner database.\n","excerpt":"A \"spanner-execute-sql\" tool executes a SQL statement against a …","ref":"/genai-toolbox/resources/tools/spanner/spanner-execute-sql/","tags":"","title":"spanner-execute-sql"},{"body":"About A spanner-sql tool executes a pre-defined SQL statement (either googlesql or postgresql) against a Cloud Spanner database. It’s compatible with any of the following sources:\nspanner GoogleSQL For the googlesql dialect, the specified SQL statement is executed as a data manipulation language (DML) statements, and specified parameters will inserted according to their name: e.g. @name.\nNote: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\nPostgreSQL For the postgresql dialect, the specified SQL statement is executed as a prepared statement, and specified parameters will inserted according to their position: e.g. $1 will be the first parameter specified, $@ will be the second parameter, and so on.\nExample Note: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\nGoogleSQL PostgreSQL tools: search_flights_by_number: kind: spanner-sql source: my-spanner-instance statement: | SELECT * FROM flights WHERE airline = @airline AND flight_number = @flight_number LIMIT 10 description: | Use this tool to get information for a specific flight. Takes an airline code and flight number and returns info on the flight. Do NOT use this tool with a flight id. Do NOT guess an airline code or flight number. A airline code is a code for an airline service consisting of two-character airline designator and followed by flight number, which is 1 to 4 digit number. For example, if given CY 0123, the airline is \"CY\", and flight_number is \"123\". Another example for this is DL 1234, the airline is \"DL\", and flight_number is \"1234\". If the tool returns more than one option choose the date closes to today. Example: {{ \"airline\": \"CY\", \"flight_number\": \"888\", }} Example: {{ \"airline\": \"DL\", \"flight_number\": \"1234\", }} parameters: - name: airline type: string description: Airline unique 2 letter identifier - name: flight_number type: string description: 1 to 4 digit number tools: search_flights_by_number: kind: spanner source: my-spanner-instance statement: | SELECT * FROM flights WHERE airline = $1 AND flight_number = $2 LIMIT 10 description: | Use this tool to get information for a specific flight. Takes an airline code and flight number and returns info on the flight. Do NOT use this tool with a flight id. Do NOT guess an airline code or flight number. A airline code is a code for an airline service consisting of two-character airline designator and followed by flight number, which is 1 to 4 digit number. For example, if given CY 0123, the airline is \"CY\", and flight_number is \"123\". Another example for this is DL 1234, the airline is \"DL\", and flight_number is \"1234\". If the tool returns more than one option choose the date closes to today. Example: {{ \"airline\": \"CY\", \"flight_number\": \"888\", }} Example: {{ \"airline\": \"DL\", \"flight_number\": \"1234\", }} parameters: - name: airline type: string description: Airline unique 2 letter identifier - name: flight_number type: string description: 1 to 4 digit number Example with Template Parameters Note: This tool allows direct modifications to the SQL statement, including identifiers, column names, and table names. This makes it more vulnerable to SQL injections. Using basic parameters only (see above) is recommended for performance and safety reasons. For more details, please check templateParameters.\ntools: list_table: kind: spanner source: my-spanner-instance statement: | SELECT * FROM {{.tableName}}; description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", }} templateParameters: - name: tableName type: string description: Table to select from Reference field type required description kind string true Must be “spanner-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. statement string true SQL statement to execute on. parameters parameters false List of parameters that will be inserted into the SQL statement. readOnly bool false When set to true, the statement is run as a read-only transaction. Default: false. templateParameters templateParameters false List of templateParameters that will be inserted into the SQL statement before executing prepared statement. ","categories":"","description":"A \"spanner-sql\" tool executes a pre-defined SQL statement against a Google Cloud Spanner database.\n","excerpt":"A \"spanner-sql\" tool executes a pre-defined SQL statement against a …","ref":"/genai-toolbox/resources/tools/spanner/spanner-sql/","tags":"","title":"spanner-sql"},{"body":"About SQL Server is a relational database management system (RDBMS) developed by Microsoft that allows users to store, retrieve, and manage large amount of data through a structured format.\nAvailable Tools mssql-sql\nExecute pre-defined SQL Server queries with placeholder parameters.\nmssql-execute-sql\nRun parameterized SQL Server queries in SQL Server.\nRequirements Database User This source only uses standard authentication. You will need to create a SQL Server user to login to the database with.\nExample sources: my-mssql-source: kind: mssql host: 127.0.0.1 port: 1433 database: my_db user: ${USER_NAME} password: ${PASSWORD} # encrypt: strict Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “mssql”. host string true IP address to connect to (e.g. “127.0.0.1”). port string true Port to connect to (e.g. “1433”). database string true Name of the SQL Server database to connect to (e.g. “my_db”). user string true Name of the SQL Server user to connect as (e.g. “my-user”). password string true Password of the SQL Server user (e.g. “my-password”). encrypt string false Encryption level for data transmitted between the client and server (e.g., “strict”). If not specified, defaults to the github.com/microsoft/go-mssqldb package’s default encrypt value. ","categories":"","description":"SQL Server is a relational database management system (RDBMS).\n","excerpt":"SQL Server is a relational database management system (RDBMS).\n","ref":"/genai-toolbox/resources/sources/mssql/","tags":"","title":"SQL Server"},{"body":"","categories":"","description":"Tools that work with SQL Server Sources, such as CloudSQL for SQL Server.\n","excerpt":"Tools that work with SQL Server Sources, such as CloudSQL for SQL …","ref":"/genai-toolbox/resources/tools/mssql/","tags":"","title":"SQL Server"},{"body":"About SQLite is a software library that provides a relational database management system. The lite in SQLite means lightweight in terms of setup, database administration, and required resources.\nSQLite has the following notable characteristics:\nSelf-contained with no external dependencies Serverless - the SQLite library accesses its storage files directly Single database file that can be easily copied or moved Zero-configuration - no setup or administration needed Transactional with ACID properties Available Tools sqlite-sql\nRun SQL queries against a local SQLite database. Requirements Database File You need a SQLite database file. This can be:\nAn existing database file A path where a new database file should be created :memory: for an in-memory database Example sources: my-sqlite-db: kind: \"sqlite\" database: \"/path/to/database.db\" For an in-memory database:\nsources: my-sqlite-memory-db: kind: \"sqlite\" database: \":memory:\" Reference Configuration Fields field type required description kind string true Must be “spanner”. database string true Path to SQLite database file, or “:memory:” for an in-memory database. Connection Properties SQLite connections are configured with these defaults for optimal performance:\nMaxOpenConns: 1 (SQLite only supports one writer at a time) MaxIdleConns: 1 ","categories":"","description":"SQLite is a C-language library that implements a small, fast, self-contained,  high-reliability, full-featured, SQL database engine.\n","excerpt":"SQLite is a C-language library that implements a small, fast, …","ref":"/genai-toolbox/resources/sources/sqlite/","tags":"","title":"SQLite"},{"body":"","categories":"","description":"Tools that work with SQLite Sources.\n","excerpt":"Tools that work with SQLite Sources.\n","ref":"/genai-toolbox/resources/tools/sqlite/","tags":"","title":"SQLite"},{"body":"About A sqlite-sql tool executes SQL statements against a SQLite database. It’s compatible with any of the following sources:\nsqlite SQLite uses the ? placeholder for parameters in SQL statements. Parameters are bound in the order they are provided.\nThe statement field supports any valid SQLite SQL statement, including SELECT, INSERT, UPDATE, DELETE, CREATE/ALTER/DROP table statements, and other DDL statements.\nExample Note: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\ntools: search-users: kind: sqlite-sql source: my-sqlite-db description: Search users by name and age parameters: - name: name type: string description: The name to search for - name: min_age type: integer description: Minimum age statement: SELECT * FROM users WHERE name LIKE ? AND age \u003e= ? Example with Template Parameters Note: This tool allows direct modifications to the SQL statement, including identifiers, column names, and table names. This makes it more vulnerable to SQL injections. Using basic parameters only (see above) is recommended for performance and safety reasons. For more details, please check templateParameters.\ntools: list_table: kind: sqlite-sql source: my-sqlite-db statement: | SELECT * FROM {{.tableName}}; description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", }} templateParameters: - name: tableName type: string description: Table to select from Reference field type required description kind string true Must be “sqlite-sql”. source string true Name of the source the SQLite source configuration. description string true Description of the tool that is passed to the LLM. statement string true The SQL statement to execute. parameters parameters false List of parameters that will be inserted into the SQL statement. templateParameters templateParameters false List of templateParameters that will be inserted into the SQL statement before executing prepared statement. ","categories":"","description":"Execute SQL statements against a SQLite database.\n","excerpt":"Execute SQL statements against a SQLite database.\n","ref":"/genai-toolbox/resources/tools/sqlite/sqlite-sql/","tags":"","title":"sqlite-sql"},{"body":"About TiDB is an open-source distributed SQL database that supports Hybrid Transactional and Analytical Processing (HTAP) workloads. It is MySQL-compatible and features horizontal scalability, strong consistency, and high availability.\nRequirements Database User This source uses standard MySQL protocol authentication. You will need to create a TiDB user to login to the database with.\nFor TiDB Cloud users, you can create database users through the TiDB Cloud console.\nSSL Configuration TiDB Cloud\nFor TiDB Cloud instances, SSL is automatically enabled when the hostname matches the TiDB Cloud pattern (gateway*.*.*.tidbcloud.com). You don’t need to explicitly set ssl: true for TiDB Cloud connections.\nSelf-Hosted TiDB\nFor self-hosted TiDB instances, you can optionally enable SSL by setting ssl: true in your configuration.\nExample TiDB Cloud\nsources: my-tidb-cloud-source: kind: tidb host: gateway01.us-west-2.prod.aws.tidbcloud.com port: 4000 database: my_db user: ${TIDB_USERNAME} password: ${TIDB_PASSWORD} # SSL is automatically enabled for TiDB Cloud Self-Hosted TiDB\nsources: my-tidb-source: kind: tidb host: 127.0.0.1 port: 4000 database: my_db user: ${TIDB_USERNAME} password: ${TIDB_PASSWORD} # ssl: true # Optional: enable SSL for secure connections Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nReference field type required description kind string true Must be “tidb”. host string true IP address or hostname to connect to (e.g. “127.0.0.1” or “gateway01.*.tidbcloud.com”). port string true Port to connect to (typically “4000” for TiDB). database string true Name of the TiDB database to connect to (e.g. “my_db”). user string true Name of the TiDB user to connect as (e.g. “my-tidb-user”). password string true Password of the TiDB user (e.g. “my-password”). ssl boolean false Whether to use SSL/TLS encryption. Automatically enabled for TiDB Cloud instances. ","categories":"","description":"TiDB is a distributed SQL database that combines the best of traditional RDBMS and NoSQL databases.\n","excerpt":"TiDB is a distributed SQL database that combines the best of …","ref":"/genai-toolbox/resources/sources/tidb/","tags":"","title":"TiDB"},{"body":"","categories":"","description":"Tools that work with TiDB Sources, such as TiDB Cloud and self-hosted TiDB.\n","excerpt":"Tools that work with TiDB Sources, such as TiDB Cloud and self-hosted …","ref":"/genai-toolbox/resources/tools/tidb/","tags":"","title":"TiDB"},{"body":"About A tidb-execute-sql tool executes a SQL statement against a TiDB database. It’s compatible with the following source:\ntidb tidb-execute-sql takes one input parameter sql and run the sql statement against the source.\nNote: This tool is intended for developer assistant workflows with human-in-the-loop and shouldn’t be used for production agents.\nExample tools: execute_sql_tool: kind: tidb-execute-sql source: my-tidb-instance description: Use this tool to execute sql statement. Reference field type required description kind string true Must be “tidb-execute-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. ","categories":"","description":"A \"tidb-execute-sql\" tool executes a SQL statement against a TiDB database.\n","excerpt":"A \"tidb-execute-sql\" tool executes a SQL statement against a TiDB …","ref":"/genai-toolbox/resources/tools/tidb/tidb-execute-sql/","tags":"","title":"tidb-execute-sql"},{"body":"About A tidb-sql tool executes a pre-defined SQL statement against a TiDB database. It’s compatible with the following source:\ntidb The specified SQL statement is executed as a prepared statement, and expects parameters in the SQL query to be in the form of placeholders ?.\nExample Note: This tool uses parameterized queries to prevent SQL injections. Query parameters can be used as substitutes for arbitrary expressions. Parameters cannot be used as substitutes for identifiers, column names, table names, or other parts of the query.\ntools: search_flights_by_number: kind: tidb-sql source: my-tidb-instance statement: | SELECT * FROM flights WHERE airline = ? AND flight_number = ? LIMIT 10 description: | Use this tool to get information for a specific flight. Takes an airline code and flight number and returns info on the flight. Do NOT use this tool with a flight id. Do NOT guess an airline code or flight number. A airline code is a code for an airline service consisting of two-character airline designator and followed by flight number, which is 1 to 4 digit number. For example, if given CY 0123, the airline is \"CY\", and flight_number is \"123\". Another example for this is DL 1234, the airline is \"DL\", and flight_number is \"1234\". If the tool returns more than one option choose the date closes to today. Example: {{ \"airline\": \"CY\", \"flight_number\": \"888\", }} Example: {{ \"airline\": \"DL\", \"flight_number\": \"1234\", }} parameters: - name: airline type: string description: Airline unique 2 letter identifier - name: flight_number type: string description: 1 to 4 digit number Example with Template Parameters Note: This tool allows direct modifications to the SQL statement, including identifiers, column names, and table names. This makes it more vulnerable to SQL injections. Using basic parameters only (see above) is recommended for performance and safety reasons. For more details, please check templateParameters.\ntools: list_table: kind: tidb-sql source: my-tidb-instance statement: | SELECT * FROM {{.tableName}}; description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", }} templateParameters: - name: tableName type: string description: Table to select from Reference field type required description kind string true Must be “tidb-sql”. source string true Name of the source the SQL should execute on. description string true Description of the tool that is passed to the LLM. statement string true SQL statement to execute on. parameters parameters false List of parameters that will be inserted into the SQL statement. templateParameters templateParameters false List of templateParameters that will be inserted into the SQL statement before executing prepared statement. ","categories":"","description":"A \"tidb-sql\" tool executes a pre-defined SQL statement against a TiDB database.\n","excerpt":"A \"tidb-sql\" tool executes a pre-defined SQL statement against a TiDB …","ref":"/genai-toolbox/resources/tools/tidb/tidb-sql/","tags":"","title":"tidb-sql"},{"body":"Toolbox UI is a built-in web interface that allows users to visually inspect and test out configured resources such as tools and toolsets.\nLaunching Toolbox UI To launch Toolbox’s interactive UI, use the --ui flag.\n./toolbox --ui Toolbox UI will be served from the same host and port as the Toolbox Server, with the /ui suffix. Once Toolbox is launched, the following INFO log with Toolbox UI’s url will be shown:\nINFO \"Toolbox UI is up and running at: http://localhost:5000/ui\" Navigating the Tools Page The tools page shows all tools loaded from your configuration file. This corresponds to the default toolset (represented by an empty string). Each tool’s name on this page will exactly match its name in the configuration file.\nTo view details for a specific tool, click on the tool name. The main content area will be populated with the tool name, description, and available parameters.\nInvoking a Tool Click on a Tool Enter appropriate parameters in each parameter field Click “Run Tool” Done! Your results will appear in the response field (Optional) Uncheck “Prettify JSON” to format the response as plain text Optional Parameters Toolbox allows users to add optional parameters with or without a default value.\nTo exclude a parameter, uncheck the box to the right of an associated parameter, and that parameter will not be included in the request body. If the parameter is not sent, Toolbox will either use it as nil value or the default value, if configured. If the parameter is required, Toolbox will throw an error.\nWhen the box is checked, parameter will be sent exactly as entered in the response field (e.g. empty string).\nEditing Headers To edit headers, press the “Edit Headers” button to display the header modal. Within this modal, users can make direct edits by typing into the header’s text area.\nToolbox UI validates that the headers are in correct JSON format. Other header-related errors (e.g., incorrect header names or values required by the tool) will be reported in the Response section after running the tool.\nGoogle OAuth Currently, Toolbox supports Google OAuth 2.0 as an AuthService, which allows tools to utilize authorized parameters. When a tool uses an authorized parameter, the parameter will be displayed but not editable, as it will be populated from the authentication token.\nTo provide the token, add your Google OAuth ID Token to the request header using the “Edit Headers” button and modal described above. The key should be the name of your AuthService as defined in your tool configuration file, suffixed with _token. The value should be your ID token as a string.\nSelect a tool that requires authenticated parameters The auth parameter’s text field is greyed out. This is because it cannot be entered manually and will be parsed from the resolved auth token To update request headers with the token, select “Edit Headers” (Optional) If you wish to manually edit the header, checkout the dropdown “How to extract Google OAuth ID Token manually” for guidance on retrieving ID token To edit the header automatically, click the “Auto Setup” button that is associated with your Auth Profile Enter the Client ID defined in your tools configuration file Click “Continue” Click “Sign in With Google” and login with your associated google account. This should automatically populate the header text area with your token Click “Save” Click “Run Tool” { \"Content-Type\": \"application/json\", \"my-google-auth_token\": \"YOUR_ID_TOKEN_HERE\" } Navigating the Toolsets Page Through the toolsets page, users can search for a specific toolset to retrieve tools from. Simply enter the toolset name in the search bar, and press “Enter” to retrieve the associated tools.\nIf the toolset name is not defined within the tools configuration file, an error message will be displayed.\n","categories":"","description":"How to effectively use Toolbox UI.\n","excerpt":"How to effectively use Toolbox UI.\n","ref":"/genai-toolbox/how-to/toolbox-ui/","tags":"","title":"Toolbox UI"},{"body":"","categories":"","description":"Tools that provide utility.\n","excerpt":"Tools that provide utility.\n","ref":"/genai-toolbox/resources/tools/utility/","tags":"","title":"Utility tools"},{"body":"About A valkey tool executes a series of pre-defined Valkey commands against a Memorystore for Valkey instance.\nThe specified Valkey commands are executed sequentially. Each command is represented as a string array, where the first element is the command name (e.g., SET, GET, HGETALL) and subsequent elements are its arguments.\nDynamic Command Parameters Command arguments can be templated using the $variableName annotation. The array type parameters will be expanded once into multiple arguments. Take the following config for example:\ncommands: - [SADD, userNames, $userNames] # Array will be flattened into multiple arguments. parameters: - name: userNames type: array description: The user names to be set. If the input is an array of strings [\"Alice\", \"Sid\", \"Bob\"], The final command to be executed after argument expansion will be [SADD, userNames, Alice, Sid, Bob].\nExample tools: user_data_tool: kind: valkey source: my-valkey-instance description: | Use this tool to interact with user data stored in Valkey. It can set, retrieve, and delete user-specific information. commands: - [SADD, userNames, $userNames] # Array will be flattened into multiple arguments. - [GET, $userId] parameters: - name: userId type: string description: The unique identifier for the user. - name: userNames type: array description: The user names to be set. ","categories":"","description":"A \"valkey\" tool executes a set of pre-defined Valkey commands against a Memorystore for Valkey instance.\n","excerpt":"A \"valkey\" tool executes a set of pre-defined Valkey commands against …","ref":"/genai-toolbox/resources/tools/valkey/valkey/","tags":"","title":"valkey"},{"body":"About Valkey is an open-source, in-memory data structure store that originated as a fork of Redis. It’s designed to be used as a database, cache, and message broker, supporting a wide range of data structures like strings, hashes, lists, sets, sorted sets with range queries, bitmaps, hyperloglogs, and geospatial indexes with radius queries.\nIf you’re new to Valkey, you can find installation and getting started guides on the official Valkey website.\nAvailable Tools valkey\nIssue Valkey (Redis-compatible) commands. Example sources: my-valkey-instance: kind: valkey address: - 127.0.0.1:6379 username: ${YOUR_USERNAME} password: ${YOUR_PASSWORD} # database: 0 # useGCPIAM: false # disableCache: false Tip\nUse environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nIAM Authentication If you are using GCP’s Memorystore for Valkey, you can connect using IAM authentication. Grant your account the required IAM role and set useGCPIAM to true:\nsources: my-valkey-instance: kind: valkey address: - 127.0.0.1:6379 useGCPIAM: true Reference field type required description kind string true Must be “valkey”. address []string true Endpoints for the Valkey instance to connect to. username string false If you are using a non-default user, specify the user name here. If you are using Memorystore for Valkey, leave this field blank password string false Password for the Valkey instance database int false The Valkey database to connect to. Not applicable for cluster enabled instances. The default database is 0. useGCPIAM bool false Set it to true if you are using GCP’s IAM authentication. Defaults to false. disableCache bool false Set it to true if you want to enable client-side caching. Defaults to false. ","categories":"","description":"Valkey is an open-source, in-memory data structure store, forked from Redis.\n","excerpt":"Valkey is an open-source, in-memory data structure store, forked from …","ref":"/genai-toolbox/resources/sources/valkey/","tags":"","title":"Valkey"},{"body":"","categories":"","description":"Tools that work with Valkey Sources.\n","excerpt":"Tools that work with Valkey Sources.\n","ref":"/genai-toolbox/resources/tools/valkey/","tags":"","title":"Valkey"},{"body":"About A wait tool pauses execution for a specified duration. This can be useful in workflows where a delay is needed between steps.\nwait takes one input parameter duration which is a string representing the time to wait (e.g., “10s”, “2m”, “1h”).\nInfo\nThis tool is intended for developer assistant workflows with human-in-the-loop and shouldn’t be used for production agents.\nExample tools: wait_for_tool: kind: wait description: Use this tool to pause execution for a specified duration. timeout: 30s Reference field type required description kind string true Must be “wait”. description string true Description of the tool that is passed to the LLM. timeout string true The default duration the tool can wait for. ","categories":"","description":"A \"wait\" tool pauses execution for a specified duration.\n","excerpt":"A \"wait\" tool pauses execution for a specified duration.\n","ref":"/genai-toolbox/resources/tools/utility/wait/","tags":"","title":"wait"},{"body":"Overview Model Context Protocol is an open protocol that standardizes how applications provide context to LLMs. Check out this page on how to connect to Toolbox via MCP.\nBefore you begin This guide assumes you have already done the following:\nCreate a AlloyDB cluster and instance with a database and user.\nConnect to the instance using AlloyDB Studio, psql command-line tool, or any other PostgreSQL client.\nEnable the pgvector and google_ml_integration extensions. These are required for Semantic Search and Natural Language to SQL tools. Run the following SQL commands:\nCREATE EXTENSION IF NOT EXISTS \"vector\"; CREATE EXTENSION IF NOT EXISTS \"google_ml_integration\"; CREATE EXTENSION IF NOT EXISTS alloydb_ai_nl cascade; CREATE EXTENSION IF NOT EXISTS parameterized_views; Step 1: Set up your AlloyDB database In this section, we will create the necessary tables and functions in your AlloyDB instance.\nCreate tables using the following commands:\nCREATE TABLE products ( product_id SERIAL PRIMARY KEY, name VARCHAR(255) NOT NULL, description TEXT, price DECIMAL(10, 2) NOT NULL, category_id INT, embedding vector(3072) -- Vector size for model(gemini-embedding-001) ); CREATE TABLE customers ( customer_id SERIAL PRIMARY KEY, name VARCHAR(255) NOT NULL, email VARCHAR(255) UNIQUE NOT NULL ); CREATE TABLE cart ( cart_id SERIAL PRIMARY KEY, customer_id INT UNIQUE NOT NULL, created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (customer_id) REFERENCES customers(customer_id) ); CREATE TABLE cart_items ( cart_item_id SERIAL PRIMARY KEY, cart_id INT NOT NULL, product_id INT NOT NULL, quantity INT NOT NULL, price DECIMAL(10, 2) NOT NULL, FOREIGN KEY (cart_id) REFERENCES cart(cart_id), FOREIGN KEY (product_id) REFERENCES products(product_id) ); CREATE TABLE categories ( category_id SERIAL PRIMARY KEY, name VARCHAR(255) NOT NULL ); Insert sample data into the tables:\nINSERT INTO categories (category_id, name) VALUES (1, 'Flowers'), (2, 'Vases'); INSERT INTO products (product_id, name, description, price, category_id, embedding) VALUES (1, 'Rose', 'A beautiful red rose', 2.50, 1, embedding('gemini-embedding-001', 'A beautiful red rose')), (2, 'Tulip', 'A colorful tulip', 1.50, 1, embedding('gemini-embedding-001', 'A colorful tulip')), (3, 'Glass Vase', 'A transparent glass vase', 10.00, 2, embedding('gemini-embedding-001', 'A transparent glass vase')), (4, 'Ceramic Vase', 'A handmade ceramic vase', 15.00, 2, embedding('gemini-embedding-001', 'A handmade ceramic vase')); INSERT INTO customers (customer_id, name, email) VALUES (1, 'John Doe', 'john.doe@example.com'), (2, 'Jane Smith', 'jane.smith@example.com'); INSERT INTO cart (cart_id, customer_id) VALUES (1, 1), (2, 2); INSERT INTO cart_items (cart_id, product_id, quantity, price) VALUES (1, 1, 2, 2.50), (1, 3, 1, 10.00), (2, 2, 5, 1.50); Step 2: Install Toolbox In this section, we will download and install the Toolbox binary.\nDownload the latest version of Toolbox as a binary:\nTip\nSelect the correct binary corresponding to your OS and CPU architecture.\nexport OS=\"linux/amd64\" # one of linux/amd64, darwin/arm64, darwin/amd64, or windows/amd64 export VERSION=\"0.11.0\" curl -O https://storage.googleapis.com/genai-toolbox/v$VERSION/$OS/toolbox Make the binary executable:\nchmod +x toolbox Step 3: Configure the tools Create a tools.yaml file and add the following content. You must replace the placeholders with your actual AlloyDB configuration.\nFirst, define the data source for your tools. This tells Toolbox how to connect to your AlloyDB instance.\nsources: alloydb-pg-source: kind: alloydb-postgres project: YOUR_PROJECT_ID region: YOUR_REGION cluster: YOUR_CLUSTER instance: YOUR_INSTANCE database: YOUR_DATABASE user: YOUR_USER password: YOUR_PASSWORD Next, define the tools the agent can use. We will categorize them into three types:\n1. Structured Queries Tools These tools execute predefined SQL statements. They are ideal for common, structured queries like managing a shopping cart. Add the following to your tools.yaml file:\ntools: access-cart-information: kind: postgres-sql source: alloydb-pg-source description: \u003e- List items in customer cart. Use this tool to list items in a customer cart. This tool requires the cart ID. parameters: - name: cart_id type: integer description: The id of the cart. statement: | SELECT p.name AS product_name, ci.quantity, ci.price AS item_price, (ci.quantity * ci.price) AS total_item_price, c.created_at AS cart_created_at, ci.product_id AS product_id FROM cart_items ci JOIN cart c ON ci.cart_id = c.cart_id JOIN products p ON ci.product_id = p.product_id WHERE c.cart_id = $1; add-to-cart: kind: postgres-sql source: alloydb-pg-source description: \u003e- Add items to customer cart using the product ID and product prices from the product list. Use this tool to add items to a customer cart. This tool requires the cart ID, product ID, quantity, and price. parameters: - name: cart_id type: integer description: The id of the cart. - name: product_id type: integer description: The id of the product. - name: quantity type: integer description: The quantity of items to add. - name: price type: float description: The price of items to add. statement: | INSERT INTO cart_items (cart_id, product_id, quantity, price) VALUES($1,$2,$3,$4); delete-from-cart: kind: postgres-sql source: alloydb-pg-source description: \u003e- Remove products from customer cart. Use this tool to remove products from a customer cart. This tool requires the cart ID and product ID. parameters: - name: cart_id type: integer description: The id of the cart. - name: product_id type: integer description: The id of the product. statement: | DELETE FROM cart_items WHERE cart_id = $1 AND product_id = $2; 2. Semantic Search Tools These tools use vector embeddings to find the most relevant results based on the meaning of a user’s query, rather than just keywords. Append the following tools to the tools section in your tools.yaml:\nsearch-product-recommendations: kind: postgres-sql source: alloydb-pg-source description: \u003e- Search for products based on user needs. Use this tool to search for products. This tool requires the user's needs. parameters: - name: query type: string description: The product characteristics statement: | SELECT product_id, name, description, ROUND(CAST(price AS numeric), 2) as price FROM products ORDER BY embedding('gemini-embedding-001', $1)::vector \u003c=\u003e embedding LIMIT 5; 3. Natural Language to SQL (NL2SQL) Tools Create a natural language configuration for your AlloyDB cluster.\nTip\nBefore using NL2SQL tools, you must first install the alloydb_ai_nl extension and create the semantic layer under a configuration named flower_shop.\nConfigure your NL2SQL tool to use your configuration. These tools translate natural language questions into SQL queries, allowing users to interact with the database conversationally. Append the following tool to the tools section:\nask-questions-about-products: kind: alloydb-ai-nl source: alloydb-pg-source nlConfig: flower_shop description: \u003e- Ask questions related to products or brands. Use this tool to ask questions about products or brands. Always SELECT the IDs of objects when generating queries. Finally, group the tools into a toolset to make them available to the model. Add the following to the end of your tools.yaml file:\ntoolsets: flower_shop: - access-cart-information - search-product-recommendations - ask-questions-about-products - add-to-cart - delete-from-cart For more info on tools, check out the Tools section.\nStep 4: Run the Toolbox server Run the Toolbox server, pointing to the tools.yaml file created earlier:\n./toolbox --tools-file \"tools.yaml\" Step 5: Connect to MCP Inspector Run the MCP Inspector:\nnpx @modelcontextprotocol/inspector Type y when it asks to install the inspector package.\nIt should show the following when the MCP Inspector is up and running (please take note of \u003cYOUR_SESSION_TOKEN\u003e):\nStarting MCP inspector... ⚙️ Proxy server listening on localhost:6277 🔑 Session token: \u003cYOUR_SESSION_TOKEN\u003e Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth 🚀 MCP Inspector is up and running at: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=\u003cYOUR_SESSION_TOKEN\u003e Open the above link in your browser.\nFor Transport Type, select Streamable HTTP.\nFor URL, type in http://127.0.0.1:5000/mcp.\nFor Configuration -\u003e Proxy Session Token, make sure \u003cYOUR_SESSION_TOKEN\u003e is present.\nClick Connect.\nSelect List Tools, you will see a list of tools configured in tools.yaml.\nTest out your tools here!\nWhat’s next Learn more about MCP Inspector. Learn more about Toolbox Resources. Learn more about Toolbox How-to guides. ","categories":"","description":"How to get started running Toolbox with MCP Inspector and AlloyDB as the source.\n","excerpt":"How to get started running Toolbox with MCP Inspector and AlloyDB as …","ref":"/genai-toolbox/samples/alloydb/","tags":"","title":"AlloyDB"},{"body":" ","categories":"","description":"Create your AlloyDB database with MCP Toolbox.\n","excerpt":"Create your AlloyDB database with MCP Toolbox.\n","ref":"/genai-toolbox/how-to/connect-ide/alloydb_pg_admin_mcp/","tags":"","title":"AlloyDB Admin API using MCP"},{"body":" ","categories":"","description":"Connect your IDE to AlloyDB using Toolbox.\n","excerpt":"Connect your IDE to AlloyDB using Toolbox.\n","ref":"/genai-toolbox/how-to/connect-ide/alloydb_pg_mcp/","tags":"","title":"AlloyDB using MCP"},{"body":" ","categories":"","description":"Connect your IDE to BigQuery using Toolbox.\n","excerpt":"Connect your IDE to BigQuery using Toolbox.\n","ref":"/genai-toolbox/how-to/connect-ide/bigquery_mcp/","tags":"","title":"BigQuery using MCP"},{"body":" ","categories":"","description":"Connect your IDE to Cloud SQL for MySQL using Toolbox.\n","excerpt":"Connect your IDE to Cloud SQL for MySQL using Toolbox.\n","ref":"/genai-toolbox/how-to/connect-ide/cloud_sql_mysql_mcp/","tags":"","title":"Cloud SQL for MySQL using MCP"},{"body":" ","categories":"","description":"Connect your IDE to Cloud SQL for Postgres using Toolbox.\n","excerpt":"Connect your IDE to Cloud SQL for Postgres using Toolbox.\n","ref":"/genai-toolbox/how-to/connect-ide/cloud_sql_pg_mcp/","tags":"","title":"Cloud SQL for Postgres using MCP"},{"body":" ","categories":"","description":"Connect your IDE to Cloud SQL for SQL Server using Toolbox.\n","excerpt":"Connect your IDE to Cloud SQL for SQL Server using Toolbox.\n","ref":"/genai-toolbox/how-to/connect-ide/cloud_sql_mssql_mcp/","tags":"","title":"Cloud SQL for SQL Server using MCP"},{"body":"","categories":"","description":"Some core concepts in Toolbox\n","excerpt":"Some core concepts in Toolbox\n","ref":"/genai-toolbox/concepts/","tags":"","title":"Concepts"},{"body":"How can I deploy or run Toolbox? MCP Toolbox for Databases is open-source and can be ran or deployed to a multitude of environments. For convenience, we release compiled binaries and docker images (but you can always compile yourself as well!).\nFor detailed instructions, check our these resources:\nQuickstart: How to Run Locally Deploy to Cloud Run Do I need a Google Cloud account/project to get started with Toolbox? Nope! While some of the sources Toolbox connects to may require GCP credentials, Toolbox doesn’t require them and can connect to a bunch of different resources that don’t.\nDoes Toolbox take contributions from external users? Absolutely! Please check out our DEVELOPER.md for instructions on how to get started developing on Toolbox instead of with it, and the CONTRIBUTING.md for instructions on completing the CLA and getting a PR accepted.\nCan Toolbox support a feature to let me do $FOO? Maybe? The best place to start is by opening an issue for discussion (or seeing if there is already one open), so we can better understand your use case and the best way to solve it. Generally we aim to prioritize the most popular issues, so make sure to +1 ones you are the most interested in.\nCan Toolbox be used for non-database tools? Currently, Toolbox is primarily focused on making it easier to create and develop tools focused on interacting with Databases. We believe that there are a lot of unique problems when interacting with Databases for Gen AI use cases, and want to prioritize solving those first.\nHowever, we’ve also received feedback that supporting more generic HTTP or GRPC tools might be helpful in assisting with migrating to Toolbox or in accomplishing more complicated workflows. We’re looking into what that might best look like in Toolbox.\nCan I use $BAR orchestration framework to use tools from Toolbox? Currently, Toolbox only supports a limited number of client SDKs at our initial launch. We are investigating support for more frameworks as well as more general approaches for users without a framework – look forward to seeing an update soon.\nWhy does Toolbox use a server-client architecture pattern? Toolbox’s server-client architecture allows us to more easily support a wide variety of languages and frameworks with a centralized implementation. It also allows us to tackle problems like connection pooling, auth, or caching more completely than entirely client-side solutions.\nWhy was Toolbox written in Go? While a large part of the Gen AI Ecosystem is predominately Python, we opted to use Go. We chose Go because it’s still easy and simple to use, but also easier to write fast, efficient, and concurrent servers. Additionally, given the server-client architecture, we can still meet many developers where they are with clients in their preferred language. As Gen AI matures, we want developers to be able to use Toolbox on the serving path of mission critical applications. It’s easier to build the needed robustness, performance and scalability in Go than in Python.\nIs Toolbox compatible with Model Context Protocol (MCP)? Yes! Toolbox is compatible with Anthropic’s Model Context Protocol (MCP). Please checkout Connect via MCP on how to connect to Toolbox with an MCP client.\n","categories":"","description":"Frequently asked questions about Toolbox.","excerpt":"Frequently asked questions about Toolbox.","ref":"/genai-toolbox/about/faq/","tags":"","title":"FAQ"},{"body":"Model Context Protocol (MCP) is an open protocol for connecting Large Language Models (LLMs) to data sources like Firestore. This guide covers how to use MCP Toolbox for Databases to expose your developer assistant tools to a Firestore instance:\nCursor Windsurf (Codium) Visual Studio Code (Copilot) Cline (VS Code extension) Claude desktop Claude code Gemini CLI Gemini Code Assist Set up Firestore Create or select a Google Cloud project.\nCreate a new project Select an existing project Enable the Firestore API for your project.\nCreate a Firestore database if you haven’t already.\nSet up authentication for your local environment.\nInstall gcloud CLI Run gcloud auth application-default login to authenticate Install MCP Toolbox Download the latest version of Toolbox as a binary. Select the correct binary corresponding to your OS and CPU architecture. You are required to use Toolbox version V0.10.0+:\nlinux/amd64 darwin/arm64 darwin/amd64 windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/linux/amd64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/darwin/arm64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/darwin/amd64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/windows/amd64/toolbox Make the binary executable:\nchmod +x toolbox Verify the installation:\n./toolbox --version Configure your MCP Client Claude code Claude desktop Cline Cursor Visual Studio Code (Copilot) Windsurf Gemini CLI Gemini Code Assist Install Claude Code.\nCreate a .mcp.json file in your project root if it doesn’t exist.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"firestore\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"firestore\",\"--stdio\"], \"env\": { \"FIRESTORE_PROJECT\": \"your-project-id\", \"FIRESTORE_DATABASE\": \"(default)\" } } } } Restart Claude code to apply the new configuration.\nOpen Claude desktop and navigate to Settings.\nUnder the Developer tab, tap Edit Config to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"firestore\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"firestore\",\"--stdio\"], \"env\": { \"FIRESTORE_PROJECT\": \"your-project-id\", \"FIRESTORE_DATABASE\": \"(default)\" } } } } Restart Claude desktop.\nFrom the new chat screen, you should see a hammer (MCP) icon appear with the new MCP server available.\nOpen the Cline extension in VS Code and tap the MCP Servers icon.\nTap Configure MCP Servers to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"firestore\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"firestore\",\"--stdio\"], \"env\": { \"FIRESTORE_PROJECT\": \"your-project-id\", \"FIRESTORE_DATABASE\": \"(default)\" } } } } You should see a green active status after the server is successfully connected.\nCreate a .cursor directory in your project root if it doesn’t exist.\nCreate a .cursor/mcp.json file if it doesn’t exist and open it.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"firestore\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"firestore\",\"--stdio\"], \"env\": { \"FIRESTORE_PROJECT\": \"your-project-id\", \"FIRESTORE_DATABASE\": \"(default)\" } } } } Cursor and navigate to Settings \u003e Cursor Settings \u003e MCP. You should see a green active status after the server is successfully connected.\nOpen VS Code and create a .vscode directory in your project root if it doesn’t exist.\nCreate a .vscode/mcp.json file if it doesn’t exist and open it.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"firestore\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"firestore\",\"--stdio\"], \"env\": { \"FIRESTORE_PROJECT\": \"your-project-id\", \"FIRESTORE_DATABASE\": \"(default)\" } } } } Open Windsurf and navigate to the Cascade assistant.\nTap on the hammer (MCP) icon, then Configure to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"firestore\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"firestore\",\"--stdio\"], \"env\": { \"FIRESTORE_PROJECT\": \"your-project-id\", \"FIRESTORE_DATABASE\": \"(default)\" } } } } Install the Gemini CLI.\nIn your working directory, create a folder named .gemini. Within it, create a settings.json file.\nAdd the following configuration, replace the environment variables with your values, and then save:\n{ \"mcpServers\": { \"firestore\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"firestore\",\"--stdio\"], \"env\": { \"FIRESTORE_PROJECT\": \"your-project-id\", \"FIRESTORE_DATABASE\": \"(default)\" } } } } Install the Gemini Code Assist extension in Visual Studio Code.\nEnable Agent Mode in Gemini Code Assist chat.\nIn your working directory, create a folder named .gemini. Within it, create a settings.json file.\nAdd the following configuration, replace the environment variables with your values, and then save:\n{ \"mcpServers\": { \"firestore\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"firestore\",\"--stdio\"], \"env\": { \"FIRESTORE_PROJECT\": \"your-project-id\", \"FIRESTORE_DATABASE\": \"(default)\" } } } } Use Tools Your AI tool is now connected to Firestore using MCP. Try asking your AI assistant to list collections, get documents, query collections, or manage security rules.\nThe following tools are available to the LLM:\nfirestore-get-documents: Gets multiple documents from Firestore by their paths firestore-list-collections: List Firestore collections for a given parent path firestore-delete-documents: Delete multiple documents from Firestore firestore-query-collection: Query documents from a collection with filtering, ordering, and limit options firestore-get-rules: Retrieves the active Firestore security rules for the current project firestore-validate-rules: Validates Firestore security rules syntax and errors Note\nPrebuilt tools are pre-1.0, so expect some tool changes between versions. LLMs will adapt to the tools available, so this shouldn’t affect most users.\n","categories":"","description":"Connect your IDE to Firestore using Toolbox.\n","excerpt":"Connect your IDE to Firestore using Toolbox.\n","ref":"/genai-toolbox/how-to/connect-ide/firestore_mcp/","tags":"","title":"Firestore using MCP"},{"body":" ","categories":"","description":"Go lang client SDK","excerpt":"Go lang client SDK","ref":"/genai-toolbox/sdks/go-sdk/","tags":"","title":"Go SDK"},{"body":" ","categories":"","description":"Javascript client SDK","excerpt":"Javascript client SDK","ref":"/genai-toolbox/sdks/js-sdk/","tags":"","title":"JS SDK"},{"body":"Model Context Protocol (MCP) is an open protocol for connecting Large Language Models (LLMs) to data sources like Postgres. This guide covers how to use MCP Toolbox for Databases to expose your developer assistant tools to a Looker instance:\nCursor Windsurf (Codium) Visual Studio Code (Copilot) Cline (VS Code extension) Claude desktop Claude code Set up Looker Get a Looker Client ID and Client Secret. Follow the directions here.\nHave the base URL of your Looker instance available. It is likely something like https://looker.example.com. In some cases the API is listening at a different port, and you will need to use https://looker.example.com:19999 instead.\nInstall MCP Toolbox Download the latest version of Toolbox as a binary. Select the correct binary corresponding to your OS and CPU architecture. You are required to use Toolbox version v0.10.0+:\nlinux/amd64 darwin/arm64 darwin/amd64 windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/linux/amd64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/darwin/arm64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/darwin/amd64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/windows/amd64/toolbox.exe Make the binary executable:\nchmod +x toolbox Verify the installation:\n./toolbox --version Configure your MCP Client Claude code Claude desktop Cline Cursor Visual Studio Code (Copilot) Windsurf Install Claude Code.\nCreate a .mcp.json file in your project root if it doesn’t exist.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"looker-toolbox\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--stdio\", \"--prebuilt\", \"looker\"], \"env\": { \"LOOKER_BASE_URL\": \"https://looker.example.com\", \"LOOKER_CLIENT_ID\": \"\", \"LOOKER_CLIENT_SECRET\": \"\", \"LOOKER_VERIFY_SSL\": \"true\" } } } } Restart Claude Code to apply the new configuration.\nOpen Claude desktop and navigate to Settings.\nUnder the Developer tab, tap Edit Config to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"looker-toolbox\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--stdio\", \"--prebuilt\", \"looker\"], \"env\": { \"LOOKER_BASE_URL\": \"https://looker.example.com\", \"LOOKER_CLIENT_ID\": \"\", \"LOOKER_CLIENT_SECRET\": \"\", \"LOOKER_VERIFY_SSL\": \"true\" } } } } Restart Claude desktop.\nFrom the new chat screen, you should see a hammer (MCP) icon appear with the new MCP server available.\nOpen the Cline extension in VS Code and tap the MCP Servers icon.\nTap Configure MCP Servers to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"looker-toolbox\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--stdio\", \"--prebuilt\", \"looker\"], \"env\": { \"LOOKER_BASE_URL\": \"https://looker.example.com\", \"LOOKER_CLIENT_ID\": \"\", \"LOOKER_CLIENT_SECRET\": \"\", \"LOOKER_VERIFY_SSL\": \"true\" } } } } You should see a green active status after the server is successfully connected.\nCreate a .cursor directory in your project root if it doesn’t exist.\nCreate a .cursor/mcp.json file if it doesn’t exist and open it.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"looker-toolbox\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--stdio\", \"--prebuilt\", \"looker\"], \"env\": { \"LOOKER_BASE_URL\": \"https://looker.example.com\", \"LOOKER_CLIENT_ID\": \"\", \"LOOKER_CLIENT_SECRET\": \"\", \"LOOKER_VERIFY_SSL\": \"true\" } } } } Open Cursor and navigate to Settings \u003e Cursor Settings \u003e MCP. You should see a green active status after the server is successfully connected.\nOpen VS Code and create a .vscode directory in your project root if it doesn’t exist.\nCreate a .vscode/mcp.json file if it doesn’t exist and open it.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"looker-toolbox\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--stdio\", \"--prebuilt\", \"looker\"], \"env\": { \"LOOKER_BASE_URL\": \"https://looker.example.com\", \"LOOKER_CLIENT_ID\": \"\", \"LOOKER_CLIENT_SECRET\": \"\", \"LOOKER_VERIFY_SSL\": \"true\" } } } } Open Windsurf and navigate to the Cascade assistant.\nTap on the hammer (MCP) icon, then Configure to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"looker-toolbox\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--stdio\", \"--prebuilt\", \"looker\"], \"env\": { \"LOOKER_BASE_URL\": \"https://looker.example.com\", \"LOOKER_CLIENT_ID\": \"\", \"LOOKER_CLIENT_SECRET\": \"\", \"LOOKER_VERIFY_SSL\": \"true\" } } } } Use Tools Your AI tool is now connected to Looker using MCP. Try asking your AI assistant to list models, explores, dimensions, and measures. Run a query, retrieve the SQL for a query, and run a saved Look.\nThe following tools are available to the LLM:\nget_models: list the LookML models in Looker get_explores: list the explores in a given model get_dimensions: list the dimensions in a given explore get_measures: list the measures in a given explore get_filters: list the filters in a given explore get_parameters: list the parameters in a given explore query: Run a query and return the data query_sql: Return the SQL generated by Looker for a query query_url: Return a link to the query in Looker for further exploration get_looks: Return the saved Looks that match a title or description run_look: Run a saved Look and return the data make_look: Create a saved Look in Looker and return the URL Note\nPrebuilt tools are pre-1.0, so expect some tool changes between versions. LLMs will adapt to the tools available, so this shouldn’t affect most users.\n","categories":"","description":"Connect your IDE to Looker using Toolbox.\n","excerpt":"Connect your IDE to Looker using Toolbox.\n","ref":"/genai-toolbox/how-to/connect-ide/looker_mcp/","tags":"","title":"Looker using MCP"},{"body":"Model Context Protocol (MCP) is an open protocol for connecting Large Language Models (LLMs) to data sources like MySQL. This guide covers how to use MCP Toolbox for Databases to expose your developer assistant tools to a MySQL instance:\nCursor Windsurf (Codium) Visual Studio Code (Copilot) Cline (VS Code extension) Claude desktop Claude code Gemini CLI Gemini Code Assist Set up the database Create or select a MySQL instance. Install MCP Toolbox Download the latest version of Toolbox as a binary. Select the correct binary corresponding to your OS and CPU architecture. You are required to use Toolbox version V0.10.0+:\nlinux/amd64 darwin/arm64 darwin/amd64 windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/linux/amd64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/darwin/arm64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/darwin/amd64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/windows/amd64/toolbox.exe Make the binary executable:\nchmod +x toolbox Verify the installation:\n./toolbox --version Configure your MCP Client Claude code Claude desktop Cline Cursor Visual Studio Code (Copilot) Windsurf Gemini CLI Gemini Code Assist Install Claude Code.\nCreate a .mcp.json file in your project root if it doesn’t exist.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"mysql\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\", \"mysql\", \"--stdio\"], \"env\": { \"MYSQL_HOST\": \"\", \"MYSQL_PORT\": \"\", \"MYSQL_DATABASE\": \"\", \"MYSQL_USER\": \"\", \"MYSQL_PASSWORD\": \"\" } } } } Restart Claude code to apply the new configuration.\nOpen Claude desktop and navigate to Settings.\nUnder the Developer tab, tap Edit Config to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"mysql\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\", \"mysql\", \"--stdio\"], \"env\": { \"MYSQL_HOST\": \"\", \"MYSQL_PORT\": \"\", \"MYSQL_DATABASE\": \"\", \"MYSQL_USER\": \"\", \"MYSQL_PASSWORD\": \"\" } } } } Restart Claude desktop.\nFrom the new chat screen, you should see a hammer (MCP) icon appear with the new MCP server available.\nOpen the Cline extension in VS Code and tap the MCP Servers icon.\nTap Configure MCP Servers to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"mysql\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\", \"mysql\", \"--stdio\"], \"env\": { \"MYSQL_HOST\": \"\", \"MYSQL_PORT\": \"\", \"MYSQL_DATABASE\": \"\", \"MYSQL_USER\": \"\", \"MYSQL_PASSWORD\": \"\" } } } } You should see a green active status after the server is successfully connected.\nCreate a .cursor directory in your project root if it doesn’t exist.\nCreate a .cursor/mcp.json file if it doesn’t exist and open it.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"mysql\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\", \"mysql\", \"--stdio\"], \"env\": { \"MYSQL_HOST\": \"\", \"MYSQL_PORT\": \"\", \"MYSQL_DATABASE\": \"\", \"MYSQL_USER\": \"\", \"MYSQL_PASSWORD\": \"\" } } } } Open Cursor and navigate to Settings \u003e Cursor Settings \u003e MCP. You should see a green active status after the server is successfully connected.\nOpen VS Code and create a .vscode directory in your project root if it doesn’t exist.\nCreate a .vscode/mcp.json file if it doesn’t exist and open it.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"mysql\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mysql\",\"--stdio\"], \"env\": { \"MYSQL_HOST\": \"\", \"MYSQL_PORT\": \"\", \"MYSQL_DATABASE\": \"\", \"MYSQL_USER\": \"\", \"MYSQL_PASSWORD\": \"\" } } } } Open Windsurf and navigate to the Cascade assistant.\nTap on the hammer (MCP) icon, then Configure to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"mysql\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mysql\",\"--stdio\"], \"env\": { \"MYSQL_HOST\": \"\", \"MYSQL_PORT\": \"\", \"MYSQL_DATABASE\": \"\", \"MYSQL_USER\": \"\", \"MYSQL_PASSWORD\": \"\" } } } } Install the Gemini CLI.\nIn your working directory, create a folder named .gemini. Within it, create a settings.json file.\nAdd the following configuration, replace the environment variables with your values, and then save:\n{ \"mcpServers\": { \"mysql\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mysql\",\"--stdio\"], \"env\": { \"MYSQL_HOST\": \"\", \"MYSQL_PORT\": \"\", \"MYSQL_DATABASE\": \"\", \"MYSQL_USER\": \"\", \"MYSQL_PASSWORD\": \"\" } } } } Install the Gemini Code Assist extension in Visual Studio Code.\nEnable Agent Mode in Gemini Code Assist chat.\nIn your working directory, create a folder named .gemini. Within it, create a settings.json file.\nAdd the following configuration, replace the environment variables with your values, and then save:\n{ \"mcpServers\": { \"mysql\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mysql\",\"--stdio\"], \"env\": { \"MYSQL_HOST\": \"\", \"MYSQL_PORT\": \"\", \"MYSQL_DATABASE\": \"\", \"MYSQL_USER\": \"\", \"MYSQL_PASSWORD\": \"\" } } } } Use Tools Your AI tool is now connected to MySQL using MCP. Try asking your AI assistant to list tables, create a table, or define and execute other SQL statements.\nThe following tools are available to the LLM:\nlist_tables: lists tables and descriptions execute_sql: execute any SQL statement Note\nPrebuilt tools are pre-1.0, so expect some tool changes between versions. LLMs will adapt to the tools available, so this shouldn’t affect most users.\n","categories":"","description":"Connect your IDE to MySQL using Toolbox.","excerpt":"Connect your IDE to MySQL using Toolbox.","ref":"/genai-toolbox/how-to/connect-ide/mysql_mcp/","tags":"","title":"MySQL using MCP"},{"body":"Model Context Protocol (MCP) is an open protocol for connecting Large Language Models (LLMs) to data sources like Postgres. This guide covers how to use MCP Toolbox for Databases to expose your developer assistant tools to a Postgres instance:\nCursor Windsurf (Codium) Visual Studio Code (Copilot) Cline (VS Code extension) Claude desktop Claude code Tip\nThis guide can be used with AlloyDB Omni.\nSet up the database Create or select a PostgreSQL instance.\nInstall PostgreSQL locally Install AlloyDB Omni Create or reuse a database user and have the username and password ready.\nInstall MCP Toolbox Download the latest version of Toolbox as a binary. Select the correct binary corresponding to your OS and CPU architecture. You are required to use Toolbox version V0.6.0+:\nlinux/amd64 darwin/arm64 darwin/amd64 windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/linux/amd64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/darwin/arm64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/darwin/amd64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/windows/amd64/toolbox.exe Make the binary executable:\nchmod +x toolbox Verify the installation:\n./toolbox --version Configure your MCP Client Claude code Claude desktop Cline Cursor Visual Studio Code (Copilot) Windsurf Install Claude Code.\nCreate a .mcp.json file in your project root if it doesn’t exist.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"postgres\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"postgres\",\"--stdio\"], \"env\": { \"POSTGRES_HOST\": \"\", \"POSTGRES_PORT\": \"\", \"POSTGRES_DATABASE\": \"\", \"POSTGRES_USER\": \"\", \"POSTGRES_PASSWORD\": \"\" } } } } Restart Claude code to apply the new configuration.\nOpen Claude desktop and navigate to Settings.\nUnder the Developer tab, tap Edit Config to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"postgres\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"postgres\",\"--stdio\"], \"env\": { \"POSTGRES_HOST\": \"\", \"POSTGRES_PORT\": \"\", \"POSTGRES_DATABASE\": \"\", \"POSTGRES_USER\": \"\", \"POSTGRES_PASSWORD\": \"\" } } } } Restart Claude desktop.\nFrom the new chat screen, you should see a hammer (MCP) icon appear with the new MCP server available.\nOpen the Cline extension in VS Code and tap the MCP Servers icon.\nTap Configure MCP Servers to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"postgres\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"postgres\",\"--stdio\"], \"env\": { \"POSTGRES_HOST\": \"\", \"POSTGRES_PORT\": \"\", \"POSTGRES_DATABASE\": \"\", \"POSTGRES_USER\": \"\", \"POSTGRES_PASSWORD\": \"\" } } } } You should see a green active status after the server is successfully connected.\nCreate a .cursor directory in your project root if it doesn’t exist.\nCreate a .cursor/mcp.json file if it doesn’t exist and open it.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"postgres\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"postgres\",\"--stdio\"], \"env\": { \"POSTGRES_HOST\": \"\", \"POSTGRES_PORT\": \"\", \"POSTGRES_DATABASE\": \"\", \"POSTGRES_USER\": \"\", \"POSTGRES_PASSWORD\": \"\" } } } } Cursor and navigate to Settings \u003e Cursor Settings \u003e MCP. You should see a green active status after the server is successfully connected.\nOpen VS Code and create a .vscode directory in your project root if it doesn’t exist.\nCreate a .vscode/mcp.json file if it doesn’t exist and open it.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"postgres\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"postgres\",\"--stdio\"], \"env\": { \"POSTGRES_HOST\": \"\", \"POSTGRES_PORT\": \"\", \"POSTGRES_DATABASE\": \"\", \"POSTGRES_USER\": \"\", \"POSTGRES_PASSWORD\": \"\" } } } } Open Windsurf and navigate to the Cascade assistant.\nTap on the hammer (MCP) icon, then Configure to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"postgres\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"postgres\",\"--stdio\"], \"env\": { \"POSTGRES_HOST\": \"\", \"POSTGRES_PORT\": \"\", \"POSTGRES_DATABASE\": \"\", \"POSTGRES_USER\": \"\", \"POSTGRES_PASSWORD\": \"\" } } } } Use Tools Your AI tool is now connected to Postgres using MCP. Try asking your AI assistant to list tables, create a table, or define and execute other SQL statements.\nThe following tools are available to the LLM:\nlist_tables: lists tables and descriptions execute_sql: execute any SQL statement Note\nPrebuilt tools are pre-1.0, so expect some tool changes between versions. LLMs will adapt to the tools available, so this shouldn’t affect most users.\n","categories":"","description":"Connect your IDE to PostgreSQL using Toolbox.\n","excerpt":"Connect your IDE to PostgreSQL using Toolbox.\n","ref":"/genai-toolbox/how-to/connect-ide/postgres_mcp/","tags":"","title":"PostgreSQL using MCP"},{"body":"\nBefore you begin This guide assumes you have already done the following:\nInstalled Python 3.9+ (including pip and your preferred virtual environment tool for managing dependencies e.g. venv). Installed PostgreSQL 16+ and the psql client. Cloud Setup (Optional) If you plan to use Google Cloud’s Vertex AI with your agent (e.g., using vertexai=True or a Google GenAI model), follow these one-time setup steps for local development:\nInstall the Google Cloud CLI\nSet up Application Default Credentials (ADC)\nSet your project and enable Vertex AI\ngcloud config set project YOUR_PROJECT_ID gcloud services enable aiplatform.googleapis.com Step 1: Set up your database In this section, we will create a database, insert some data that needs to be accessed by our agent, and create a database user for Toolbox to connect with.\nConnect to postgres using the psql command:\npsql -h 127.0.0.1 -U postgres Here, postgres denotes the default postgres superuser.\nInfo\nHaving trouble connecting? Password Prompt: If you are prompted for a password for the postgres user and do not know it (or a blank password doesn’t work), your PostgreSQL installation might require a password or a different authentication method. FATAL: role \"postgres\" does not exist: This error means the default postgres superuser role isn’t available under that name on your system. Connection refused: Ensure your PostgreSQL server is actually running. You can typically check with sudo systemctl status postgresql and start it with sudo systemctl start postgresql on Linux systems. Common Solution For password issues or if the postgres role seems inaccessible directly, try switching to the postgres operating system user first. This user often has permission to connect without a password for local connections (this is called peer authentication).\nsudo -i -u postgres psql -h 127.0.0.1 Once you are in the psql shell using this method, you can proceed with the database creation steps below. Afterwards, type \\q to exit psql, and then exit to return to your normal user shell.\nIf desired, once connected to psql as the postgres OS user, you can set a password for the postgres database user using: ALTER USER postgres WITH PASSWORD 'your_chosen_password';. This would allow direct connection with -U postgres and a password next time.\nCreate a new database and a new user:\nTip\nFor a real application, it’s best to follow the principle of least permission and only grant the privileges your application needs.\nCREATE USER toolbox_user WITH PASSWORD 'my-password'; CREATE DATABASE toolbox_db; GRANT ALL PRIVILEGES ON DATABASE toolbox_db TO toolbox_user; ALTER DATABASE toolbox_db OWNER TO toolbox_user; End the database session:\n\\q (If you used sudo -i -u postgres and then psql, remember you might also need to type exit after \\q to leave the postgres user’s shell session.)\nConnect to your database with your new user:\npsql -h 127.0.0.1 -U toolbox_user -d toolbox_db Create a table using the following command:\nCREATE TABLE hotels( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR NOT NULL, location VARCHAR NOT NULL, price_tier VARCHAR NOT NULL, checkin_date DATE NOT NULL, checkout_date DATE NOT NULL, booked BIT NOT NULL ); Insert data into the table.\nINSERT INTO hotels(id, name, location, price_tier, checkin_date, checkout_date, booked) VALUES (1, 'Hilton Basel', 'Basel', 'Luxury', '2024-04-22', '2024-04-20', B'0'), (2, 'Marriott Zurich', 'Zurich', 'Upscale', '2024-04-14', '2024-04-21', B'0'), (3, 'Hyatt Regency Basel', 'Basel', 'Upper Upscale', '2024-04-02', '2024-04-20', B'0'), (4, 'Radisson Blu Lucerne', 'Lucerne', 'Midscale', '2024-04-24', '2024-04-05', B'0'), (5, 'Best Western Bern', 'Bern', 'Upper Midscale', '2024-04-23', '2024-04-01', B'0'), (6, 'InterContinental Geneva', 'Geneva', 'Luxury', '2024-04-23', '2024-04-28', B'0'), (7, 'Sheraton Zurich', 'Zurich', 'Upper Upscale', '2024-04-27', '2024-04-02', B'0'), (8, 'Holiday Inn Basel', 'Basel', 'Upper Midscale', '2024-04-24', '2024-04-09', B'0'), (9, 'Courtyard Zurich', 'Zurich', 'Upscale', '2024-04-03', '2024-04-13', B'0'), (10, 'Comfort Inn Bern', 'Bern', 'Midscale', '2024-04-04', '2024-04-16', B'0'); End the database session:\n\\q Step 2: Install and configure Toolbox In this section, we will download Toolbox, configure our tools in a tools.yaml, and then run the Toolbox server.\nDownload the latest version of Toolbox as a binary:\nTip\nSelect the correct binary corresponding to your OS and CPU architecture.\nexport OS=\"linux/amd64\" # one of linux/amd64, darwin/arm64, darwin/amd64, or windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/$OS/toolbox Make the binary executable:\nchmod +x toolbox Write the following into a tools.yaml file. Be sure to update any fields such as user, password, or database that you may have customized in the previous step.\nTip\nIn practice, use environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nsources: my-pg-source: kind: postgres host: 127.0.0.1 port: 5432 database: toolbox_db user: ${USER_NAME} password: ${PASSWORD} tools: search-hotels-by-name: kind: postgres-sql source: my-pg-source description: Search for hotels based on name. parameters: - name: name type: string description: The name of the hotel. statement: SELECT * FROM hotels WHERE name ILIKE '%' || $1 || '%'; search-hotels-by-location: kind: postgres-sql source: my-pg-source description: Search for hotels based on location. parameters: - name: location type: string description: The location of the hotel. statement: SELECT * FROM hotels WHERE location ILIKE '%' || $1 || '%'; book-hotel: kind: postgres-sql source: my-pg-source description: \u003e- Book a hotel by its ID. If the hotel is successfully booked, returns a NULL, raises an error if not. parameters: - name: hotel_id type: string description: The ID of the hotel to book. statement: UPDATE hotels SET booked = B'1' WHERE id = $1; update-hotel: kind: postgres-sql source: my-pg-source description: \u003e- Update a hotel's check-in and check-out dates by its ID. Returns a message indicating whether the hotel was successfully updated or not. parameters: - name: hotel_id type: string description: The ID of the hotel to update. - name: checkin_date type: string description: The new check-in date of the hotel. - name: checkout_date type: string description: The new check-out date of the hotel. statement: \u003e- UPDATE hotels SET checkin_date = CAST($2 as date), checkout_date = CAST($3 as date) WHERE id = $1; cancel-hotel: kind: postgres-sql source: my-pg-source description: Cancel a hotel by its ID. parameters: - name: hotel_id type: string description: The ID of the hotel to cancel. statement: UPDATE hotels SET booked = B'0' WHERE id = $1; toolsets: my-toolset: - search-hotels-by-name - search-hotels-by-location - book-hotel - update-hotel - cancel-hotel For more info on tools, check out the Resources section of the docs.\nRun the Toolbox server, pointing to the tools.yaml file created earlier:\n./toolbox --tools-file \"tools.yaml\" Note\nToolbox enables dynamic reloading by default. To disable, use the `--disable-reload` flag. Step 3: Connect your agent to Toolbox In this section, we will write and run an agent that will load the Tools from Toolbox.\nTip\nIf you prefer to experiment within a Google Colab environment, you can connect to a local runtime.\nIn a new terminal, install the SDK package.\nADK Langchain LlamaIndex Core pip install toolbox-core pip install toolbox-langchain pip install toolbox-llamaindex pip install toolbox-core Install other required dependencies:\nADK Langchain LlamaIndex Core pip install google-adk # TODO(developer): replace with correct package if needed pip install langgraph langchain-google-vertexai # pip install langchain-google-genai # pip install langchain-anthropic # TODO(developer): replace with correct package if needed pip install llama-index-llms-google-genai # pip install llama-index-llms-anthropic pip install google-genai Create a new file named hotel_agent.py and copy the following code to create an agent: ADK LangChain LlamaIndex Core from google.adk.agents import Agent from google.adk.runners import Runner from google.adk.sessions import InMemorySessionService from google.adk.artifacts.in_memory_artifact_service import InMemoryArtifactService from google.genai import types from toolbox_core import ToolboxSyncClient import asyncio import os # TODO(developer): replace this with your Google API key os.environ['GOOGLE_API_KEY'] = 'your-api-key' async def main(): with ToolboxSyncClient(\"\u003chttp://127.0.0.1:5000\u003e\") as toolbox_client: prompt = \"\"\" You're a helpful hotel assistant. You handle hotel searching, booking and cancellations. When the user searches for a hotel, mention it's name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. \"\"\" root_agent = Agent( model='gemini-2.0-flash-001', name='hotel_agent', description='A helpful AI assistant.', instruction=prompt, tools=toolbox_client.load_toolset(\"my-toolset\"), ) session_service = InMemorySessionService() artifacts_service = InMemoryArtifactService() session = await session_service.create_session( state={}, app_name='hotel_agent', user_id='123' ) runner = Runner( app_name='hotel_agent', agent=root_agent, artifact_service=artifacts_service, session_service=session_service, ) queries = [ \"Find hotels in Basel with Basel in it's name.\", \"Can you book the Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", ] for query in queries: content = types.Content(role='user', parts=[types.Part(text=query)]) events = runner.run(session_id=session.id, user_id='123', new_message=content) responses = ( part.text for event in events for part in event.content.parts if part.text is not None ) for text in responses: print(text) asyncio.run(main()) import asyncio from langgraph.prebuilt import create_react_agent # TODO(developer): replace this with another import if needed from langchain_google_vertexai import ChatVertexAI # from langchain_google_genai import ChatGoogleGenerativeAI # from langchain_anthropic import ChatAnthropic from langgraph.checkpoint.memory import MemorySaver from toolbox_langchain import ToolboxClient prompt = \"\"\" You're a helpful hotel assistant. You handle hotel searching, booking and cancellations. When the user searches for a hotel, mention it's name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. \"\"\" queries = [ \"Find hotels in Basel with Basel in it's name.\", \"Can you book the Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", ] async def run_application(): # TODO(developer): replace this with another model if needed model = ChatVertexAI(model_name=\"gemini-2.0-flash-001\") # model = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash-001\") # model = ChatAnthropic(model=\"claude-3-5-sonnet-20240620\") # Load the tools from the Toolbox server async with ToolboxClient(\"http://127.0.0.1:5000\") as client: tools = await client.aload_toolset() agent = create_react_agent(model, tools, checkpointer=MemorySaver()) config = {\"configurable\": {\"thread_id\": \"thread-1\"}} for query in queries: inputs = {\"messages\": [(\"user\", prompt + query)]} response = agent.invoke(inputs, stream_mode=\"values\", config=config) print(response[\"messages\"][-1].content) asyncio.run(run_application()) import asyncio import os from llama_index.core.agent.workflow import AgentWorkflow from llama_index.core.workflow import Context # TODO(developer): replace this with another import if needed from llama_index.llms.google_genai import GoogleGenAI # from llama_index.llms.anthropic import Anthropic from toolbox_llamaindex import ToolboxClient prompt = \"\"\" You're a helpful hotel assistant. You handle hotel searching, booking and cancellations. When the user searches for a hotel, mention it's name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. \"\"\" queries = [ \"Find hotels in Basel with Basel in it's name.\", \"Can you book the Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", ] async def run_application(): # TODO(developer): replace this with another model if needed llm = GoogleGenAI( model=\"gemini-2.0-flash-001\", vertexai_config={\"project\": \"project-id\", \"location\": \"us-central1\"}, ) # llm = GoogleGenAI( # api_key=os.getenv(\"GOOGLE_API_KEY\"), # model=\"gemini-2.0-flash-001\", # ) # llm = Anthropic( # model=\"claude-3-7-sonnet-latest\", # api_key=os.getenv(\"ANTHROPIC_API_KEY\") # ) # Load the tools from the Toolbox server async with ToolboxClient(\"http://127.0.0.1:5000\") as client: tools = await client.aload_toolset() agent = AgentWorkflow.from_tools_or_functions( tools, llm=llm, system_prompt=prompt, ) ctx = Context(agent) for query in queries: response = await agent.run(user_msg=query, ctx=ctx) print(f\"---- {query} ----\") print(str(response)) asyncio.run(run_application()) import asyncio from google import genai from google.genai.types import ( Content, FunctionDeclaration, GenerateContentConfig, Part, Tool, ) from toolbox_core import ToolboxClient prompt = \"\"\" You're a helpful hotel assistant. You handle hotel searching, booking and cancellations. When the user searches for a hotel, mention it's name, id, location and price tier. Always mention hotel id while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. \"\"\" queries = [ \"Find hotels in Basel with Basel in it's name.\", \"Please book the hotel Hilton Basel for me.\", \"This is too expensive. Please cancel it.\", \"Please book Hyatt Regency for me\", \"My check in dates for my booking would be from April 10, 2024 to April 19, 2024.\", ] async def run_application(): async with ToolboxClient(\"\u003chttp://127.0.0.1:5000\u003e\") as toolbox_client: # The toolbox_tools list contains Python callables (functions/methods) designed for LLM tool-use # integration. While this example uses Google's genai client, these callables can be adapted for # various function-calling or agent frameworks. For easier integration with supported frameworks # (https://github.com/googleapis/mcp-toolbox-python-sdk/tree/main/packages), use the # provided wrapper packages, which handle framework-specific boilerplate. toolbox_tools = await toolbox_client.load_toolset(\"my-toolset\") genai_client = genai.Client( vertexai=True, project=\"project-id\", location=\"us-central1\" ) genai_tools = [ Tool( function_declarations=[ FunctionDeclaration.from_callable_with_api_option(callable=tool) ] ) for tool in toolbox_tools ] history = [] for query in queries: user_prompt_content = Content( role=\"user\", parts=[Part.from_text(text=query)], ) history.append(user_prompt_content) response = genai_client.models.generate_content( model=\"gemini-2.0-flash-001\", contents=history, config=GenerateContentConfig( system_instruction=prompt, tools=genai_tools, ), ) history.append(response.candidates[0].content) function_response_parts = [] for function_call in response.function_calls: fn_name = function_call.name # The tools are sorted alphabetically if fn_name == \"search-hotels-by-name\": function_result = await toolbox_tools[3](**function_call.args) elif fn_name == \"search-hotels-by-location\": function_result = await toolbox_tools[2](**function_call.args) elif fn_name == \"book-hotel\": function_result = await toolbox_tools[0](**function_call.args) elif fn_name == \"update-hotel\": function_result = await toolbox_tools[4](**function_call.args) elif fn_name == \"cancel-hotel\": function_result = await toolbox_tools[1](**function_call.args) else: raise ValueError(\"Function name not present.\") function_response = {\"result\": function_result} function_response_part = Part.from_function_response( name=function_call.name, response=function_response, ) function_response_parts.append(function_response_part) if function_response_parts: tool_response_content = Content(role=\"tool\", parts=function_response_parts) history.append(tool_response_content) response2 = genai_client.models.generate_content( model=\"gemini-2.0-flash-001\", contents=history, config=GenerateContentConfig( tools=genai_tools, ), ) final_model_response_content = response2.candidates[0].content history.append(final_model_response_content) print(response2.text) asyncio.run(run_application()) ADK Langchain LlamaIndex Core To learn more about Agent Development Kit, check out the ADK documentation.\nTo learn more about Agents in LangChain, check out the LangGraph Agent documentation.\nTo learn more about Agents in LlamaIndex, check out the LlamaIndex AgentWorkflow documentation.\nTo learn more about tool calling with Google GenAI, check out the Google GenAI Documentation.\nRun your agent, and observe the results:\npython hotel_agent.py Info\nFor more information, visit the Python SDK repo.\n","categories":"","description":"How to get started running Toolbox locally with [Python](https://github.com/googleapis/mcp-toolbox-sdk-python), PostgreSQL, and  [Agent Development Kit](https://google.github.io/adk-docs/), [LangGraph](https://www.langchain.com/langgraph), [LlamaIndex](https://www.llamaindex.ai/) or [GoogleGenAI](https://pypi.org/project/google-genai/).\n","excerpt":"How to get started running Toolbox locally with …","ref":"/genai-toolbox/getting-started/local_quickstart/","tags":"","title":"Python Quickstart (Local)"},{"body":" ","categories":"","description":"Python client SDK","excerpt":"Python client SDK","ref":"/genai-toolbox/sdks/python-sdk/","tags":"","title":"Python SDK"},{"body":"Overview Model Context Protocol is an open protocol that standardizes how applications provide context to LLMs. Check out this page on how to connect to Toolbox via MCP.\nStep 1: Set up your BigQuery Dataset and Table In this section, we will create a BigQuery dataset and a table, then insert some data that needs to be accessed by our agent.\nCreate a new BigQuery dataset (replace YOUR_DATASET_NAME with your desired dataset name, e.g., toolbox_mcp_ds, and optionally specify a location like US or EU):\nexport BQ_DATASET_NAME=\"YOUR_DATASET_NAME\" export BQ_LOCATION=\"US\" bq --location=$BQ_LOCATION mk $BQ_DATASET_NAME You can also do this through the Google Cloud Console.\nThe hotels table needs to be defined in your new dataset. First, create a file named create_hotels_table.sql with the following content:\nCREATE TABLE IF NOT EXISTS `YOUR_PROJECT_ID.YOUR_DATASET_NAME.hotels` ( id INT64 NOT NULL, name STRING NOT NULL, location STRING NOT NULL, price_tier STRING NOT NULL, checkin_date DATE NOT NULL, checkout_date DATE NOT NULL, booked BOOLEAN NOT NULL ); Note: Replace YOUR_PROJECT_ID and YOUR_DATASET_NAME in the SQL with your actual project ID and dataset name.\nThen run the command below to execute the sql query:\nbq query --project_id=$GOOGLE_CLOUD_PROJECT --dataset_id=$BQ_DATASET_NAME --use_legacy_sql=false \u003c create_hotels_table.sql . Next, populate the hotels table with some initial data. To do this, create a file named insert_hotels_data.sql and add the following SQL INSERT statement to it.\nINSERT INTO `YOUR_PROJECT_ID.YOUR_DATASET_NAME.hotels` (id, name, location, price_tier, checkin_date, checkout_date, booked) VALUES (1, 'Hilton Basel', 'Basel', 'Luxury', '2024-04-20', '2024-04-22', FALSE), (2, 'Marriott Zurich', 'Zurich', 'Upscale', '2024-04-14', '2024-04-21', FALSE), (3, 'Hyatt Regency Basel', 'Basel', 'Upper Upscale', '2024-04-02', '2024-04-20', FALSE), (4, 'Radisson Blu Lucerne', 'Lucerne', 'Midscale', '2024-04-05', '2024-04-24', FALSE), (5, 'Best Western Bern', 'Bern', 'Upper Midscale', '2024-04-01', '2024-04-23', FALSE), (6, 'InterContinental Geneva', 'Geneva', 'Luxury', '2024-04-23', '2024-04-28', FALSE), (7, 'Sheraton Zurich', 'Zurich', 'Upper Upscale', '2024-04-02', '2024-04-27', FALSE), (8, 'Holiday Inn Basel', 'Basel', 'Upper Midscale', '2024-04-09', '2024-04-24', FALSE), (9, 'Courtyard Zurich', 'Zurich', 'Upscale', '2024-04-03', '2024-04-13', FALSE), (10, 'Comfort Inn Bern', 'Bern', 'Midscale', '2024-04-04', '2024-04-16', FALSE); Note: Replace YOUR_PROJECT_ID and YOUR_DATASET_NAME in the SQL with your actual project ID and dataset name.\nThen run the command below to execute the sql query:\nbq query --project_id=$GOOGLE_CLOUD_PROJECT --dataset_id=$BQ_DATASET_NAME --use_legacy_sql=false \u003c insert_hotels_data.sql Step 2: Install and configure Toolbox In this section, we will download Toolbox, configure our tools in a tools.yaml, and then run the Toolbox server.\nDownload the latest version of Toolbox as a binary:\nTip\nSelect the correct binary corresponding to your OS and CPU architecture.\nexport OS=\"linux/amd64\" # one of linux/amd64, darwin/arm64, darwin/amd64, or windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/$OS/toolbox Make the binary executable:\nchmod +x toolbox Write the following into a tools.yaml file. You must replace the YOUR_PROJECT_ID and YOUR_DATASET_NAME placeholder in the config with your actual BigQuery project and dataset name. The location field is optional; if not specified, it defaults to ‘us’. The table name hotels is used directly in the statements.\nTip\nAuthentication with BigQuery is handled via Application Default Credentials (ADC). Ensure you have run gcloud auth application-default login.\nsources: my-bigquery-source: kind: bigquery project: YOUR_PROJECT_ID location: us tools: search-hotels-by-name: kind: bigquery-sql source: my-bigquery-source description: Search for hotels based on name. parameters: - name: name type: string description: The name of the hotel. statement: SELECT * FROM `YOUR_DATASET_NAME.hotels` WHERE LOWER(name) LIKE LOWER(CONCAT('%', @name, '%')); search-hotels-by-location: kind: bigquery-sql source: my-bigquery-source description: Search for hotels based on location. parameters: - name: location type: string description: The location of the hotel. statement: SELECT * FROM `YOUR_DATASET_NAME.hotels` WHERE LOWER(location) LIKE LOWER(CONCAT('%', @location, '%')); book-hotel: kind: bigquery-sql source: my-bigquery-source description: \u003e- Book a hotel by its ID. If the hotel is successfully booked, returns a NULL, raises an error if not. parameters: - name: hotel_id type: integer description: The ID of the hotel to book. statement: UPDATE `YOUR_DATASET_NAME.hotels` SET booked = TRUE WHERE id = @hotel_id; update-hotel: kind: bigquery-sql source: my-bigquery-source description: \u003e- Update a hotel's check-in and check-out dates by its ID. Returns a message indicating whether the hotel was successfully updated or not. parameters: - name: checkin_date type: string description: The new check-in date of the hotel. - name: checkout_date type: string description: The new check-out date of the hotel. - name: hotel_id type: integer description: The ID of the hotel to update. statement: \u003e- UPDATE `YOUR_DATASET_NAME.hotels` SET checkin_date = PARSE_DATE('%Y-%m-%d', @checkin_date), checkout_date = PARSE_DATE('%Y-%m-%d', @checkout_date) WHERE id = @hotel_id; cancel-hotel: kind: bigquery-sql source: my-bigquery-source description: Cancel a hotel by its ID. parameters: - name: hotel_id type: integer description: The ID of the hotel to cancel. statement: UPDATE `YOUR_DATASET_NAME.hotels` SET booked = FALSE WHERE id = @hotel_id; toolsets: my-toolset: - search-hotels-by-name - search-hotels-by-location - book-hotel - update-hotel - cancel-hotel For more info on tools, check out the Tools section.\nRun the Toolbox server, pointing to the tools.yaml file created earlier:\n./toolbox --tools-file \"tools.yaml\" Step 3: Connect to MCP Inspector Run the MCP Inspector:\nnpx @modelcontextprotocol/inspector Type y when it asks to install the inspector package.\nIt should show the following when the MCP Inspector is up and running (please take note of \u003cYOUR_SESSION_TOKEN\u003e):\nStarting MCP inspector... ⚙️ Proxy server listening on localhost:6277 🔑 Session token: \u003cYOUR_SESSION_TOKEN\u003e Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth 🚀 MCP Inspector is up and running at: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=\u003cYOUR_SESSION_TOKEN\u003e Open the above link in your browser.\nFor Transport Type, select Streamable HTTP.\nFor URL, type in http://127.0.0.1:5000/mcp.\nFor Configuration -\u003e Proxy Session Token, make sure \u003cYOUR_SESSION_TOKEN\u003e is present.\nClick Connect.\nSelect List Tools, you will see a list of tools configured in tools.yaml.\nTest out your tools here!\n","categories":"","description":"How to get started running Toolbox with MCP Inspector and BigQuery as the source.\n","excerpt":"How to get started running Toolbox with MCP Inspector and BigQuery as …","ref":"/genai-toolbox/samples/bigquery/mcp_quickstart/","tags":"","title":"Quickstart (MCP with BigQuery)"},{"body":"Overview Model Context Protocol is an open protocol that standardizes how applications provide context to LLMs. Check out this page on how to connect to Toolbox via MCP.\nStep 1: Get a Looker Client ID and Client Secret The Looker Client ID and Client Secret can be obtained from the Users page of your Looker instance. Refer to the documentation here. You may need to ask an administrator to get the Client ID and Client Secret for you.\nStep 2: Install and configure Toolbox In this section, we will download Toolbox and run the Toolbox server.\nDownload the latest version of Toolbox as a binary:\nTip\nSelect the correct binary corresponding to your OS and CPU architecture.\nexport OS=\"linux/amd64\" # one of linux/amd64, darwin/arm64, darwin/amd64, or windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/$OS/toolbox Make the binary executable:\nchmod +x toolbox Edit the file ~/.gemini/settings.json and add the following to the list of mcpServers. Use the Client Id and Client Secret you obtained earlier. The name of the server - here looker-toolbox - can be anything meaningful to you.\n\"mcpServers\": { \"looker-toolbox\": { \"command\": \"/path/to/toolbox\", \"args\": [ \"--stdio\", \"--prebuilt\", \"looker\" ], \"env\": { \"LOOKER_BASE_URL\": \"https://looker.example.com\", \"LOOKER_CLIENT_ID\": \"\", \"LOOKER_CLIENT_SECRET\": \"\", \"LOOKER_VERIFY_SSL\": \"true\" } } } In some instances you may need to append :19999 to the LOOKER_BASE_URL.\nStep 3: Start Gemini-CLI Run Gemini-CLI:\nnpx https://github.com/google-gemini/gemini-cli Type y when it asks to download.\nLog into Gemini-CLI\nEnter the command /mcp and you should see a list of available tools like\nℹ Configured MCP servers: 🟢 looker-toolbox - Ready (10 tools) - looker-toolbox__get_models - looker-toolbox__query - looker-toolbox__get_looks - looker-toolbox__get_measures - looker-toolbox__get_filters - looker-toolbox__get_parameters - looker-toolbox__get_explores - looker-toolbox__query_sql - looker-toolbox__get_dimensions - looker-toolbox__run_look - looker-toolbox__query_url Start exploring your Looker instance with commands like Find an explore to see orders or show me my current inventory broken down by item category.\nGemini will prompt you for your approval before using a tool. You can approve all the tools at once or one at a time.\n","categories":"","description":"How to get started running Toolbox with Gemini-CLI and Looker as the source.\n","excerpt":"How to get started running Toolbox with Gemini-CLI and Looker as the …","ref":"/genai-toolbox/samples/looker/looker_gemini/","tags":"","title":"Quickstart (MCP with Looker and Gemini-CLI)"},{"body":"Overview Model Context Protocol is an open protocol that standardizes how applications provide context to LLMs. Check out this page on how to connect to Toolbox via MCP.\nStep 1: Get a Looker Client ID and Client Secret The Looker Client ID and Client Secret can be obtained from the Users page of your Looker instance. Refer to the documentation here. You may need to ask an administrator to get the Client ID and Client Secret for you.\nStep 2: Install and configure Toolbox In this section, we will download Toolbox and run the Toolbox server.\nDownload the latest version of Toolbox as a binary:\nTip\nSelect the correct binary corresponding to your OS and CPU architecture.\nexport OS=\"linux/amd64\" # one of linux/amd64, darwin/arm64, darwin/amd64, or windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/$OS/toolbox Make the binary executable:\nchmod +x toolbox Create a file looker_env with the settings for your Looker instance. Use the Client ID and Client Secret you obtained earlier.\nexport LOOKER_BASE_URL=https://looker.example.com export LOOKER_VERIFY_SSL=true export LOOKER_CLIENT_ID=Q7ynZkRkvj9S9FHPm4Wj export LOOKER_CLIENT_SECRET=P5JvZstFnhpkhCYy2yNSfJ6x In some instances you may need to append :19999 to the LOOKER_BASE_URL.\nLoad the looker_env file into your environment.\nsource looker_env Run the Toolbox server using the prebuilt Looker tools.\n./toolbox --prebuilt looker Step 3: Connect to MCP Inspector Run the MCP Inspector:\nnpx @modelcontextprotocol/inspector Type y when it asks to install the inspector package.\nIt should show the following when the MCP Inspector is up and running:\n🔍 MCP Inspector is up and running at http://127.0.0.1:5173 🚀 Open the above link in your browser.\nFor Transport Type, select SSE.\nFor URL, type in http://127.0.0.1:5000/mcp/sse.\nClick Connect.\nSelect List Tools, you will see a list of tools.\nTest out your tools here!\n","categories":"","description":"How to get started running Toolbox with MCP Inspector and Looker as the source.\n","excerpt":"How to get started running Toolbox with MCP Inspector and Looker as …","ref":"/genai-toolbox/samples/looker/looker_mcp_inspector/","tags":"","title":"Quickstart (MCP with Looker)"},{"body":" ","categories":"","description":"Connect your IDE to Spanner using Toolbox.\n","excerpt":"Connect your IDE to Spanner using Toolbox.\n","ref":"/genai-toolbox/how-to/connect-ide/spanner_mcp/","tags":"","title":"Spanner using MCP"},{"body":"Model Context Protocol (MCP) is an open protocol for connecting Large Language Models (LLMs) to data sources like SQL Server. This guide covers how to use MCP Toolbox for Databases to expose your developer assistant tools to a SQL Server instance:\nCursor Windsurf (Codium) Visual Studio Code (Copilot) Cline (VS Code extension) Claude desktop Claude code Gemini CLI Gemini Code Assist Set up the database Create or select a SQL Server instance. Install MCP Toolbox Download the latest version of Toolbox as a binary. Select the correct binary corresponding to your OS and CPU architecture. You are required to use Toolbox version V0.10.0+:\nlinux/amd64 darwin/arm64 darwin/amd64 windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/linux/amd64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/darwin/arm64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/darwin/amd64/toolbox curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/windows/amd64/toolbox.exe Make the binary executable:\nchmod +x toolbox Verify the installation:\n./toolbox --version Configure your MCP Client Claude code Claude desktop Cline Cursor Visual Studio Code (Copilot) Windsurf Gemini CLI Gemini Code Assist Install Claude Code.\nCreate a .mcp.json file in your project root if it doesn’t exist.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"sqlserver\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mssql\",\"--stdio\"], \"env\": { \"MSSQL_HOST\": \"\", \"MSSQL_PORT\": \"\", \"MSSQL_DATABASE\": \"\", \"MSSQL_USER\": \"\", \"MSSQL_PASSWORD\": \"\" } } } } Restart Claude code to apply the new configuration.\nOpen Claude desktop and navigate to Settings.\nUnder the Developer tab, tap Edit Config to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"sqlserver\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mssql\",\"--stdio\"], \"env\": { \"MSSQL_HOST\": \"\", \"MSSQL_PORT\": \"\", \"MSSQL_DATABASE\": \"\", \"MSSQL_USER\": \"\", \"MSSQL_PASSWORD\": \"\" } } } } Restart Claude desktop.\nFrom the new chat screen, you should see a hammer (MCP) icon appear with the new MCP server available.\nOpen the Cline extension in VS Code and tap the MCP Servers icon.\nTap Configure MCP Servers to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"sqlserver\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mssql\",\"--stdio\"], \"env\": { \"MSSQL_HOST\": \"\", \"MSSQL_PORT\": \"\", \"MSSQL_DATABASE\": \"\", \"MSSQL_USER\": \"\", \"MSSQL_PASSWORD\": \"\" } } } } You should see a green active status after the server is successfully connected.\nCreate a .cursor directory in your project root if it doesn’t exist.\nCreate a .cursor/mcp.json file if it doesn’t exist and open it.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"sqlserver\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mssql\",\"--stdio\"], \"env\": { \"MSSQL_HOST\": \"\", \"MSSQL_PORT\": \"\", \"MSSQL_DATABASE\": \"\", \"MSSQL_USER\": \"\", \"MSSQL_PASSWORD\": \"\" } } } } Open Cursor and navigate to Settings \u003e Cursor Settings \u003e MCP. You should see a green active status after the server is successfully connected.\nOpen VS Code and create a .vscode directory in your project root if it doesn’t exist.\nCreate a .vscode/mcp.json file if it doesn’t exist and open it.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcp\" : { \"servers\": { \"cloud-sql-sqlserver\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"cloud-sql-mssql\",\"--stdio\"], \"env\": { \"MSSQL_HOST\": \"\", \"MSSQL_PORT\": \"\", \"MSSQL_DATABASE\": \"\", \"MSSQL_USER\": \"\", \"MSSQL_PASSWORD\": \"\" } } } } } Open Windsurf and navigate to the Cascade assistant.\nTap on the hammer (MCP) icon, then Configure to open the configuration file.\nAdd the following configuration, replace the environment variables with your values, and save:\n{ \"mcpServers\": { \"sqlserver\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mssql\",\"--stdio\"], \"env\": { \"MSSQL_HOST\": \"\", \"MSSQL_PORT\": \"\", \"MSSQL_DATABASE\": \"\", \"MSSQL_USER\": \"\", \"MSSQL_PASSWORD\": \"\" } } } } Install the Gemini CLI.\nIn your working directory, create a folder named .gemini. Within it, create a settings.json file.\nAdd the following configuration, replace the environment variables with your values, and then save:\n{ \"mcpServers\": { \"sqlserver\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mssql\",\"--stdio\"], \"env\": { \"MSSQL_HOST\": \"\", \"MSSQL_PORT\": \"\", \"MSSQL_DATABASE\": \"\", \"MSSQL_USER\": \"\", \"MSSQL_PASSWORD\": \"\" } } } } Install the Gemini Code Assist extension in Visual Studio Code.\nEnable Agent Mode in Gemini Code Assist chat.\nIn your working directory, create a folder named .gemini. Within it, create a settings.json file.\nAdd the following configuration, replace the environment variables with your values, and then save:\n{ \"mcpServers\": { \"sqlserver\": { \"command\": \"./PATH/TO/toolbox\", \"args\": [\"--prebuilt\",\"mssql\",\"--stdio\"], \"env\": { \"MSSQL_HOST\": \"\", \"MSSQL_PORT\": \"\", \"MSSQL_DATABASE\": \"\", \"MSSQL_USER\": \"\", \"MSSQL_PASSWORD\": \"\" } } } } Use Tools Your AI tool is now connected to SQL Server using MCP. Try asking your AI assistant to list tables, create a table, or define and execute other SQL statements.\nThe following tools are available to the LLM:\nlist_tables: lists tables and descriptions execute_sql: execute any SQL statement Note\nPrebuilt tools are pre-1.0, so expect some tool changes between versions. LLMs will adapt to the tools available, so this shouldn’t affect most users.\n","categories":"","description":"Connect your IDE to SQL Server using Toolbox.","excerpt":"Connect your IDE to SQL Server using Toolbox.","ref":"/genai-toolbox/how-to/connect-ide/mssql_mcp/","tags":"","title":"SQL Server using MCP"},{"body":"About Telemetry data such as logs, metrics, and traces will help developers understand the internal state of the system. This page walks though different types of telemetry and observability available in Toolbox.\nToolbox exports telemetry data of logs via standard out/err, and traces/metrics through OpenTelemetry. Additional flags can be passed to Toolbox to enable different logging behavior, or to export metrics through a specific exporter.\nLogging The following flags can be used to customize Toolbox logging:\nFlag Description --log-level Preferred log level, allowed values: debug, info, warn, error. Default: info. --logging-format Preferred logging format, allowed values: standard, json. Default: standard. Example:\n./toolbox --tools-file \"tools.yaml\" --log-level warn --logging-format json Level Toolbox supports the following log levels, including:\nLog level Description Debug Debug logs typically contain information that is only useful during the debugging phase and may be of little value during production. Info Info logs include information about successful operations within the application, such as a successful start, pause, or exit of the application. Warn Warning logs are slightly less severe than error conditions. While it does not cause an error, it indicates that an operation might fail in the future if action is not taken now. Error Error log is assigned to event logs that contain an application error message. Toolbox will only output logs that are equal or more severe to the level that it is set. Below are the log levels that Toolbox supports in the order of severity.\nFormat Toolbox supports both standard and structured logging format.\nThe standard logging outputs log as string:\n2024-11-12T15:08:11.451377-08:00 INFO \"Initialized 0 sources.\\n\" The structured logging outputs log as JSON:\n{ \"timestamp\":\"2024-11-04T16:45:11.987299-08:00\", \"severity\":\"ERROR\", \"logging.googleapis.com/sourceLocation\":{...}, \"message\":\"unable to parse tool file at \\\"tools.yaml\\\": \\\"cloud-sql-postgres1\\\" is not a valid kind of data source\" } Tip\nlogging.googleapis.com/sourceLocation shows the source code location information associated with the log entry, if any.\nTelemetry Toolbox is supports exporting metrics and traces to any OpenTelemetry compatible exporter.\nMetrics A metric is a measurement of a service captured at runtime. The collected data can be used to provide important insights into the service. Toolbox provides the following custom metrics:\nMetric Name Description toolbox.server.toolset.get.count Counts the number of toolset manifest requests served toolbox.server.tool.get.count Counts the number of tool manifest requests served toolbox.server.tool.get.invoke Counts the number of tool invocation requests served toolbox.server.mcp.sse.count Counts the number of mcp sse connection requests served toolbox.server.mcp.post.count Counts the number of mcp post requests served All custom metrics have the following attributes/labels:\nMetric Attributes Description toolbox.name Name of the toolset or tool, if applicable. toolbox.operation.status Operation status code, for example: success, failure. toolbox.sse.sessionId Session id for sse connection, if applicable. toolbox.method Method of JSON-RPC request, if applicable. Traces A trace is a tree of spans that shows the path that a request makes through an application.\nSpans generated by Toolbox server is prefixed with toolbox/server/. For example, when user run Toolbox, it will generate spans for the following, with toolbox/server/init as the root span:\nResource Attributes All metrics and traces generated within Toolbox will be associated with a unified resource. The list of resource attributes included are:\nResource Name Description TelemetrySDK TelemetrySDK version info. OS OS attributes including OS description and OS type. Container Container attributes including container ID, if applicable. Host Host attributes including host name. SchemaURL Sets the schema URL for the configured resource. service.name Open telemetry service name. Defaulted to toolbox. User can set the service name via flag mentioned above to distinguish between different toolbox service. service.version The version of Toolbox used. Exporter An exporter is responsible for processing and exporting telemetry data. Toolbox generates telemetry data within the OpenTelemetry Protocol (OTLP), and user can choose to use exporters that are designed to support the OpenTelemetry Protocol. Within Toolbox, we provide two types of exporter implementation to choose from, either the Google Cloud Exporter that will send data directly to the backend, or the OTLP Exporter along with a Collector that will act as a proxy to collect and export data to the telemetry backend of user’s choice.\nGoogle Cloud Exporter The Google Cloud Exporter directly exports telemetry to Google Cloud Monitoring. It utilizes the GCP Metric Exporter and GCP Trace Exporter.\nNote\nIf you’re using Google Cloud Monitoring, the following APIs will need to be enabled:\nCloud Logging API Cloud Monitoring API Cloud Trace API OTLP Exporter This implementation uses the default OTLP Exporter over HTTP for metrics and traces. You can use this exporter if you choose to export your telemetry data to a Collector.\nCollector A collector acts as a proxy between the application and the telemetry backend. It receives telemetry data, transforms it, and then exports data to backends that can store it permanently. Toolbox provide an option to export telemetry data to user’s choice of backend(s) that are compatible with the Open Telemetry Protocol (OTLP). If you would like to use a collector, please refer to this Export Telemetry using the Otel Collector.\nFlags The following flags are used to determine Toolbox’s telemetry configuration:\nflag type description --telemetry-gcp bool Enable exporting directly to Google Cloud Monitoring. Default is false. --telemetry-otlp string Enable exporting using OpenTelemetry Protocol (OTLP) to the specified endpoint (e.g. “http://127.0.0.1:4318”). --telemetry-service-name string Sets the value of the service.name resource attribute. Default is toolbox. In addition to the flags noted above, you can also make additional configuration for OpenTelemetry via the General SDK Configuration through environmental variables.\nExamples:\nTo enable Google Cloud Exporter:\n./toolbox --telemetry-gcp To enable OTLP Exporter, provide Collector endpoint:\n./toolbox --telemetry-otlp=\"http://127.0.0.1:4553\" ","categories":"","description":"An overview of telemetry and observability in Toolbox.\n","excerpt":"An overview of telemetry and observability in Toolbox.\n","ref":"/genai-toolbox/concepts/telemetry/","tags":"","title":"Telemetry"},{"body":"A tool represents an action your agent can take, such as running a SQL statement. You can define Tools as a map in the tools section of your tools.yaml file. Typically, a tool will require a source to act on:\ntools: search_flights_by_number: kind: postgres-sql source: my-pg-instance statement: | SELECT * FROM flights WHERE airline = $1 AND flight_number = $2 LIMIT 10 description: | Use this tool to get information for a specific flight. Takes an airline code and flight number and returns info on the flight. Do NOT use this tool with a flight id. Do NOT guess an airline code or flight number. An airline code is a code for an airline service consisting of a two-character airline designator and followed by a flight number, which is a 1 to 4 digit number. For example, if given CY 0123, the airline is \"CY\", and flight_number is \"123\". Another example for this is DL 1234, the airline is \"DL\", and flight_number is \"1234\". If the tool returns more than one option choose the date closest to today. Example: {{ \"airline\": \"CY\", \"flight_number\": \"888\", }} Example: {{ \"airline\": \"DL\", \"flight_number\": \"1234\", }} parameters: - name: airline type: string description: Airline unique 2 letter identifier - name: flight_number type: string description: 1 to 4 digit number Specifying Parameters Parameters for each Tool will define what inputs the agent will need to provide to invoke them. Parameters should be pass as a list of Parameter objects:\nparameters: - name: airline type: string description: Airline unique 2 letter identifier - name: flight_number type: string description: 1 to 4 digit number Basic Parameters Basic parameters types include string, integer, float, boolean types. In most cases, the description will be provided to the LLM as context on specifying the parameter.\nparameters: - name: airline type: string description: Airline unique 2 letter identifier field type required description name string true Name of the parameter. type string true Must be one of “string”, “integer”, “float”, “boolean” “array” description string true Natural language description of the parameter to describe it to the agent. default parameter type false Default value of the parameter. If provided, required will be false. required bool false Indicate if the parameter is required. Default to true. Array Parameters The array type is a list of items passed in as a single parameter. To use the array type, you must also specify what kind of items are in the list using the items field:\nparameters: - name: preferred_airlines type: array description: A list of airline, ordered by preference. items: name: name type: string description: Name of the airline. statement: | SELECT * FROM airlines WHERE preferred_airlines = ANY($1); field type required description name string true Name of the parameter. type string true Must be “array” description string true Natural language description of the parameter to describe it to the agent. default parameter type false Default value of the parameter. If provided, required will be false. required bool false Indicate if the parameter is required. Default to true. items parameter object true Specify a Parameter object for the type of the values in the array. Note\nItems in array should not have a default or required value. If provided, it will be ignored.\nMap Parameters The map type is a collection of key-value pairs. It can be configured in two ways:\nGeneric Map: By default, it accepts values of any primitive type (string, integer, float, boolean), allowing for mixed data. Typed Map: By setting the valueType field, you can enforce that all values within the map must be of the same specified type. Generic Map (Mixed Value Types) This is the default behavior when valueType is omitted. It’s useful for passing a flexible group of settings.\nparameters: - name: execution_context type: map description: A flexible set of key-value pairs for the execution environment. Typed Map Specify valueType to ensure all values in the map are of the same type. An error will be thrown in case of value type mismatch.\nparameters: - name: user_scores type: map description: A map of user IDs to their scores. All scores must be integers. valueType: integer # This enforces the value type for all entries. Authenticated Parameters Authenticated parameters are automatically populated with user information decoded from ID tokens that are passed in request headers. They do not take input values in request bodies like other parameters. To use authenticated parameters, you must configure the tool to map the required authServices to specific claims within the user’s ID token.\ntools: search_flights_by_user_id: kind: postgres-sql source: my-pg-instance statement: | SELECT * FROM flights WHERE user_id = $1 parameters: - name: user_id type: string description: Auto-populated from Google login authServices: # Refer to one of the `authServices` defined - name: my-google-auth # `sub` is the OIDC claim field for user ID field: sub field type required description name string true Name of the authServices used to verify the OIDC auth token. field string true Claim field decoded from the OIDC token used to auto-populate this parameter. Template Parameters Template parameters types include string, integer, float, boolean types. In most cases, the description will be provided to the LLM as context on specifying the parameter. Template parameters will be inserted into the SQL statement before executing the prepared statement. They will be inserted without quotes, so to insert a string using template parameters, quotes must be explicitly added within the string.\nTemplate parameter arrays can also be used similarly to basic parameters, and array items must be strings. Once inserted into the SQL statement, the outer layer of quotes will be removed. Therefore to insert strings into the SQL statement, a set of quotes must be explicitly added within the string.\nWarning\nBecause template parameters can directly replace identifiers, column names, and table names, they are prone to SQL injections. Basic parameters are preferred for performance and safety reasons.\ntools: select_columns_from_table: kind: postgres-sql source: my-pg-instance statement: | SELECT {{array .columnNames}} FROM {{.tableName}} description: | Use this tool to list all information from a specific table. Example: {{ \"tableName\": \"flights\", \"columnNames\": [\"id\", \"name\"] }} templateParameters: - name: tableName type: string description: Table to select from - name: columnNames type: array description: The columns to select items: name: column type: string description: Name of a column to select field type required description name string true Name of the template parameter. type string true Must be one of “string”, “integer”, “float”, “boolean” “array” description string true Natural language description of the template parameter to describe it to the agent. items parameter object true (if array) Specify a Parameter object for the type of the values in the array (string only). Authorized Invocations You can require an authorization check for any Tool invocation request by specifying an authRequired field. Specify a list of authServices defined in the previous section.\ntools: search_all_flight: kind: postgres-sql source: my-pg-instance statement: | SELECT * FROM flights # A list of `authServices` defined previously authRequired: - my-google-auth - other-auth-service Kinds of tools ","categories":"","description":"Tools define actions an agent can take -- such as reading and writing to a source.\n","excerpt":"Tools define actions an agent can take -- such as reading and writing …","ref":"/genai-toolbox/resources/tools/","tags":"","title":"Tools"},{"body":"Before you begin Install the Google Cloud CLI.\nSet the PROJECT_ID environment variable:\nexport PROJECT_ID=\"my-project-id\" Initialize gcloud CLI:\ngcloud init gcloud config set project $PROJECT_ID Make sure you’ve set up and initialized your database.\nYou must have the following APIs enabled:\ngcloud services enable run.googleapis.com \\ cloudbuild.googleapis.com \\ artifactregistry.googleapis.com \\ iam.googleapis.com \\ secretmanager.googleapis.com To create an IAM account, you must have the following IAM permissions (or roles):\nCreate Service Account role (roles/iam.serviceAccountCreator) To create a secret, you must have the following roles:\nSecret Manager Admin role (roles/secretmanager.admin) To deploy to Cloud Run, you must have the following set of roles:\nCloud Run Developer (roles/run.developer) Service Account User role (roles/iam.serviceAccountUser) Note\nIf you are using sources that require VPC-access (such as AlloyDB or Cloud SQL over private IP), make sure your Cloud Run service and the database are in the same VPC network.\nCreate a service account Create a backend service account if you don’t already have one:\ngcloud iam service-accounts create toolbox-identity Grant permissions to use secret manager:\ngcloud projects add-iam-policy-binding $PROJECT_ID \\ --member serviceAccount:toolbox-identity@$PROJECT_ID.iam.gserviceaccount.com \\ --role roles/secretmanager.secretAccessor Grant additional permissions to the service account that are specific to the source, e.g.:\nAlloyDB for PostgreSQL Cloud SQL for PostgreSQL Configure tools.yaml file Create a tools.yaml file that contains your configuration for Toolbox. For details, see the configuration section.\nDeploy to Cloud Run Upload tools.yaml as a secret:\ngcloud secrets create tools --data-file=tools.yaml If you already have a secret and want to update the secret version, execute the following:\ngcloud secrets versions add tools --data-file=tools.yaml Set an environment variable to the container image that you want to use for cloud run:\nexport IMAGE=us-central1-docker.pkg.dev/database-toolbox/toolbox/toolbox:latest Deploy Toolbox to Cloud Run using the following command:\ngcloud run deploy toolbox \\ --image $IMAGE \\ --service-account toolbox-identity \\ --region us-central1 \\ --set-secrets \"/app/tools.yaml=tools:latest\" \\ --args=\"--tools-file=/app/tools.yaml\",\"--address=0.0.0.0\",\"--port=8080\" # --allow-unauthenticated # https://cloud.google.com/run/docs/authenticating/public#gcloud If you are using a VPC network, use the command below:\ngcloud run deploy toolbox \\ --image $IMAGE \\ --service-account toolbox-identity \\ --region us-central1 \\ --set-secrets \"/app/tools.yaml=tools:latest\" \\ --args=\"--tools-file=/app/tools.yaml\",\"--address=0.0.0.0\",\"--port=8080\" \\ # TODO(dev): update the following to match your VPC if necessary --network default \\ --subnet default # --allow-unauthenticated # https://cloud.google.com/run/docs/authenticating/public#gcloud Connecting with Toolbox Client SDK You can connect to Toolbox Cloud Run instances directly through the SDK.\nSet up Cloud Run Invoker role access to your Cloud Run service.\n(Only for local runs) Set up Application Default Credentials for the principle you set up the Cloud Run Invoker role access to.\nRun the following to retrieve a non-deterministic URL for the cloud run service:\ngcloud run services describe toolbox --format 'value(status.url)' Import and initialize the toolbox client with the URL retrieved above:\nfrom toolbox_core import ToolboxClient, auth_methods # Replace with the Cloud Run service URL generated in the previous step. URL = \"https://cloud-run-url.app\" auth_token_provider = auth_methods.aget_google_id_token(URL) # can also use sync method async with ToolboxClient( URL, client_headers={\"Authorization\": auth_token_provider}, ) as toolbox: Now, you can use this client to connect to the deployed Cloud Run instance!\n","categories":"","description":"How to set up and configure Toolbox to run on Cloud Run.\n","excerpt":"How to set up and configure Toolbox to run on Cloud Run.\n","ref":"/genai-toolbox/how-to/deploy_toolbox/","tags":"","title":"Deploy to Cloud Run"},{"body":"","categories":"","description":"List of guides detailing how to do different things with Toolbox. \n","excerpt":"List of guides detailing how to do different things with Toolbox. \n","ref":"/genai-toolbox/how-to/","tags":"","title":"How-to"},{"body":"Before you begin This guide assumes you have already done the following:\nInstalled Node.js (v18 or higher). Installed PostgreSQL 16+ and the psql client. Cloud Setup (Optional) If you plan to use Google Cloud’s Vertex AI with your agent (e.g., using Gemini or PaLM models), follow these one-time setup steps:\nInstall the Google Cloud CLI\nSet up Application Default Credentials (ADC)\nSet your project and enable Vertex AI\ngcloud config set project YOUR_PROJECT_ID gcloud services enable aiplatform.googleapis.com Step 1: Set up your database In this section, we will create a database, insert some data that needs to be accessed by our agent, and create a database user for Toolbox to connect with.\nConnect to postgres using the psql command:\npsql -h 127.0.0.1 -U postgres Here, postgres denotes the default postgres superuser.\nInfo\nHaving trouble connecting? Password Prompt: If you are prompted for a password for the postgres user and do not know it (or a blank password doesn’t work), your PostgreSQL installation might require a password or a different authentication method. FATAL: role \"postgres\" does not exist: This error means the default postgres superuser role isn’t available under that name on your system. Connection refused: Ensure your PostgreSQL server is actually running. You can typically check with sudo systemctl status postgresql and start it with sudo systemctl start postgresql on Linux systems. Common Solution For password issues or if the postgres role seems inaccessible directly, try switching to the postgres operating system user first. This user often has permission to connect without a password for local connections (this is called peer authentication).\nsudo -i -u postgres psql -h 127.0.0.1 Once you are in the psql shell using this method, you can proceed with the database creation steps below. Afterwards, type \\q to exit psql, and then exit to return to your normal user shell.\nIf desired, once connected to psql as the postgres OS user, you can set a password for the postgres database user using: ALTER USER postgres WITH PASSWORD 'your_chosen_password';. This would allow direct connection with -U postgres and a password next time.\nCreate a new database and a new user:\nTip\nFor a real application, it’s best to follow the principle of least permission and only grant the privileges your application needs.\nCREATE USER toolbox_user WITH PASSWORD 'my-password'; CREATE DATABASE toolbox_db; GRANT ALL PRIVILEGES ON DATABASE toolbox_db TO toolbox_user; ALTER DATABASE toolbox_db OWNER TO toolbox_user; End the database session:\n\\q (If you used sudo -i -u postgres and then psql, remember you might also need to type exit after \\q to leave the postgres user’s shell session.)\nConnect to your database with your new user:\npsql -h 127.0.0.1 -U toolbox_user -d toolbox_db Create a table using the following command:\nCREATE TABLE hotels( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR NOT NULL, location VARCHAR NOT NULL, price_tier VARCHAR NOT NULL, checkin_date DATE NOT NULL, checkout_date DATE NOT NULL, booked BIT NOT NULL ); Insert data into the table.\nINSERT INTO hotels(id, name, location, price_tier, checkin_date, checkout_date, booked) VALUES (1, 'Hilton Basel', 'Basel', 'Luxury', '2024-04-22', '2024-04-20', B'0'), (2, 'Marriott Zurich', 'Zurich', 'Upscale', '2024-04-14', '2024-04-21', B'0'), (3, 'Hyatt Regency Basel', 'Basel', 'Upper Upscale', '2024-04-02', '2024-04-20', B'0'), (4, 'Radisson Blu Lucerne', 'Lucerne', 'Midscale', '2024-04-24', '2024-04-05', B'0'), (5, 'Best Western Bern', 'Bern', 'Upper Midscale', '2024-04-23', '2024-04-01', B'0'), (6, 'InterContinental Geneva', 'Geneva', 'Luxury', '2024-04-23', '2024-04-28', B'0'), (7, 'Sheraton Zurich', 'Zurich', 'Upper Upscale', '2024-04-27', '2024-04-02', B'0'), (8, 'Holiday Inn Basel', 'Basel', 'Upper Midscale', '2024-04-24', '2024-04-09', B'0'), (9, 'Courtyard Zurich', 'Zurich', 'Upscale', '2024-04-03', '2024-04-13', B'0'), (10, 'Comfort Inn Bern', 'Bern', 'Midscale', '2024-04-04', '2024-04-16', B'0'); End the database session:\n\\q Step 2: Install and configure Toolbox In this section, we will download Toolbox, configure our tools in a tools.yaml, and then run the Toolbox server.\nDownload the latest version of Toolbox as a binary:\nTip\nSelect the correct binary corresponding to your OS and CPU architecture.\nexport OS=\"linux/amd64\" # one of linux/amd64, darwin/arm64, darwin/amd64, or windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/$OS/toolbox Make the binary executable:\nchmod +x toolbox Write the following into a tools.yaml file. Be sure to update any fields such as user, password, or database that you may have customized in the previous step.\nTip\nIn practice, use environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nsources: my-pg-source: kind: postgres host: 127.0.0.1 port: 5432 database: toolbox_db user: ${USER_NAME} password: ${PASSWORD} tools: search-hotels-by-name: kind: postgres-sql source: my-pg-source description: Search for hotels based on name. parameters: - name: name type: string description: The name of the hotel. statement: SELECT * FROM hotels WHERE name ILIKE '%' || $1 || '%'; search-hotels-by-location: kind: postgres-sql source: my-pg-source description: Search for hotels based on location. parameters: - name: location type: string description: The location of the hotel. statement: SELECT * FROM hotels WHERE location ILIKE '%' || $1 || '%'; book-hotel: kind: postgres-sql source: my-pg-source description: \u003e- Book a hotel by its ID. If the hotel is successfully booked, returns a NULL, raises an error if not. parameters: - name: hotel_id type: string description: The ID of the hotel to book. statement: UPDATE hotels SET booked = B'1' WHERE id = $1; update-hotel: kind: postgres-sql source: my-pg-source description: \u003e- Update a hotel's check-in and check-out dates by its ID. Returns a message indicating whether the hotel was successfully updated or not. parameters: - name: hotel_id type: string description: The ID of the hotel to update. - name: checkin_date type: string description: The new check-in date of the hotel. - name: checkout_date type: string description: The new check-out date of the hotel. statement: \u003e- UPDATE hotels SET checkin_date = CAST($2 as date), checkout_date = CAST($3 as date) WHERE id = $1; cancel-hotel: kind: postgres-sql source: my-pg-source description: Cancel a hotel by its ID. parameters: - name: hotel_id type: string description: The ID of the hotel to cancel. statement: UPDATE hotels SET booked = B'0' WHERE id = $1; toolsets: my-toolset: - search-hotels-by-name - search-hotels-by-location - book-hotel - update-hotel - cancel-hotel For more info on tools, check out the Resources section of the docs.\nRun the Toolbox server, pointing to the tools.yaml file created earlier:\n./toolbox --tools-file \"tools.yaml\" Note\nToolbox enables dynamic reloading by default. To disable, use the `--disable-reload` flag. Step 3: Connect your agent to Toolbox In this section, we will write and run an agent that will load the Tools from Toolbox.\n(Optional) Initialize a Node.js project:\nnpm init -y In a new terminal, install the SDK.\nnpm install @toolbox-sdk/core Install other required dependencies\nLangChain GenkitJS LlamaIndex npm install langchain @langchain/google-genai npm install genkit @genkit-ai/vertexai npm install llamaindex @llamaindex/google @llamaindex/workflow Create a new file named hotelAgent.js and copy the following code to create an agent:\nLangChain GenkitJS LlamaIndex import { ChatGoogleGenerativeAI } from \"@langchain/google-genai\"; import { ToolboxClient } from \"@toolbox-sdk/core\"; import { tool } from \"@langchain/core/tools\"; import { createReactAgent } from \"@langchain/langgraph/prebuilt\"; import { MemorySaver } from \"@langchain/langgraph\"; // Replace it with your API key process.env.GOOGLE_API_KEY = 'your-api-key'; const prompt = ` You're a helpful hotel assistant. You handle hotel searching, booking, and cancellations. When the user searches for a hotel, mention its name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. `; const queries = [ \"Find hotels in Basel with Basel in its name.\", \"Can you book the Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", ]; async function runApplication() { const model = new ChatGoogleGenerativeAI({ model: \"gemini-2.0-flash\", }); const client = new ToolboxClient(\"http://127.0.0.1:5000\"); const toolboxTools = await client.loadToolset(\"my-toolset\"); // Define the basics of the tool: name, description, schema and core logic const getTool = (toolboxTool) =\u003e tool(toolboxTool, { name: toolboxTool.getName(), description: toolboxTool.getDescription(), schema: toolboxTool.getParamSchema() }); const tools = toolboxTools.map(getTool); const agent = createReactAgent({ llm: model, tools: tools, checkpointer: new MemorySaver(), systemPrompt: prompt, }); const langGraphConfig = { configurable: { thread_id: \"test-thread\", }, }; for (const query of queries) { const agentOutput = await agent.invoke( { messages: [ { role: \"user\", content: query, }, ], verbose: true, }, langGraphConfig ); const response = agentOutput.messages[agentOutput.messages.length - 1].content; console.log(response); } } runApplication() .catch(console.error) .finally(() =\u003e console.log(\"\\nApplication finished.\")); import { ToolboxClient } from \"@toolbox-sdk/core\"; import { genkit } from \"genkit\"; import { googleAI } from '@genkit-ai/googleai'; // Replace it with your API key process.env.GOOGLE_API_KEY = 'your-api-key'; const systemPrompt = ` You're a helpful hotel assistant. You handle hotel searching, booking, and cancellations. When the user searches for a hotel, mention its name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. `; const queries = [ \"Find hotels in Basel with Basel in its name.\", \"Can you book the Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", ]; async function run() { const toolboxClient = new ToolboxClient(\"http://127.0.0.1:5000\"); const ai = genkit({ plugins: [ googleAI({ apiKey: process.env.GEMINI_API_KEY || process.env.GOOGLE_API_KEY }) ], model: googleAI.model('gemini-2.0-flash'), }); const toolboxTools = await toolboxClient.loadToolset(\"my-toolset\"); const toolMap = Object.fromEntries( toolboxTools.map((tool) =\u003e { const definedTool = ai.defineTool( { name: tool.getName(), description: tool.getDescription(), inputSchema: tool.getParamSchema(), }, tool ); return [tool.getName(), definedTool]; }) ); const tools = Object.values(toolMap); let conversationHistory = [{ role: \"system\", content: [{ text: systemPrompt }] }]; for (const query of queries) { conversationHistory.push({ role: \"user\", content: [{ text: query }] }); const response = await ai.generate({ messages: conversationHistory, tools: tools, }); conversationHistory.push(response.message); const toolRequests = response.toolRequests; if (toolRequests?.length \u003e 0) { // Execute tools concurrently and collect their responses. const toolResponses = await Promise.all( toolRequests.map(async (call) =\u003e { try { const toolOutput = await toolMap[call.name].invoke(call.input); return { role: \"tool\", content: [{ toolResponse: { name: call.name, output: toolOutput } }] }; } catch (e) { console.error(`Error executing tool ${call.name}:`, e); return { role: \"tool\", content: [{ toolResponse: { name: call.name, output: { error: e.message } } }] }; } }) ); conversationHistory.push(...toolResponses); // Call the AI again with the tool results. response = await ai.generate({ messages: conversationHistory, tools }); conversationHistory.push(response.message); } console.log(response.text); } } run(); import { gemini, GEMINI_MODEL } from \"@llamaindex/google\"; import { agent } from \"@llamaindex/workflow\"; import { createMemory, staticBlock, tool } from \"llamaindex\"; import { ToolboxClient } from \"@toolbox-sdk/core\"; const TOOLBOX_URL = \"http://127.0.0.1:5000\"; // Update if needed process.env.GOOGLE_API_KEY = 'your-api-key'; // Replace it with your API key const prompt = ` You're a helpful hotel assistant. You handle hotel searching, booking and cancellations. When the user searches for a hotel, mention its name, id, location and price tier. Always mention hotel ids while performing any searches â€” this is very important for operations. For any bookings or cancellations, please provide the appropriate confirmation. Update check-in or check-out dates if mentioned by the user. Don't ask for confirmations from the user. `; const queries = [ \"Find hotels in Basel with Basel in its name.\", \"Can you book the Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", ]; async function main() { // Connect to MCP Toolbox const client = new ToolboxClient(TOOLBOX_URL); const toolboxTools = await client.loadToolset(\"my-toolset\"); const tools = toolboxTools.map((toolboxTool) =\u003e { return tool({ name: toolboxTool.getName(), description: toolboxTool.getDescription(), parameters: toolboxTool.getParamSchema(), execute: toolboxTool, }); }); // Initialize LLM const llm = gemini({ model: GEMINI_MODEL.GEMINI_2_0_FLASH, apiKey: process.env.GOOGLE_API_KEY, }); const memory = createMemory({ memoryBlocks: [ staticBlock({ content: prompt, }), ], }); // Create the Agent const myAgent = agent({ tools: tools, llm, memory, systemPrompt: prompt, }); for (const query of queries) { const result = await myAgent.run(query); const output = result.data.result; console.log(`\\nUser: ${query}`); if (typeof output === \"string\") { console.log(output.trim()); } else if (typeof output === \"object\" \u0026\u0026 \"text\" in output) { console.log(output.text.trim()); } else { console.log(JSON.stringify(output)); } } //You may observe some extra logs during execution due to the run method provided by Llama. console.log(\"Agent run finished.\"); } main(); Run your agent, and observe the results:\nnode hotelAgent.js Info\nFor more information, visit the JS SDK repo.\n","categories":"","description":"How to get started running Toolbox locally with [JavaScript](https://github.com/googleapis/mcp-toolbox-sdk-js), PostgreSQL, and orchestration frameworks such as [LangChain](https://js.langchain.com/docs/introduction/), [GenkitJS](https://genkit.dev/docs/get-started/), and [LlamaIndex](https://ts.llamaindex.ai/).\n","excerpt":"How to get started running Toolbox locally with …","ref":"/genai-toolbox/getting-started/local_quickstart_js/","tags":"","title":"JS Quickstart (Local)"},{"body":"Before you begin Set the PROJECT_ID environment variable:\nexport PROJECT_ID=\"my-project-id\" Install the gcloud CLI.\nInitialize gcloud CLI:\ngcloud init gcloud config set project $PROJECT_ID You must have the following APIs enabled:\ngcloud services enable artifactregistry.googleapis.com \\ cloudbuild.googleapis.com \\ container.googleapis.com \\ iam.googleapis.com kubectl is used to manage Kubernetes, the cluster orchestration system used by GKE. Verify if you have kubectl installed:\nkubectl version --client If needed, install kubectl component using the Google Cloud CLI:\ngcloud components install kubectl Create a service account Specify a name for your service account with an environment variable:\nexport SA_NAME=toolbox Create a backend service account:\ngcloud iam service-accounts create $SA_NAME Grant any IAM roles necessary to the IAM service account. Each source have a list of necessary IAM permissions listed on it’s page. The example below is for cloud sql postgres source:\ngcloud projects add-iam-policy-binding $PROJECT_ID \\ --member serviceAccount:$SA_NAME@$PROJECT_ID.iam.gserviceaccount.com \\ --role roles/cloudsql.client AlloyDB IAM Identity CloudSQL IAM Identity Spanner IAM Identity Deploy to Kubernetes Set environment variables:\nexport CLUSTER_NAME=toolbox-cluster export DEPLOYMENT_NAME=toolbox export SERVICE_NAME=toolbox-service export REGION=us-central1 export NAMESPACE=toolbox-namespace export SECRET_NAME=toolbox-config export KSA_NAME=toolbox-service-account Create a GKE cluster.\ngcloud container clusters create-auto $CLUSTER_NAME \\ --location=us-central1 Get authentication credentials to interact with the cluster. This also configures kubectl to use the cluster.\ngcloud container clusters get-credentials $CLUSTER_NAME \\ --region=$REGION \\ --project=$PROJECT_ID View the current context for kubectl.\nkubectl config current-context Create namespace for the deployment.\nkubectl create namespace $NAMESPACE Create a Kubernetes Service Account (KSA).\nkubectl create serviceaccount $KSA_NAME --namespace $NAMESPACE Enable the IAM binding between Google Service Account (GSA) and Kubernetes Service Account (KSA).\ngcloud iam service-accounts add-iam-policy-binding \\ --role=\"roles/iam.workloadIdentityUser\" \\ --member=\"serviceAccount:$PROJECT_ID.svc.id.goog[$NAMESPACE/$KSA_NAME]\" \\ $SA_NAME@$PROJECT_ID.iam.gserviceaccount.com Add annotation to KSA to complete binding:\nkubectl annotate serviceaccount \\ $KSA_NAME \\ iam.gke.io/gcp-service-account=$SA_NAME@$PROJECT_ID.iam.gserviceaccount.com \\ --namespace $NAMESPACE Prepare the Kubernetes secret for your tools.yaml file.\nkubectl create secret generic $SECRET_NAME \\ --from-file=./tools.yaml \\ --namespace=$NAMESPACE Create a Kubernetes manifest file (k8s_deployment.yaml) to build deployment.\napiVersion: apps/v1 kind: Deployment metadata: name: toolbox namespace: toolbox-namespace spec: selector: matchLabels: app: toolbox template: metadata: labels: app: toolbox spec: serviceAccountName: toolbox-service-account containers: - name: toolbox # Recommend to use the latest version of toolbox image: us-central1-docker.pkg.dev/database-toolbox/toolbox/toolbox:latest args: [\"--address\", \"0.0.0.0\"] ports: - containerPort: 5000 volumeMounts: - name: toolbox-config mountPath: \"/app/tools.yaml\" subPath: tools.yaml readOnly: true volumes: - name: toolbox-config secret: secretName: toolbox-config items: - key: tools.yaml path: tools.yaml Create the deployment.\nkubectl apply -f k8s_deployment.yaml --namespace $NAMESPACE Check the status of deployment.\nkubectl get deployments --namespace $NAMESPACE Create a Kubernetes manifest file (k8s_service.yaml) to build service.\napiVersion: v1 kind: Service metadata: name: toolbox-service namespace: toolbox-namespace annotations: cloud.google.com/l4-rbs: \"enabled\" spec: selector: app: toolbox ports: - port: 5000 targetPort: 5000 type: LoadBalancer Create the service.\nkubectl apply -f k8s_service.yaml --namespace $NAMESPACE You can find your IP address created for your service by getting the service information through the following.\nkubectl describe services $SERVICE_NAME --namespace $NAMESPACE To look at logs, run the following.\nkubectl logs -f deploy/$DEPLOYMENT_NAME --namespace $NAMESPACE You might have to wait a couple of minutes. It is ready when you can see EXTERNAL-IP with the following command:\nkubectl get svc -n $NAMESPACE Access toolbox locally.\ncurl \u003cEXTERNAL-IP\u003e:5000 Clean up resources Delete secret.\nkubectl delete secret $SECRET_NAME --namespace $NAMESPACE Delete deployment.\nkubectl delete deployment $DEPLOYMENT_NAME --namespace $NAMESPACE Delete the application’s service.\nkubectl delete service $SERVICE_NAME --namespace $NAMESPACE Delete the Kubernetes cluster.\ngcloud container clusters delete $CLUSTER_NAME \\ --location=$REGION ","categories":"","description":"How to set up and configure Toolbox to deploy on Kubernetes with Google Kubernetes Engine (GKE).\n","excerpt":"How to set up and configure Toolbox to deploy on Kubernetes with …","ref":"/genai-toolbox/how-to/deploy_gke/","tags":"","title":"Deploy to Kubernetes"},{"body":" Before you begin Install Docker Compose. Configure tools.yaml file Create a tools.yaml file that contains your configuration for Toolbox. For details, see the configuration section.\nDeploy using Docker Compose Create a docker-compose.yml file, customizing as needed: services: toolbox: # TODO: It is recommended to pin to a specific image version instead of latest. image: us-central1-docker.pkg.dev/database-toolbox/toolbox/toolbox:latest hostname: toolbox platform: linux/amd64 ports: - \"5000:5000\" volumes: - ./config:/config command: [ \"toolbox\", \"--tools-file\", \"/config/tools.yaml\", \"--address\", \"0.0.0.0\"] depends_on: db: condition: service_healthy networks: - tool-network db: # TODO: It is recommended to pin to a specific image version instead of latest. image: postgres hostname: db environment: POSTGRES_USER: toolbox_user POSTGRES_PASSWORD: my-password POSTGRES_DB: toolbox_db ports: - \"5432:5432\" volumes: - ./db:/var/lib/postgresql/data # This file can be used to bootstrap your schema if needed. # See \"initialization scripts\" on https://hub.docker.com/_/postgres/ for more info - ./config/init.sql:/docker-entrypoint-initdb.d/init.sql healthcheck: test: [\"CMD-SHELL\", \"pg_isready -U toolbox_user -d toolbox_db\"] interval: 10s timeout: 5s retries: 5 networks: - tool-network networks: tool-network: Run the following command to bring up the Toolbox and Postgres instance\ndocker-compose up -d Tip\nYou can use this setup quickly set up Toolbox + Postgres to follow along in our Quickstart\nConnecting with Toolbox Client SDK Next, we will use Toolbox with the Client SDKs:\nThe url for the Toolbox server running using docker-compose will be:\nhttp://localhost:5000 Import and initialize the client with the URL:\nLangChain Llamaindex from toolbox_langchain import ToolboxClient # Replace with the cloud run service URL generated above async with ToolboxClient(\"http://$YOUR_URL\") as toolbox: from toolbox_llamaindex import ToolboxClient # Replace with the cloud run service URL generated above async with ToolboxClient(\"http://$YOUR_URL\") as toolbox: ","categories":"","description":"How to deploy Toolbox using Docker Compose.\n","excerpt":"How to deploy Toolbox using Docker Compose.\n","ref":"/genai-toolbox/how-to/deploy_docker/","tags":"","title":"Deploy using Docker Compose"},{"body":"Before you begin This guide assumes you have already done the following:\nInstalled Go (v1.24.2 or higher). Installed PostgreSQL 16+ and the psql client. Cloud Setup (Optional) If you plan to use Google Cloud’s Vertex AI with your agent (e.g., using Gemini or PaLM models), follow these one-time setup steps:\nInstall the Google Cloud CLI\nSet up Application Default Credentials (ADC)\nSet your project and enable Vertex AI\ngcloud config set project YOUR_PROJECT_ID gcloud services enable aiplatform.googleapis.com Step 1: Set up your database In this section, we will create a database, insert some data that needs to be accessed by our agent, and create a database user for Toolbox to connect with.\nConnect to postgres using the psql command:\npsql -h 127.0.0.1 -U postgres Here, postgres denotes the default postgres superuser.\nInfo\nHaving trouble connecting? Password Prompt: If you are prompted for a password for the postgres user and do not know it (or a blank password doesn’t work), your PostgreSQL installation might require a password or a different authentication method. FATAL: role \"postgres\" does not exist: This error means the default postgres superuser role isn’t available under that name on your system. Connection refused: Ensure your PostgreSQL server is actually running. You can typically check with sudo systemctl status postgresql and start it with sudo systemctl start postgresql on Linux systems. Common Solution For password issues or if the postgres role seems inaccessible directly, try switching to the postgres operating system user first. This user often has permission to connect without a password for local connections (this is called peer authentication).\nsudo -i -u postgres psql -h 127.0.0.1 Once you are in the psql shell using this method, you can proceed with the database creation steps below. Afterwards, type \\q to exit psql, and then exit to return to your normal user shell.\nIf desired, once connected to psql as the postgres OS user, you can set a password for the postgres database user using: ALTER USER postgres WITH PASSWORD 'your_chosen_password';. This would allow direct connection with -U postgres and a password next time.\nCreate a new database and a new user:\nTip\nFor a real application, it’s best to follow the principle of least permission and only grant the privileges your application needs.\nCREATE USER toolbox_user WITH PASSWORD 'my-password'; CREATE DATABASE toolbox_db; GRANT ALL PRIVILEGES ON DATABASE toolbox_db TO toolbox_user; ALTER DATABASE toolbox_db OWNER TO toolbox_user; End the database session:\n\\q (If you used sudo -i -u postgres and then psql, remember you might also need to type exit after \\q to leave the postgres user’s shell session.)\nConnect to your database with your new user:\npsql -h 127.0.0.1 -U toolbox_user -d toolbox_db Create a table using the following command:\nCREATE TABLE hotels( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR NOT NULL, location VARCHAR NOT NULL, price_tier VARCHAR NOT NULL, checkin_date DATE NOT NULL, checkout_date DATE NOT NULL, booked BIT NOT NULL ); Insert data into the table.\nINSERT INTO hotels(id, name, location, price_tier, checkin_date, checkout_date, booked) VALUES (1, 'Hilton Basel', 'Basel', 'Luxury', '2024-04-22', '2024-04-20', B'0'), (2, 'Marriott Zurich', 'Zurich', 'Upscale', '2024-04-14', '2024-04-21', B'0'), (3, 'Hyatt Regency Basel', 'Basel', 'Upper Upscale', '2024-04-02', '2024-04-20', B'0'), (4, 'Radisson Blu Lucerne', 'Lucerne', 'Midscale', '2024-04-24', '2024-04-05', B'0'), (5, 'Best Western Bern', 'Bern', 'Upper Midscale', '2024-04-23', '2024-04-01', B'0'), (6, 'InterContinental Geneva', 'Geneva', 'Luxury', '2024-04-23', '2024-04-28', B'0'), (7, 'Sheraton Zurich', 'Zurich', 'Upper Upscale', '2024-04-27', '2024-04-02', B'0'), (8, 'Holiday Inn Basel', 'Basel', 'Upper Midscale', '2024-04-24', '2024-04-09', B'0'), (9, 'Courtyard Zurich', 'Zurich', 'Upscale', '2024-04-03', '2024-04-13', B'0'), (10, 'Comfort Inn Bern', 'Bern', 'Midscale', '2024-04-04', '2024-04-16', B'0'); End the database session:\n\\q Step 2: Install and configure Toolbox In this section, we will download Toolbox, configure our tools in a tools.yaml, and then run the Toolbox server.\nDownload the latest version of Toolbox as a binary:\nTip\nSelect the correct binary corresponding to your OS and CPU architecture.\nexport OS=\"linux/amd64\" # one of linux/amd64, darwin/arm64, darwin/amd64, or windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/$OS/toolbox Make the binary executable:\nchmod +x toolbox Write the following into a tools.yaml file. Be sure to update any fields such as user, password, or database that you may have customized in the previous step.\nTip\nIn practice, use environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nsources: my-pg-source: kind: postgres host: 127.0.0.1 port: 5432 database: toolbox_db user: ${USER_NAME} password: ${PASSWORD} tools: search-hotels-by-name: kind: postgres-sql source: my-pg-source description: Search for hotels based on name. parameters: - name: name type: string description: The name of the hotel. statement: SELECT * FROM hotels WHERE name ILIKE '%' || $1 || '%'; search-hotels-by-location: kind: postgres-sql source: my-pg-source description: Search for hotels based on location. parameters: - name: location type: string description: The location of the hotel. statement: SELECT * FROM hotels WHERE location ILIKE '%' || $1 || '%'; book-hotel: kind: postgres-sql source: my-pg-source description: \u003e- Book a hotel by its ID. If the hotel is successfully booked, returns a NULL, raises an error if not. parameters: - name: hotel_id type: string description: The ID of the hotel to book. statement: UPDATE hotels SET booked = B'1' WHERE id = $1; update-hotel: kind: postgres-sql source: my-pg-source description: \u003e- Update a hotel's check-in and check-out dates by its ID. Returns a message indicating whether the hotel was successfully updated or not. parameters: - name: hotel_id type: string description: The ID of the hotel to update. - name: checkin_date type: string description: The new check-in date of the hotel. - name: checkout_date type: string description: The new check-out date of the hotel. statement: \u003e- UPDATE hotels SET checkin_date = CAST($2 as date), checkout_date = CAST($3 as date) WHERE id = $1; cancel-hotel: kind: postgres-sql source: my-pg-source description: Cancel a hotel by its ID. parameters: - name: hotel_id type: string description: The ID of the hotel to cancel. statement: UPDATE hotels SET booked = B'0' WHERE id = $1; toolsets: my-toolset: - search-hotels-by-name - search-hotels-by-location - book-hotel - update-hotel - cancel-hotel For more info on tools, check out the Resources section of the docs.\nRun the Toolbox server, pointing to the tools.yaml file created earlier:\n./toolbox --tools-file \"tools.yaml\" Note\nToolbox enables dynamic reloading by default. To disable, use the `--disable-reload` flag. Step 3: Connect your agent to Toolbox In this section, we will write and run an agent that will load the Tools from Toolbox.\nInitialize a go module:\ngo mod init main In a new terminal, install the SDK.\ngo get github.com/googleapis/mcp-toolbox-sdk-go Create a new file named hotelagent.go and copy the following code to create an agent:\nLangChain Go Genkit Go Go GenAI OpenAI Go package main import ( \"context\" \"encoding/json\" \"fmt\" \"log\" \"os\" \"github.com/googleapis/mcp-toolbox-sdk-go/core\" \"github.com/tmc/langchaingo/llms\" \"github.com/tmc/langchaingo/llms/googleai\" ) // ConvertToLangchainTool converts a generic core.ToolboxTool into a LangChainGo llms.Tool. func ConvertToLangchainTool(toolboxTool *core.ToolboxTool) llms.Tool { // Fetch the tool's input schema inputschema, err := toolboxTool.InputSchema() if err != nil { return llms.Tool{} } var paramsSchema map[string]any _ = json.Unmarshal(inputschema, \u0026paramsSchema) // Convert into LangChain's llms.Tool return llms.Tool{ Type: \"function\", Function: \u0026llms.FunctionDefinition{ Name: toolboxTool.Name(), Description: toolboxTool.Description(), Parameters: paramsSchema, }, } } const systemPrompt = ` You're a helpful hotel assistant. You handle hotel searching, booking, and cancellations. When the user searches for a hotel, mention its name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. ` var queries = []string{ \"Find hotels in Basel with Basel in its name.\", \"Can you book the hotel Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it.\", \"Please book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", } func main() { genaiKey := os.Getenv(\"GOOGLE_API_KEY\") toolboxURL := \"http://localhost:5000\" ctx := context.Background() // Initialize the Google AI client (LLM). llm, err := googleai.New(ctx, googleai.WithAPIKey(genaiKey), googleai.WithDefaultModel(\"gemini-1.5-flash\")) if err != nil { log.Fatalf(\"Failed to create Google AI client: %v\", err) } // Initialize the MCP Toolbox client. toolboxClient, err := core.NewToolboxClient(toolboxURL) if err != nil { log.Fatalf(\"Failed to create Toolbox client: %v\", err) } // Load the tool using the MCP Toolbox SDK. tools, err := toolboxClient.LoadToolset(\"my-toolset\", ctx) if err != nil { log.Fatalf(\"Failed to load tools: %v\\nMake sure your Toolbox server is running and the tool is configured.\", err) } toolsMap := make(map[string]*core.ToolboxTool, len(tools)) langchainTools := make([]llms.Tool, len(tools)) // Convert the loaded ToolboxTools into the format LangChainGo requires. for i, tool := range tools { langchainTools[i] = ConvertToLangchainTool(tool) toolsMap[tool.Name()] = tool } // Start the conversation history. messageHistory := []llms.MessageContent{ llms.TextParts(llms.ChatMessageTypeSystem, systemPrompt), } for _, query := range queries { messageHistory = append(messageHistory, llms.TextParts(llms.ChatMessageTypeHuman, query)) // Make the first call to the LLM, making it aware of the tool. resp, err := llm.GenerateContent(ctx, messageHistory, llms.WithTools(langchainTools)) if err != nil { log.Fatalf(\"LLM call failed: %v\", err) } respChoice := resp.Choices[0] assistantResponse := llms.TextParts(llms.ChatMessageTypeAI, respChoice.Content) for _, tc := range respChoice.ToolCalls { assistantResponse.Parts = append(assistantResponse.Parts, tc) } messageHistory = append(messageHistory, assistantResponse) // Process each tool call requested by the model. for _, tc := range respChoice.ToolCalls { toolName := tc.FunctionCall.Name tool := toolsMap[toolName] var args map[string]any if err := json.Unmarshal([]byte(tc.FunctionCall.Arguments), \u0026args); err != nil { log.Fatalf(\"Failed to unmarshal arguments for tool '%s': %v\", toolName, err) } toolResult, err := tool.Invoke(ctx, args) if err != nil { log.Fatalf(\"Failed to execute tool '%s': %v\", toolName, err) } if toolResult == \"\" || toolResult == nil { toolResult = \"Operation completed successfully with no specific return value.\" } // Create the tool call response message and add it to the history. toolResponse := llms.MessageContent{ Role: llms.ChatMessageTypeTool, Parts: []llms.ContentPart{ llms.ToolCallResponse{ Name: toolName, Content: fmt.Sprintf(\"%v\", toolResult), }, }, } messageHistory = append(messageHistory, toolResponse) } finalResp, err := llm.GenerateContent(ctx, messageHistory) if err != nil { log.Fatalf(\"Final LLM call failed after tool execution: %v\", err) } // Add the final textual response from the LLM to the history messageHistory = append(messageHistory, llms.TextParts(llms.ChatMessageTypeAI, finalResp.Choices[0].Content)) fmt.Println(finalResp.Choices[0].Content) } } package main import ( \"context\" \"fmt\" \"log\" \"github.com/googleapis/mcp-toolbox-sdk-go/core\" \"github.com/googleapis/mcp-toolbox-sdk-go/tbgenkit\" \"github.com/firebase/genkit/go/ai\" \"github.com/firebase/genkit/go/genkit\" \"github.com/firebase/genkit/go/plugins/googlegenai\" ) const systemPrompt = ` You're a helpful hotel assistant. You handle hotel searching, booking, and cancellations. When the user searches for a hotel, mention its name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. ` var queries = []string{ \"Find hotels in Basel with Basel in its name.\", \"Can you book the hotel Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", } func main() { ctx := context.Background() // Create Toolbox Client toolboxClient, err := core.NewToolboxClient(\"http://127.0.0.1:5000\") if err != nil { log.Fatalf(\"Failed to create Toolbox client: %v\", err) } // Load the tools using the MCP Toolbox SDK. tools, err := toolboxClient.LoadToolset(\"my-toolset\", ctx) if err != nil { log.Fatalf(\"Failed to load tools: %v\\nMake sure your Toolbox server is running and the tool is configured.\", err) } // Initialize Genkit g, err := genkit.Init(ctx, genkit.WithPlugins(\u0026googlegenai.GoogleAI{}), genkit.WithDefaultModel(\"googleai/gemini-1.5-flash\"), ) if err != nil { log.Fatalf(\"Failed to init genkit: %v\\n\", err) } // Create a conversation history conversationHistory := []*ai.Message{ ai.NewSystemTextMessage(systemPrompt), } // Convert your tool to a Genkit tool. genkitTools := make([]ai.Tool, len(tools)) for i, tool := range tools { newTool, err := tbgenkit.ToGenkitTool(tool, g) if err != nil { log.Fatalf(\"Failed to convert tool: %v\\n\", err) } genkitTools[i] = newTool } toolRefs := make([]ai.ToolRef, len(genkitTools)) for i, tool := range genkitTools { toolRefs[i] = tool } for _, query := range queries { conversationHistory = append(conversationHistory, ai.NewUserTextMessage(query)) response, err := genkit.Generate(ctx, g, ai.WithMessages(conversationHistory...), ai.WithTools(toolRefs...), ai.WithReturnToolRequests(true), ) if err != nil { log.Fatalf(\"%v\\n\", err) } conversationHistory = append(conversationHistory, response.Message) parts := []*ai.Part{} for _, req := range response.ToolRequests() { tool := genkit.LookupTool(g, req.Name) if tool == nil { log.Fatalf(\"tool %q not found\", req.Name) } output, err := tool.RunRaw(ctx, req.Input) if err != nil { log.Fatalf(\"tool %q execution failed: %v\", tool.Name(), err) } parts = append(parts, ai.NewToolResponsePart(\u0026ai.ToolResponse{ Name: req.Name, Ref: req.Ref, Output: output, })) } if len(parts) \u003e 0 { resp, err := genkit.Generate(ctx, g, ai.WithMessages(append(response.History(), ai.NewMessage(ai.RoleTool, nil, parts...))...), ai.WithTools(toolRefs...), ) if err != nil { log.Fatal(err) } fmt.Println(\"\\n\", resp.Text()) conversationHistory = append(conversationHistory, resp.Message) } else { fmt.Println(\"\\n\", response.Text()) } } } package main import ( \"context\" \"encoding/json\" \"fmt\" \"log\" \"os\" \"github.com/googleapis/mcp-toolbox-sdk-go/core\" \"google.golang.org/genai\" ) // ConvertToGenaiTool translates a ToolboxTool into the genai.FunctionDeclaration format. func ConvertToGenaiTool(toolboxTool *core.ToolboxTool) *genai.Tool { inputschema, err := toolboxTool.InputSchema() if err != nil { return \u0026genai.Tool{} } var paramsSchema *genai.Schema _ = json.Unmarshal(inputschema, \u0026paramsSchema) // First, create the function declaration. funcDeclaration := \u0026genai.FunctionDeclaration{ Name: toolboxTool.Name(), Description: toolboxTool.Description(), Parameters: paramsSchema, } // Then, wrap the function declaration in a genai.Tool struct. return \u0026genai.Tool{ FunctionDeclarations: []*genai.FunctionDeclaration{funcDeclaration}, } } func printResponse(resp *genai.GenerateContentResponse) { for _, cand := range resp.Candidates { if cand.Content != nil { for _, part := range cand.Content.Parts { fmt.Println(part.Text) } } } } const systemPrompt = ` You're a helpful hotel assistant. You handle hotel searching, booking, and cancellations. When the user searches for a hotel, mention its name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. ` var queries = []string{ \"Find hotels in Basel with Basel in its name.\", \"Can you book the hotel Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it.\", \"Please book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", } func main() { // Setup ctx := context.Background() apiKey := os.Getenv(\"GOOGLE_API_KEY\") toolboxURL := \"http://localhost:5000\" // Initialize the Google GenAI client using the explicit ClientConfig. client, err := genai.NewClient(ctx, \u0026genai.ClientConfig{ APIKey: apiKey, }) if err != nil { log.Fatalf(\"Failed to create Google GenAI client: %v\", err) } // Initialize the MCP Toolbox client. toolboxClient, err := core.NewToolboxClient(toolboxURL) if err != nil { log.Fatalf(\"Failed to create Toolbox client: %v\", err) } // Load the tool using the MCP Toolbox SDK. tools, err := toolboxClient.LoadToolset(\"my-toolset\", ctx) if err != nil { log.Fatalf(\"Failed to load tools: %v\\nMake sure your Toolbox server is running and the tool is configured.\", err) } genAITools := make([]*genai.Tool, len(tools)) toolsMap := make(map[string]*core.ToolboxTool, len(tools)) for i, tool := range tools { genAITools[i] = ConvertToGenaiTool(tool) toolsMap[tool.Name()] = tool } // Set up the generative model with the available tool. modelName := \"gemini-2.0-flash\" // Create the initial content prompt for the model. messageHistory := []*genai.Content{ genai.NewContentFromText(systemPrompt, genai.RoleUser), } config := \u0026genai.GenerateContentConfig{ Tools: genAITools, ToolConfig: \u0026genai.ToolConfig{ FunctionCallingConfig: \u0026genai.FunctionCallingConfig{ Mode: genai.FunctionCallingConfigModeAny, }, }, } for _, query := range queries { messageHistory = append(messageHistory, genai.NewContentFromText(query, genai.RoleUser)) genContentResp, err := client.Models.GenerateContent(ctx, modelName, messageHistory, config) if err != nil { log.Fatalf(\"LLM call failed for query '%s': %v\", query, err) } if len(genContentResp.Candidates) \u003e 0 \u0026\u0026 genContentResp.Candidates[0].Content != nil { messageHistory = append(messageHistory, genContentResp.Candidates[0].Content) } functionCalls := genContentResp.FunctionCalls() toolResponseParts := []*genai.Part{} for _, fc := range functionCalls { toolToInvoke, found := toolsMap[fc.Name] if !found { log.Fatalf(\"Tool '%s' not found in loaded tools map. Check toolset configuration.\", fc.Name) } toolResult, invokeErr := toolToInvoke.Invoke(ctx, fc.Args) if invokeErr != nil { log.Fatalf(\"Failed to execute tool '%s': %v\", fc.Name, invokeErr) } // Enhanced Tool Result Handling (retained to prevent nil issues) toolResultString := \"\" if toolResult != nil { jsonBytes, marshalErr := json.Marshal(toolResult) if marshalErr == nil { toolResultString = string(jsonBytes) } else { toolResultString = fmt.Sprintf(\"%v\", toolResult) } } responseMap := map[string]any{\"result\": toolResultString} toolResponseParts = append(toolResponseParts, genai.NewPartFromFunctionResponse(fc.Name, responseMap)) } // Add all accumulated tool responses for this turn to the message history. toolResponseContent := genai.NewContentFromParts(toolResponseParts, \"function\") messageHistory = append(messageHistory, toolResponseContent) finalResponse, err := client.Models.GenerateContent(ctx, modelName, messageHistory, \u0026genai.GenerateContentConfig{}) if err != nil { log.Fatalf(\"Error calling GenerateContent (with function result): %v\", err) } printResponse(finalResponse) // Add the final textual response from the LLM to the history if len(finalResponse.Candidates) \u003e 0 \u0026\u0026 finalResponse.Candidates[0].Content != nil { messageHistory = append(messageHistory, finalResponse.Candidates[0].Content) } } } package main import ( \"context\" \"encoding/json\" \"log\" \"github.com/googleapis/mcp-toolbox-sdk-go/core\" openai \"github.com/openai/openai-go\" ) // ConvertToOpenAITool converts a ToolboxTool into the go-openai library's Tool format. func ConvertToOpenAITool(toolboxTool *core.ToolboxTool) openai.ChatCompletionToolParam { // Get the input schema jsonSchemaBytes, err := toolboxTool.InputSchema() if err != nil { return openai.ChatCompletionToolParam{} } // Unmarshal the JSON bytes into FunctionParameters var paramsSchema openai.FunctionParameters if err := json.Unmarshal(jsonSchemaBytes, \u0026paramsSchema); err != nil { return openai.ChatCompletionToolParam{} } // Create and return the final tool parameter struct. return openai.ChatCompletionToolParam{ Function: openai.FunctionDefinitionParam{ Name: toolboxTool.Name(), Description: openai.String(toolboxTool.Description()), Parameters: paramsSchema, }, } } const systemPrompt = ` You're a helpful hotel assistant. You handle hotel searching, booking, and cancellations. When the user searches for a hotel, mention its name, id, location and price tier. Always mention hotel ids while performing any searches. This is very important for any operations. For any bookings or cancellations, please provide the appropriate confirmation. Be sure to update checkin or checkout dates if mentioned by the user. Don't ask for confirmations from the user. ` var queries = []string{ \"Find hotels in Basel with Basel in its name.\", \"Can you book the hotel Hilton Basel for me?\", \"Oh wait, this is too expensive. Please cancel it and book the Hyatt Regency instead.\", \"My check in dates would be from April 10, 2024 to April 19, 2024.\", } func main() { // Setup ctx := context.Background() toolboxURL := \"http://localhost:5000\" openAIClient := openai.NewClient() // Initialize the MCP Toolbox client. toolboxClient, err := core.NewToolboxClient(toolboxURL) if err != nil { log.Fatalf(\"Failed to create Toolbox client: %v\", err) } // Load the tools using the MCP Toolbox SDK. tools, err := toolboxClient.LoadToolset(\"my-toolset\", ctx) if err != nil { log.Fatalf(\"Failed to load tool : %v\\nMake sure your Toolbox server is running and the tool is configured.\", err) } openAITools := make([]openai.ChatCompletionToolParam, len(tools)) toolsMap := make(map[string]*core.ToolboxTool, len(tools)) for i, tool := range tools { // Convert the Toolbox tool into the openAI FunctionDeclaration format. openAITools[i] = ConvertToOpenAITool(tool) // Add tool to a map for lookup later toolsMap[tool.Name()] = tool } params := openai.ChatCompletionNewParams{ Messages: []openai.ChatCompletionMessageParamUnion{ openai.SystemMessage(systemPrompt), }, Tools: openAITools, Seed: openai.Int(0), Model: openai.ChatModelGPT4o, } for _, query := range queries { params.Messages = append(params.Messages, openai.UserMessage(query)) // Make initial chat completion request completion, err := openAIClient.Chat.Completions.New(ctx, params) if err != nil { panic(err) } toolCalls := completion.Choices[0].Message.ToolCalls // Return early if there are no tool calls if len(toolCalls) == 0 { log.Println(\"No function call\") } // If there is a was a function call, continue the conversation params.Messages = append(params.Messages, completion.Choices[0].Message.ToParam()) for _, toolCall := range toolCalls { toolName := toolCall.Function.Name toolToInvoke := toolsMap[toolName] var args map[string]any err := json.Unmarshal([]byte(toolCall.Function.Arguments), \u0026args) if err != nil { panic(err) } result, err := toolToInvoke.Invoke(ctx, args) if err != nil { log.Fatal(\"Could not invoke tool\", err) } params.Messages = append(params.Messages, openai.ToolMessage(result.(string), toolCall.ID)) } completion, err = openAIClient.Chat.Completions.New(ctx, params) if err != nil { panic(err) } params.Messages = append(params.Messages, openai.AssistantMessage(query)) println(\"\\n\", completion.Choices[0].Message.Content) } } Ensure all dependencies are installed:\ngo mod tidy Run your agent, and observe the results:\ngo run hotelagent.go Info\nFor more information, visit the Go SDK repo.\n","categories":"","description":"How to get started running Toolbox locally with [Go](https://github.com/googleapis/mcp-toolbox-sdk-go), PostgreSQL, and orchestration frameworks such as [LangChain Go](https://tmc.github.io/langchaingo/docs/), [GenkitGo](https://genkit.dev/go/docs/get-started-go/), [Go GenAI](https://github.com/googleapis/go-genai) and [OpenAI Go](https://github.com/openai/openai-go).\n","excerpt":"How to get started running Toolbox locally with …","ref":"/genai-toolbox/getting-started/local_quickstart_go/","tags":"","title":"Go Quickstart (Local)"},{"body":"","categories":"","description":"List of reference documentation for resources in Toolbox.\n","excerpt":"List of reference documentation for resources in Toolbox.\n","ref":"/genai-toolbox/resources/","tags":"","title":"Resources"},{"body":"About The OpenTelemetry Collector offers a vendor-agnostic implementation of how to receive, process and export telemetry data. It removes the need to run, operate, and maintain multiple agents/collectors.\nConfigure the Collector To configure the collector, you will have to provide a configuration file. The configuration file consists of four classes of pipeline component that access telemetry data.\nReceivers Processors Exporters Connectors Example of setting up the classes of pipeline components (in this example, we don’t use connectors):\nreceivers: otlp: protocols: http: endpoint: \"127.0.0.1:4553\" exporters: googlecloud: project: \u003cYOUR_GOOGLE_CLOUD_PROJECT\u003e processors: batch: send_batch_size: 200 After each pipeline component is configured, you will enable it within the service section of the configuration file.\nservice: pipelines: traces: receivers: [\"otlp\"] processors: [\"batch\"] exporters: [\"googlecloud\"] Running the Collector There are a couple of steps to run and use a Collector.\nInstall the Collector binary. Pull a binary or Docker image for the OpenTelemetry contrib collector.\nSet up credentials for telemetry backend.\nSet up the Collector config. Below are some examples for setting up the Collector config:\nGoogle Cloud Exporter Google Managed Service for Prometheus Exporter Run the Collector with the configuration file.\n./otelcol-contrib --config=collector-config.yaml Run toolbox with the --telemetry-otlp flag. Configure it to send them to http://127.0.0.1:4553 (for HTTP) or the Collector’s URL.\n./toolbox --telemetry-otlp=http://127.0.0.1:4553 Once telemetry datas are collected, you can view them in your telemetry backend. If you are using GCP exporters, telemetry will be visible in GCP dashboard at Metrics Explorer and Trace Explorer.\nNote\nIf you are exporting to Google Cloud monitoring, we recommend that you use the Google Cloud Exporter for traces and the Google Managed Service for Prometheus Exporter for metrics.\n","categories":"","description":"How to set up and configure Toolbox to use the Otel Collector.\n","excerpt":"How to set up and configure Toolbox to use the Otel Collector.\n","ref":"/genai-toolbox/how-to/export_telemetry/","tags":"","title":"Export Telemetry"},{"body":"Overview Model Context Protocol is an open protocol that standardizes how applications provide context to LLMs. Check out this page on how to connect to Toolbox via MCP.\nStep 1: Set up your database In this section, we will create a database, insert some data that needs to be access by our agent, and create a database user for Toolbox to connect with.\nConnect to postgres using the psql command:\npsql -h 127.0.0.1 -U postgres Here, postgres denotes the default postgres superuser.\nCreate a new database and a new user:\nTip\nFor a real application, it’s best to follow the principle of least permission and only grant the privileges your application needs.\nCREATE USER toolbox_user WITH PASSWORD 'my-password'; CREATE DATABASE toolbox_db; GRANT ALL PRIVILEGES ON DATABASE toolbox_db TO toolbox_user; ALTER DATABASE toolbox_db OWNER TO toolbox_user; End the database session:\n\\q Connect to your database with your new user:\npsql -h 127.0.0.1 -U toolbox_user -d toolbox_db Create a table using the following command:\nCREATE TABLE hotels( id INTEGER NOT NULL PRIMARY KEY, name VARCHAR NOT NULL, location VARCHAR NOT NULL, price_tier VARCHAR NOT NULL, checkin_date DATE NOT NULL, checkout_date DATE NOT NULL, booked BIT NOT NULL ); Insert data into the table.\nINSERT INTO hotels(id, name, location, price_tier, checkin_date, checkout_date, booked) VALUES (1, 'Hilton Basel', 'Basel', 'Luxury', '2024-04-22', '2024-04-20', B'0'), (2, 'Marriott Zurich', 'Zurich', 'Upscale', '2024-04-14', '2024-04-21', B'0'), (3, 'Hyatt Regency Basel', 'Basel', 'Upper Upscale', '2024-04-02', '2024-04-20', B'0'), (4, 'Radisson Blu Lucerne', 'Lucerne', 'Midscale', '2024-04-24', '2024-04-05', B'0'), (5, 'Best Western Bern', 'Bern', 'Upper Midscale', '2024-04-23', '2024-04-01', B'0'), (6, 'InterContinental Geneva', 'Geneva', 'Luxury', '2024-04-23', '2024-04-28', B'0'), (7, 'Sheraton Zurich', 'Zurich', 'Upper Upscale', '2024-04-27', '2024-04-02', B'0'), (8, 'Holiday Inn Basel', 'Basel', 'Upper Midscale', '2024-04-24', '2024-04-09', B'0'), (9, 'Courtyard Zurich', 'Zurich', 'Upscale', '2024-04-03', '2024-04-13', B'0'), (10, 'Comfort Inn Bern', 'Bern', 'Midscale', '2024-04-04', '2024-04-16', B'0'); End the database session:\n\\q Step 2: Install and configure Toolbox In this section, we will download Toolbox, configure our tools in a tools.yaml, and then run the Toolbox server.\nDownload the latest version of Toolbox as a binary:\nTip\nSelect the correct binary corresponding to your OS and CPU architecture.\nexport OS=\"linux/amd64\" # one of linux/amd64, darwin/arm64, darwin/amd64, or windows/amd64 curl -O https://storage.googleapis.com/genai-toolbox/v0.11.0/$OS/toolbox Make the binary executable:\nchmod +x toolbox Write the following into a tools.yaml file. Be sure to update any fields such as user, password, or database that you may have customized in the previous step.\nTip\nIn practice, use environment variable replacement with the format ${ENV_NAME} instead of hardcoding your secrets into the configuration file.\nsources: my-pg-source: kind: postgres host: 127.0.0.1 port: 5432 database: toolbox_db user: toolbox_user password: my-password tools: search-hotels-by-name: kind: postgres-sql source: my-pg-source description: Search for hotels based on name. parameters: - name: name type: string description: The name of the hotel. statement: SELECT * FROM hotels WHERE name ILIKE '%' || $1 || '%'; search-hotels-by-location: kind: postgres-sql source: my-pg-source description: Search for hotels based on location. parameters: - name: location type: string description: The location of the hotel. statement: SELECT * FROM hotels WHERE location ILIKE '%' || $1 || '%'; book-hotel: kind: postgres-sql source: my-pg-source description: \u003e- Book a hotel by its ID. If the hotel is successfully booked, returns a NULL, raises an error if not. parameters: - name: hotel_id type: string description: The ID of the hotel to book. statement: UPDATE hotels SET booked = B'1' WHERE id = $1; update-hotel: kind: postgres-sql source: my-pg-source description: \u003e- Update a hotel's check-in and check-out dates by its ID. Returns a message indicating whether the hotel was successfully updated or not. parameters: - name: hotel_id type: string description: The ID of the hotel to update. - name: checkin_date type: string description: The new check-in date of the hotel. - name: checkout_date type: string description: The new check-out date of the hotel. statement: \u003e- UPDATE hotels SET checkin_date = CAST($2 as date), checkout_date = CAST($3 as date) WHERE id = $1; cancel-hotel: kind: postgres-sql source: my-pg-source description: Cancel a hotel by its ID. parameters: - name: hotel_id type: string description: The ID of the hotel to cancel. statement: UPDATE hotels SET booked = B'0' WHERE id = $1; toolsets: my-toolset: - search-hotels-by-name - search-hotels-by-location - book-hotel - update-hotel - cancel-hotel For more info on tools, check out the Tools section.\nRun the Toolbox server, pointing to the tools.yaml file created earlier:\n./toolbox --tools-file \"tools.yaml\" Step 3: Connect to MCP Inspector Run the MCP Inspector:\nnpx @modelcontextprotocol/inspector Type y when it asks to install the inspector package.\nIt should show the following when the MCP Inspector is up and running (please take note of \u003cYOUR_SESSION_TOKEN\u003e):\nStarting MCP inspector... ⚙️ Proxy server listening on localhost:6277 🔑 Session token: \u003cYOUR_SESSION_TOKEN\u003e Use this token to authenticate requests or set DANGEROUSLY_OMIT_AUTH=true to disable auth 🚀 MCP Inspector is up and running at: http://localhost:6274/?MCP_PROXY_AUTH_TOKEN=\u003cYOUR_SESSION_TOKEN\u003e Open the above link in your browser.\nFor Transport Type, select Streamable HTTP.\nFor URL, type in http://127.0.0.1:5000/mcp.\nFor Configuration -\u003e Proxy Session Token, make sure \u003cYOUR_SESSION_TOKEN\u003e is present.\nClick Connect.\nSelect List Tools, you will see a list of tools configured in tools.yaml.\nTest out your tools here!\n","categories":"","description":"How to get started running Toolbox locally with MCP Inspector.\n","excerpt":"How to get started running Toolbox locally with MCP Inspector.\n","ref":"/genai-toolbox/getting-started/mcp_quickstart/","tags":"","title":"Quickstart (MCP)"},{"body":"","categories":"","description":"Samples and guides for using Toolbox.\n","excerpt":"Samples and guides for using Toolbox.\n","ref":"/genai-toolbox/samples/","tags":"","title":"Samples"},{"body":"","categories":"","description":"A list of other information related to Toolbox.\n","excerpt":"A list of other information related to Toolbox.\n","ref":"/genai-toolbox/about/","tags":"","title":"About"},{"body":"The primary way to configure Toolbox is through the tools.yaml file. If you have multiple files, you can tell toolbox which to load with the --tools-file tools.yaml flag.\nYou can find more detailed reference documentation to all resource types in the Resources.\nUsing Environment Variables To avoid hardcoding certain secret fields like passwords, usernames, API keys etc., you could use environment variables instead with the format ${ENV_NAME}.\nuser: ${USER_NAME} password: ${PASSWORD} Sources The sources section of your tools.yaml defines what data sources your Toolbox should have access to. Most tools will have at least one source to execute against.\nsources: my-pg-source: kind: postgres host: 127.0.0.1 port: 5432 database: toolbox_db user: ${USER_NAME} password: ${PASSWORD} For more details on configuring different types of sources, see the Sources.\nTools The tools section of your tools.yaml define your the actions your agent can take: what kind of tool it is, which source(s) it affects, what parameters it uses, etc.\ntools: search-hotels-by-name: kind: postgres-sql source: my-pg-source description: Search for hotels based on name. parameters: - name: name type: string description: The name of the hotel. statement: SELECT * FROM hotels WHERE name ILIKE '%' || $1 || '%'; For more details on configuring different types of tools, see the Tools.\nToolsets The toolsets section of your tools.yaml allows you to define groups of tools that you want to be able to load together. This can be useful for defining different sets for different agents or different applications.\ntoolsets: my_first_toolset: - my_first_tool - my_second_tool my_second_toolset: - my_second_tool - my_third_tool You can load toolsets by name:\n# This will load all tools all_tools = client.load_toolset() # This will only load the tools listed in 'my_second_toolset' my_second_toolset = client.load_toolset(\"my_second_toolset\") ","categories":"","description":"How to configure Toolbox's tools.yaml file.\n","excerpt":"How to configure Toolbox's tools.yaml file.\n","ref":"/genai-toolbox/getting-started/configure/","tags":"","title":"Configuration"},{"body":"","categories":"","description":"Client SDKs to connect to the MCP Toolbox server.\n","excerpt":"Client SDKs to connect to the MCP Toolbox server.\n","ref":"/genai-toolbox/sdks/","tags":"","title":"SDKs"},{"body":"The alloydb-wait-for-operation tool is a utility tool that waits for a long-running AlloyDB operation to complete. It does this by polling the AlloyDB Admin API operation status endpoint until the operation is finished, using exponential backoff.\nInfo\nThis tool is intended for developer assistant workflows with human-in-the-loop and shouldn’t be used for production agents.\nExample sources: alloydb-api-source: kind: http baseUrl: https://alloydb.googleapis.com headers: Authorization: Bearer ${API_KEY} Content-Type: application/json tools: alloydb-operations-get: kind: alloydb-wait-for-operation source: alloydb-api-source description: \"This will poll on operations API until the operation is done. For checking operation status we need projectId, locationID and operationId. Once instance is created give follow up steps on how to use the variables to bring data plane MCP server up in local and remote setup.\" delay: 1s maxDelay: 4m multiplier: 2 maxRetries: 10 Reference field type required description kind string true Must be “alloydb-wait-for-operation”. source string true Name of the source the HTTP request should be sent to. description string true A description of the tool. delay duration false The initial delay between polling requests (e.g., 3s). Defaults to 3 seconds. maxDelay duration false The maximum delay between polling requests (e.g., 4m). Defaults to 4 minutes. multiplier float false The multiplier for the polling delay. The delay is multiplied by this value after each request. Defaults to 2.0. maxRetries int false The maximum number of polling attempts before giving up. Defaults to 10. ","categories":"","description":"Wait for a long-running AlloyDB operation to complete.\n","excerpt":"Wait for a long-running AlloyDB operation to complete.\n","ref":"/genai-toolbox/resources/tools/utility/alloydbwaitforoperation/","tags":"","title":"alloydb-wait-for-operation"},{"body":"","categories":"","description":"","excerpt":"","ref":"/genai-toolbox/categories/","tags":"","title":"Categories"},{"body":"","categories":"","description":"","excerpt":"","ref":"/genai-toolbox/tags/","tags":"","title":"Tags"}]